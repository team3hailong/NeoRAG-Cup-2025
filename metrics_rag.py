import pandas as pd
import re
import os
import ast
from dotenv import load_dotenv
import requests
import time
from google import genai
from rerank import Reranker
from query_expansion import QueryExpansion

# Helper to retrieve and optionally rerank results with query expansion
def retrieve_and_rerank(query, embedding, vector_db, reranker, k, use_query_expansion=True):
    """
    Enhanced retrieval function v·ªõi Query Expansion techniques
    
    Args:
        query: C√¢u h·ªèi g·ªëc
        embedding: Model embedding
        vector_db: Vector database
        reranker: Reranking model (optional)
        k: S·ªë l∆∞·ª£ng documents c·∫ßn l·∫•y
        use_query_expansion: C√≥ s·ª≠ d·ª•ng query expansion hay kh√¥ng
    """
    all_results = []
    seen_docs = set()
    
    if use_query_expansion:
        query_expander = QueryExpansion()
        
        expanded_queries = query_expander.expand_query(
            query, 
            techniques=["rewriting", "synonym", "context"],
            max_expansions=3
        )
        
        ranked_queries = query_expander.rank_expanded_queries(query, expanded_queries, embedding)
        
        weights = [1.0, 0.8, 0.6, 0.5, 0.4, 0.3]
        
        for i, exp_query in enumerate(ranked_queries):
            weight = weights[i] if i < len(weights) else 0.2
            
            # T·∫°o embedding cho c√¢u h·ªèi m·ªü r·ªông
            user_embedding = embedding.encode(exp_query)
            
            initial_limit = k * 2 if reranker else k
            results = vector_db.query("information", user_embedding, limit=initial_limit)
            
            # Th√™m weight score v√† tr√°nh duplicate
            for result in results:
                doc_id = result.get('title', str(hash(result['information'])))
                if doc_id not in seen_docs:
                    result['expansion_weight'] = weight
                    result['source_query'] = exp_query
                    all_results.append(result)
                    seen_docs.add(doc_id)
        all_results.sort(key=lambda x: x.get('expansion_weight', 0), reverse=True)
        all_results = all_results[:k]
        
    else:
        # Retrieval truy·ªÅn th·ªëng kh√¥ng c√≥ query expansion
        user_embedding = embedding.encode(query)
        initial_limit = k * 2 if reranker else k
        all_results = vector_db.query("information", user_embedding, limit=initial_limit)
    
    if reranker and all_results:
        top_results = all_results[:int(k*1.5)] if len(all_results) > int(k*1.5) else all_results
        passages = [res['information'] for res in top_results]
        ranked_scores, ranked_passages = reranker(query, passages)
        
        reranked_results = []
        for rp in ranked_passages[:k]:
            for res in top_results:
                if res['information'] == rp:
                    reranked_results.append(res)
                    break
        return reranked_results
    
    return all_results[:k]

load_dotenv()

LLM_PROVIDER = "nvidia" # "groq" or "nvidia"

from groq import Groq

# client = Groq(api_key=os.getenv("GROQ_API_KEY"))
# GROQ_MODEL = os.getenv("GROQ_MODEL", "meta-llama/llama-4-maverick-17b-128e-instruct")

NVIDIA_API_KEY = os.getenv("NVIDIA_API_KEY")
NVIDIA_MODEL = os.getenv("NVIDIA_MODEL", "qwen/qwen2.5-coder-7b-instruct")
NVIDIA_ENDPOINT = os.getenv("NVIDIA_ENDPOINT", "https://integrate.api.nvidia.com/v1/chat/completions")

# üîß HELPER FUNCTION: Wrapper ƒë·ªÉ h·ªó tr·ª£ c·∫£ OpenAI v√† Gemini, c√≥ th·ªÉ thay ƒë·ªïi temperature, max_tokens
def get_llm_response(messages):
    max_retries = 3
    backoff = 1
    for attempt in range(1, max_retries + 1):
        try:
            if LLM_PROVIDER == "nvidia" and NVIDIA_API_KEY:
                headers = {
                    "Authorization": f"Bearer {NVIDIA_API_KEY}",
                    "accept": "application/json",
                    "Content-Type": "application/json"
                }
                payload = {
                    "model": NVIDIA_MODEL,
                    "messages": messages,
                    "max_tokens": 1024,
                    "stream": False,
                    "temperature": 0.0,
                    "top_p": 1,
                    "frequency_penalty": 0,
                    "presence_penalty": 0
                }
                resp = requests.post(NVIDIA_ENDPOINT, headers=headers, json=payload)
                resp.raise_for_status()
                data = resp.json()
                return data["choices"][0]["message"]["content"]
            else:
                response = client.chat.completions.create(
                    model=GROQ_MODEL,
                    messages=messages,
                    temperature=0.0,
                    max_completion_tokens=848,
                    top_p=1,
                    stream=False,
                    stop=None
                )
                return response.choices[0].message.content
        except Exception as e:
            print(f"Error calling LLM (attempt {attempt}/{max_retries}): {e}")
            if attempt < max_retries:
                time.sleep(backoff)
                backoff *= 2
    return ""

# N√™n ch·∫°y t·ª´ng h√†m t·ª´ ƒëo·∫°n n√†y ƒë·ªÉ test

def hit_k(file_clb_proptit, file_train_data_proptit, embedding, vector_db, reranker=None, k=5, use_query_expansion=True):
    df_clb = pd.read_csv(file_clb_proptit)
    df_train = pd.read_excel(file_train_data_proptit)

    hits = 0
    total_queries = len(df_train)

    for index, row in df_train.iterrows():
        query = row['Query']
        ground_truth_doc = row['Ground truth document']
        # TODO: N·∫øu c√°c em d√πng Text2SQL RAG hay c√°c ph∆∞∆°ng ph√°p s·ª≠ d·ª•ng ng√¥n ng·ªØ truy v·∫•n, c√≥ th·ªÉ b·ªè qua bi·∫øn user_embedding
        # C√°c em c√≥ th·ªÉ d√πng c√°c kƒ© thu·∫≠t ƒë·ªÉ vi·∫øt l·∫°i c√¢u query, Reranking, ... ·ªü ƒëo·∫°n n√†y.
        # Retrieve top-k (with optional reranking and query expansion)
        results = retrieve_and_rerank(query, embedding, vector_db, reranker, k, use_query_expansion)
         
        # L·∫•y danh s√°ch t√†i li·ªáu ƒë∆∞·ª£c truy su·∫•t
        retrieved_docs = [int(result['title'].split(' ')[-1]) for result in results]
        ground_truth_docs =  []
        if type(ground_truth_doc) is str:
            for doc in ground_truth_doc.split(","):
                ground_truth_docs.append(int(doc))
        else:
            ground_truth_docs.append(int(ground_truth_doc))
        if any(doc in retrieved_docs for doc in ground_truth_docs):
            hits += 1
    return hits / total_queries if total_queries > 0 else 0


# H√†m recall@k
def recall_k(file_clb_proptit, file_train, embedding, vector_db, reranker=None, k=5, use_query_expansion=True):
    df_clb = pd.read_csv(file_clb_proptit)
    df_train = pd.read_excel(file_train)

    
    ans = 0

    for index, row in df_train.iterrows():
        hits = 0
        query = row['Query']
        ground_truth_doc = row['Ground truth document']
        # TODO: N·∫øu c√°c em d√πng Text2SQL RAG hay c√°c ph∆∞∆°ng ph√°p s·ª≠ d·ª•ng ng√¥n ng·ªØ truy v·∫•n, c√≥ th·ªÉ b·ªè qua bi·∫øn user_embedding
        # C√°c em c√≥ th·ªÉ d√πng c√°c kƒ© thu·∫≠t ƒë·ªÉ vi·∫øt l·∫°i c√¢u query, Reranking, ... ·ªü ƒëo·∫°n n√†y.
        # Embedding c√¢u query
        # Retrieve top-k (with optional reranking and query expansion)
        results = retrieve_and_rerank(query, embedding, vector_db, reranker, k, use_query_expansion)

        # L·∫•y danh s√°ch t√†i li·ªáu ƒë∆∞·ª£c truy su·∫•t
        retrieved_docs = [int(result['title'].split(' ')[-1]) for result in results]
        ground_truth_docs =  []
        if type(ground_truth_doc) is str:
            for doc in ground_truth_doc.split(","):
                ground_truth_docs.append(int(doc))
        else:
            ground_truth_docs.append(int(ground_truth_doc))
        if any(doc in retrieved_docs for doc in ground_truth_docs):
            hits = len([doc for doc in ground_truth_docs if doc in retrieved_docs])
        ans += hits / len(ground_truth_docs) 
    return ans / len(df_train)


# H√†m precision@k
def precision_k(file_clb_proptit, file_train, embedding, vector_db, reranker=None, k=5, use_query_expansion=True):
    df_clb = pd.read_csv(file_clb_proptit)
    df_train = pd.read_excel(file_train)

    ans = 0

    for index, row in df_train.iterrows():
        hits = 0
        query = row['Query']
        ground_truth_doc = row['Ground truth document']
        # T·∫°o embedding cho c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng
        user_embedding = embedding.encode(query)

        # Retrieve top-k (with optional reranking and query expansion)
        results = retrieve_and_rerank(query, embedding, vector_db, reranker, k, use_query_expansion)

        # L·∫•y danh s√°ch t√†i li·ªáu ƒë∆∞·ª£c truy su·∫•t
        retrieved_docs = [int(result['title'].split(' ')[-1]) for result in results]
        # print(f"Retrieved documents: {retrieved_docs}")
        ground_truth_docs =  []
        if type(ground_truth_doc) is str:
            for doc in ground_truth_doc.split(","):
                ground_truth_docs.append(int(doc))
        else:
            ground_truth_docs.append(int(ground_truth_doc))
        # print("Ground truth documents:", ground_truth_docs)
        # Ki·ªÉm tra xem c√≥ √≠t nh·∫•t m·ªôt t√†i li·ªáu ƒë√∫ng trong k·∫øt qu·∫£ t√¨m ki·∫øm
        if any(doc in retrieved_docs for doc in ground_truth_docs):
            hits = len([doc for doc in retrieved_docs if doc in ground_truth_docs])
        ans += hits / k 
        # print("Hits / k for this query:", hits / k)
    return ans / len(df_train)


# H√†m f1@k
def f1_k(file_clb_proptit, file_train, embedding, vector_db, reranker=None, k=5, use_query_expansion=True):
    precision = precision_k(file_clb_proptit, file_train, embedding, vector_db, reranker, k, use_query_expansion)
    recall = recall_k(file_clb_proptit, file_train, embedding, vector_db, reranker, k, use_query_expansion)
    if precision + recall == 0:
        return 0
    return 2 * (precision * recall) / (precision + recall)

# H√†m MAP@k

def map_k(file_clb_proptit, file_train, embedding, vector_db, reranker=None, k=5, use_query_expansion=True):
    df_clb = pd.read_csv(file_clb_proptit)
    df_train = pd.read_excel(file_train)

    total_map = 0

    for index, row in df_train.iterrows():
        hits = 0
        ap = 0
        query = row['Query']
        ground_truth_doc = row['Ground truth document']
        # T·∫°o embedding cho c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng
        # Retrieve top-k (with optional reranking and query expansion)
        results = retrieve_and_rerank(query, embedding, vector_db, reranker, k, use_query_expansion)

        # L·∫•y danh s√°ch t√†i li·ªáu ƒë∆∞·ª£c truy su·∫•t
        retrieved_docs = [int(result['title'].split(' ')[-1]) for result in results]
        # print(f"Retrieved documents: {retrieved_docs}")
        ground_truth_docs =  []
        if type(ground_truth_doc) is str:
            for doc in ground_truth_doc.split(","):
                ground_truth_docs.append(int(doc))
        else:
            ground_truth_docs.append(int(ground_truth_doc))
        # print("Ground truth documents:", ground_truth_docs)
        
        # T√≠nh MAP cho 1 truy v·∫•n
        for i, doc in enumerate(retrieved_docs):
            if doc in ground_truth_docs:
                hits += 1
                ap += hits / (i + 1)
        if hits > 0:
            ap /= hits
        # print(f"Average Precision for this query: {ap}")
        total_map += ap 
    return total_map / len(df_train)

# H√†m MRR@k
def mrr_k(file_clb_proptit, file_train, embedding, vector_db, reranker=None, k=5, use_query_expansion=True):
    df_clb = pd.read_csv(file_clb_proptit)
    df_train = pd.read_excel(file_train)

    total_mrr = 0

    for index, row in df_train.iterrows():
        query = row['Query']
        ground_truth_doc = row['Ground truth document']
        # T·∫°o embedding cho c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng
        # Retrieve top-k (with optional reranking and query expansion)
        results = retrieve_and_rerank(query, embedding, vector_db, reranker, k, use_query_expansion)

        # L·∫•y danh s√°ch t√†i li·ªáu ƒë∆∞·ª£c truy su·∫•t
        retrieved_docs = [int(result['title'].split(' ')[-1]) for result in results]
        # print(f"Retrieved documents: {retrieved_docs}")
        ground_truth_docs =  []
        if type(ground_truth_doc) is str:
            for doc in ground_truth_doc.split(","):
                ground_truth_docs.append(int(doc))
        else:
            ground_truth_docs.append(int(ground_truth_doc))
        # print("Ground truth documents:", ground_truth_docs)
        
        # T√≠nh MRR cho 1 truy v·∫•n
        for i, doc in enumerate(retrieved_docs):
            if doc in ground_truth_docs:
                total_mrr += 1 / (i + 1)
                break
    return total_mrr / len(df_train) if len(df_train) > 0 else 0

# H√†m NDCG@k
import numpy as np
import torch
def dcg_at_k(relevances, k):
    relevances = np.array(relevances)[:k]
    return np.sum((2**relevances - 1) / np.log2(np.arange(2, len(relevances) + 2)))

def ndcg_at_k(relevances, k):
    dcg_max = dcg_at_k(sorted(relevances, reverse=True), k)
    if dcg_max == 0:
        return 0.0
    return dcg_at_k(relevances, k) / dcg_max

def similarity(embedding1, embedding2):
    # Use torch cosine similarity on appropriate device for performance
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    a = torch.tensor(embedding1, device=device, dtype=torch.float)
    b = torch.tensor(embedding2, device=device, dtype=torch.float)
    norm1 = torch.norm(a)
    norm2 = torch.norm(b)
    if norm1.item() == 0 or norm2.item() == 0:
        return 0.0
    cos_sim = torch.dot(a, b) / (norm1 * norm2)
    # Normalize to [0,1]
    return ((cos_sim + 1) / 2).item()


def ndcg_k(file_clb_proptit, file_train, embedding, vector_db, reranker=None, k=5, use_query_expansion=True):
    df_clb = pd.read_csv(file_clb_proptit)
    df_train = pd.read_excel(file_train)

    total_ndcg = 0

    for index, row in df_train.iterrows():
        query = row['Query']
        ground_truth_doc = row['Ground truth document']
        # T·∫°o embedding cho c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng
        user_embedding = embedding.encode(query)

        # Retrieve top-k (with optional reranking and query expansion)
        results = retrieve_and_rerank(query, embedding, vector_db, reranker, k, use_query_expansion)


        retrieved_docs = [int(result['title'].split(' ')[-1]) for result in results]

        ground_truth_docs = []
        if type(ground_truth_doc) is str:
            for doc in ground_truth_doc.split(","):
                ground_truth_docs.append(int(doc))
        else:
            ground_truth_docs.append(int(ground_truth_doc))


        # N·∫øu ƒëi·ªÉm t∆∞∆°ng ƒë·ªìng > 0.9 th√¨ g√°n 3, n·∫øu > 0.7 th√¨ g√°n 2, n·∫øu > 0.5 th√¨ g√°n 1, c√≤n l·∫°i th√¨ g√°n 0 
        relevances = []
        for doc in retrieved_docs:
            if doc in ground_truth_docs:
                # Gi·∫£ s·ª≠ ta c√≥ m·ªôt h√†m ƒë·ªÉ t√≠nh ƒë·ªô t∆∞∆°ng ƒë·ªìng gi·ªØa c√¢u h·ªèi v√† t√†i li·ªáu, doc l√† s·ªë th·ª© t·ª± c·ªßa t√†i li·ªáu trong file CLB_PROPTIT.csv
                doc_result = [r for r in results if int(r['title'].split(' ')[-1]) == doc][0]
                doc_embedding = embedding.encode(doc_result['information'])
                similarity_score = similarity(user_embedding, doc_embedding)
                if similarity_score > 0.9:
                    relevances.append(3)
                elif similarity_score > 0.7:
                    relevances.append(2)
                elif similarity_score > 0.5:
                    relevances.append(1)
                else:
                    relevances.append(0)
            else:
                relevances.append(0)
        ndcg = ndcg_at_k(relevances, k)
        # print(f"NDCG for this query: {ndcg}")
        total_ndcg += ndcg

    return total_ndcg / len(df_train) if len(df_train) > 0 else 0

# H√†m Context Precision@k (LLM Judged)
def context_precision_k(file_clb_proptit, file_train, embedding, vector_db, reranker=None, k=5, use_query_expansion=True):
    df_clb = pd.read_csv(file_clb_proptit)
    df_train = pd.read_excel(file_train)

    # Ch·ªâ l·∫•y 30 h√†ng ƒë·ªÉ test nhanh
    # sample_size = 30
    # df_train = df_train.tail(sample_size)
    # print(f"Testing with {len(df_train)} queries out of total {sample_size * 5} queries")

    total_precision = 0

    for index, row in df_train.iterrows():
        print(f"Processing query {index+1}/{len(df_train)}: {row['Query'][:50]}...")
        # TODO: T·∫°o ra LLM Answer, c√°c em h√£y t·ª± vi·∫øt ph·∫ßn system prompt
        messages = [
            {
                "role": "system",
                "content": """B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n cung c·∫•p th√¥ng tin v·ªÅ C√¢u l·∫°c b·ªô L·∫≠p tr√¨nh ProPTIT.
B·∫°n s·∫Ω nh·∫≠n ƒë∆∞·ª£c d·ªØ li·ªáu ng·ªØ c·∫£nh (context) t·ª´ m·ªôt h·ªá th·ªëng Retrieval-Augmented Generation (RAG) ch·ª©a c√°c th√¥ng tin ch√≠nh x√°c v·ªÅ CLB.

1. Ch·ªâ s·ª≠ d·ª•ng th√¥ng tin t·ª´ context ƒë∆∞·ª£c cung c·∫•p ƒë·ªÉ tr·∫£ l·ªùi. Context s·∫Ω ƒë∆∞·ª£c cung c·∫•p ·ªü ƒë·∫ßu m·ªói query c·ªßa ng∆∞·ªùi d√πng. C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng n·∫±m ·ªü cu·ªëi. 
2. N·∫øu ng∆∞·ªùi d√πng h·ªèi c√¢u h·ªèi kh√¥ng li√™n quan ƒë·∫øn CLB ProPTIT, h√£y tr·∫£ l·ªùi nh∆∞ b√¨nh th∆∞·ªùng, nh∆∞ng kh√¥ng s·ª≠ d·ª•ng th√¥ng tin t·ª´ context
3. Tr√¨nh b√†y c√¢u tr·∫£ l·ªùi r√µ r√†ng, d·ªÖ hi·ªÉu. C√≥ th·ªÉ s·ª≠ d·ª•ng emoij icon khi c·∫ßn.
4. Tuy·ªát ƒë·ªëi kh√¥ng suy ƒëo√°n ho·∫∑c b·ªãa th√¥ng tin.
5. Gi·ªØ phong c√°ch tr·∫£ l·ªùi th√¢n thi·ªán, chuy√™n nghi·ªáp v√† nh·∫•t qu√°n.
6. Trong context c√≥ th·ªÉ ch·ª©a nhi·ªÅu th√¥ng tin kh√°c nhau, h√£y t·∫≠p trung v√†o c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng ƒë·ªÉ tr·∫£ l·ªùi ch√≠nh x√°c nh·∫•t.

Nhi·ªám v·ª• c·ªßa b·∫°n:
- Tr·∫£ l·ªùi c√°c c√¢u h·ªèi v·ªÅ CLB L·∫≠p tr√¨nh ProPTIT: l·ªãch s·ª≠, th√†nh vi√™n, ho·∫°t ƒë·ªông, s·ª± ki·ªán, d·ª± √°n, n·ªôi quy, th√†nh vi√™n ti√™u bi·ªÉu, v√† c√°c th√¥ng tin li√™n quan kh√°c."""
            }
        ]
        hits = 0
        query = row['Query']

        # T√¨m ki·∫øm th√¥ng tin li√™n quan trong c∆° s·ªü d·ªØ li·ªáu v·ªõi optional reranking/query expansion
        results = retrieve_and_rerank(query, embedding, vector_db, reranker, k, use_query_expansion)
         
        # TODO: vi·∫øt c√¢u query c·ªßa ng∆∞·ªùi d√πng (bao g·ªìm document retrieval v√† c√¢u query)
        context = "Content t·ª´ c√°c t√†i li·ªáu li√™n quan:\n"
        context += "\n".join([result["information"] for result in results])

        # Th√™m context v√†o messages
        messages.append({
            "role": "user",
            "content": context + "\n\nC√¢u h·ªèi: " + query
        })
        # G·ªçi  API ƒë·ªÉ l·∫•y c√¢u tr·∫£ l·ªùi
        reply = get_llm_response(messages)

        system_judge = {
            "role": "system",
            "content": "B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n ƒë√°nh gi√° ƒë·ªô ch√≠nh x√°c c·ªßa c√°c c√¢u tr·∫£ l·ªùi d·ª±a tr√™n ng·ªØ c·∫£nh ƒë∆∞·ª£c cung c·∫•p. "
                      "B·∫°n s·∫Ω ƒë∆∞·ª£c cung c·∫•p m·ªôt c√¢u h·ªèi, m·ªôt c√¢u tr·∫£ l·ªùi, v√† danh s√°ch ng·ªØ c·∫£nh. "
                      "Nhi·ªám v·ª•: ƒë√°nh gi√° m·ª©c ƒë·ªô li√™n quan c·ªßa m·ªói ng·ªØ c·∫£nh v·ªõi c√¢u tr·∫£ l·ªùi. "
                      f"Tr·∫£ v·ªÅ m·ªôt chu·ªói li·ªÅn m·∫°ch g·ªìm {k} k√Ω t·ª±, m·ªói k√Ω t·ª± l√† 1 n·∫øu ng·ªØ c·∫£nh t∆∞∆°ng ·ª©ng li√™n quan, 0 n·∫øu kh√¥ng, s·∫Øp x·∫øp theo ƒë√∫ng th·ª© t·ª± c√°c ng·ªØ c·∫£nh v√† gi·∫£i th√≠ch ng·∫Øn g·ªçn b·∫±ng duy nh·∫•t 1 c√¢u ·ªü b√™n d∆∞·ªõi."
        }
        
        context_sections = results
        user_content = f"C√¢u h·ªèi: {query}\nC√¢u tr·∫£ l·ªùi: {reply}\n\nNg·ªØ c·∫£nh:\n"
        for idx, res in enumerate(context_sections, 1):
            user_content += f"{idx}. {res['information']}\n"
            print(f"Context {idx}: {res['information'][:50]}...")
        user_content += f"\nH√£y ƒë√°nh gi√° m·ª©c ƒë·ªô li√™n quan cho m·ªói ng·ªØ c·∫£nh, tr·∫£ l·ªùi chu·ªói g·ªìm {k} k√Ω t·ª± 1 ho·∫∑c 0 theo th·ª© t·ª± tr√™n, kh√¥ng gi·∫£i th√≠ch."
        messages_judged = [system_judge, {"role": "user", "content": user_content}]
        judged_reply = get_llm_response(messages_judged)
        raw = str(judged_reply)
        # D√πng regex ƒë·ªÉ t√¨m chu·ªói ƒë√∫ng ƒë·ªãnh d·∫°ng
        match = re.search(rf'[01]{{{k}}}', raw)
        if match:
            flags = match.group(0)
        else:
            # Fallback: ƒëi·ªÅn 0 b√π
            tmp = ''.join(c for c in raw if c in '01')
            flags = (tmp + '0' * k)[:k]
        hits = flags.count('1')
        # Debug
        print(f"Judged reply: {judged_reply}")
        print(f"Flags: {flags}, Hits: {hits}")

        precision = hits / k if k > 0 else 0
        total_precision += precision
        time.sleep(1)
    return total_precision / len(df_train) if len(df_train) > 0 else 0


# H√†m Context Recall@k (LLM Judged)
def context_recall_k(file_clb_proptit, file_train, embedding, vector_db, reranker=None, k=5, use_query_expansion=True):
    df_clb = pd.read_csv(file_clb_proptit)
    df_train = pd.read_excel(file_train)
    # Sample a subset for faster testing / stable evaluation (same behavior as context_precision_k)
    # sample_size = 20
    # df_train = df_train.head(sample_size)
    # print(f"Testing context_recall_k with {len(df_train)} queries (sample_size={sample_size})")

    total_recall = 0

    for index, row in df_train.iterrows():
        query = row['Query']

        # T√¨m ki·∫øm th√¥ng tin li√™n quan trong c∆° s·ªü d·ªØ li·ªáu
        results = retrieve_and_rerank(query, embedding, vector_db, reranker, k, use_query_expansion)
        reply = row['Ground truth answer']
        

        # Build a single judge prompt that asks for a k-length 0/1 string (no explanation)
        system_judge = {
            "role": "system",
            "content": (
                "B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n ƒë√°nh gi√° ƒë·ªô ch√≠nh x√°c c·ªßa c√°c c√¢u tr·∫£ l·ªùi d·ª±a tr√™n ng·ªØ c·∫£nh ƒë∆∞·ª£c cung c·∫•p. "
                "B·∫°n s·∫Ω ƒë∆∞·ª£c cung c·∫•p m·ªôt c√¢u h·ªèi, m·ªôt c√¢u tr·∫£ l·ªùi ch√≠nh x√°c (ground-truth), v√† m·ªôt danh s√°ch ng·ªØ c·∫£nh. "
                "Nhi·ªám v·ª•: cho bi·∫øt m·ªói ng·ªØ c·∫£nh c√≥ ƒë·ªß th√¥ng tin (ho·∫∑c m·ªôt ph·∫ßn th√¥ng tin) ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n c√¢u tr·∫£ l·ªùi ch√≠nh x√°c hay kh√¥ng. "
                "Tr·∫£ v·ªÅ m·ªôt chu·ªói g·ªìm ch√≠nh x√°c {k} k√Ω t·ª±, m·ªói k√Ω t·ª± l√† 1 n·∫øu ng·ªØ c·∫£nh t∆∞∆°ng ·ª©ng li√™n quan/useful, 0 n·∫øu kh√¥ng, theo ƒë√∫ng th·ª© t·ª± c√°c ng·ªØ c·∫£nh. "
                "KH√îNG gi·∫£i th√≠ch th√™m, ch·ªâ tr·∫£ v·ªÅ chu·ªói 0/1."
            )
        }

        user_content = f"C√¢u h·ªèi: {query}\nC√¢u tr·∫£ l·ªùi ch√≠nh x√°c: {reply}\n\nNg·ªØ c·∫£nh:\n"
        for idx, res in enumerate(results, 1):
            user_content += f"{idx}. {res['information']}\n"
            print(f"Context {idx}: {res['information'][:50]}...")

        messages_judged = [system_judge, {"role": "user", "content": user_content}]
        judged_reply = get_llm_response(messages_judged)

        raw = str(judged_reply)
        # Try to extract a k-length binary string
        match = re.search(rf'[01]{{{k}}}', raw)
        if match:
            flags = match.group(0)
        else:
            tmp = ''.join(c for c in raw if c in '01')
            flags = (tmp + '0' * k)[:k]

        hits = flags.count('1')
        recall = hits / k if k > 0 else 0
        total_recall += recall
        # debug
        print(f"Query {index+1}/{len(df_train)} - Flags: {flags} - Hits: {hits} - Recall: {recall:.3f}")
        time.sleep(1)

    return total_recall / len(df_train) if len(df_train) > 0 else 0

# H√†m Context Entities Recall@k (LLM Judged)
def context_entities_recall_k(file_clb_proptit, file_train, embedding, vector_db, reranker=None, k=5, use_query_expansion=True):
    df_clb = pd.read_csv(file_clb_proptit)
    df_train = pd.read_excel(file_train)
    # Sample subset for faster testing
    # sample_size = 20
    # df_train = df_train.head(sample_size)
    # print(f"Testing context_entities_recall_k with {len(df_train)} queries (sample_size={sample_size})")

    total_recall = 0
    for index, row in df_train.iterrows():
        hits = 0
        query = row['Query']

        # T√¨m ki·∫øm th√¥ng tin li√™n quan trong c∆° s·ªü d·ªØ li·ªáu
        results = retrieve_and_rerank(query, embedding, vector_db, reranker, k, use_query_expansion)
        reply = row['Ground truth answer']
        # Tr√≠ch xu·∫•t c√°c th·ª±c th·ªÉ t·ª´ Ground truth answer b·∫±ng LLM
        # NOTE: C√°c em c√≥ th·ªÉ thay ƒë·ªïi messages_entities n·∫øu mu·ªën
        messages_entities = [
            {
                "role": "system",
                "content": """B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n tr√≠ch xu·∫•t c√°c th·ª±c th·ªÉ t·ª´ c√¢u tr·∫£ l·ªùi. B·∫°n s·∫Ω ƒë∆∞·ª£c cung c·∫•p m·ªôt c√¢u tr·∫£ l·ªùi v√† nhi·ªám v·ª• c·ªßa b·∫°n l√† tr√≠ch xu·∫•t c√°c th·ª±c th·ªÉ t·ª´ c√¢u tr·∫£ l·ªùi ƒë√≥. C√°c th·ª±c th·ªÉ c√≥ th·ªÉ l√† t√™n ng∆∞·ªùi, ƒë·ªãa ƒëi·ªÉm, t·ªï ch·ª©c, s·ª± ki·ªán, v.v. H√£y tr·∫£ l·ªùi d∆∞·ªõi d·∫°ng m·ªôt danh s√°ch c√°c th·ª±c th·ªÉ.
                V√≠ d·ª•:
                C√¢u tr·∫£ l·ªùi: N·∫øu b·∫°n thu·ªôc ng√†nh kh√°c b·∫°n v·∫´n c√≥ th·ªÉ tham gia CLB ch√∫ng m√¨nh. N·∫øu ƒë·ªãnh h∆∞·ªõng c·ªßa b·∫°n ho√†n to√†n l√† theo CNTT th√¨ CLB ch·∫Øc ch·∫Øn l√† n∆°i ph√π h·ª£p nh·∫•t ƒë·ªÉ c√°c b·∫°n ph√°t tri·ªÉn. Tr·ªü ng·∫°i l·ªõn nh·∫•t s·∫Ω l√† do b·∫°n theo m·ªôt h∆∞·ªõng kh√°c n·ªØa n√™n s·∫Ω ph·∫£i t·∫≠p trung v√†o c·∫£ 2 m·∫£ng n√™n s·∫Ω c·∫ßn c·ªë g·∫Øng nhi·ªÅu h∆°n.
                ["ng√†nh kh√°c", "CLB", "CNTT", "m·∫£ng]
                C√¢u tr·∫£ l·ªùi: C√¢u l·∫°c b·ªô L·∫≠p Tr√¨nh PTIT (Programming PTIT), t√™n vi·∫øt t·∫Øt l√† PROPTIT ƒë∆∞·ª£c th√†nh l·∫≠p ng√†y 9/10/2011. V·ªõi ph∆∞∆°ng ch√¢m ho·∫°t ƒë·ªông "Chia s·∫ª ƒë·ªÉ c√πng nhau ph√°t tri·ªÉn", c√¢u l·∫°c b·ªô l√† n∆°i giao l∆∞u, ƒë√†o t·∫°o c√°c m√¥n l·∫≠p tr√¨nh v√† c√°c m√¥n h·ªçc trong tr∆∞·ªùng, t·∫°o ƒëi·ªÅu ki·ªán ƒë·ªÉ sinh vi√™n trong H·ªçc vi·ªán c√≥ m√¥i tr∆∞·ªùng h·ªçc t·∫≠p nƒÉng ƒë·ªông s√°ng t·∫°o. Slogan: L·∫≠p Tr√¨nh PTIT - L·∫≠p tr√¨nh t·ª´ tr√°i tim.
                ["C√¢u l·∫°c b·ªô L·∫≠p Tr√¨nh PTIT (Programming PTIT)", "PROPTIT", "9/10/2011", "Chia s·∫ª ƒë·ªÉ c√πng nhau ph√°t tri·ªÉn", "sinh vi√™n", "H·ªçc vi·ªán", "L·∫≠p Tr√¨nh PTIT - L·∫≠p tr√¨nh t·ª´ tr√°i tim"]"""
            }
        ]
        # NOTE: C√°c em c√≥ th·ªÉ thay ƒë·ªïi content n·∫øu mu·ªën
        messages_entities.append({
            "role": "user",
            "content": f"C√¢u tr·∫£ l·ªùi: {reply}"
        })
        # G·ªçi  API ƒë·ªÉ tr√≠ch xu·∫•t c√°c th·ª±c th·ªÉ
        entities_str = get_llm_response(messages_entities)
        try:
            entities = ast.literal_eval(entities_str.strip())
            if not isinstance(entities, list):
                entities = []
        except Exception:
            # Fallback: extract first bracketed list and parse
            match = re.search(r'\[.*?\]', entities_str, re.DOTALL)
            if match:
                try:
                    entities = ast.literal_eval(match.group(0))
                except Exception:
                    entities = []
            else:
                entities = []
        tmp = len(entities)
        for result in results:
            context = result['information']
            for entity in entities:
                if entity.strip() in context:
                    hits += 1
                    entities.remove(entity.strip())
        total_recall += hits / tmp if tmp > 0 else 0
        print(f"Query {index+1}/{len(df_train)} - Total Recall: {total_recall:.3f}")
        time.sleep(1)

    return total_recall / len(df_train) if len(df_train) > 0 else 0



# H√†m t√≠nh to√°n t·∫•t c·∫£ metrics li√™n quan ƒë·∫øn Retrieval

def calculate_metrics_retrieval(file_clb_proptit, file_train , embedding, vector_db, train, reranker=None, use_query_expansion=True):
    # T·∫°o ra 1 b·∫£ng csv, c·ªôt th·ª© nh·∫•t l√† K value, c√°c c·ªôt c√≤n l·∫°i l√† metrics. S·∫Ω c√≥ 3 h√†ng t∆∞∆°ng tr∆∞ng v·ªõi k = 3, 5, 7
    k_values = [3, 5, 7]
    metrics = {
        "K": [],
        "hit@k": [],
        "recall@k": [],
        "precision@k": [],
        "f1@k": [],
        "map@k": [],
        "mrr@k": [],
        "ndcg@k": [],
        "context_precision@k": [],
        "context_recall@k": [],
        "context_entities_recall@k": []
    }
    # L∆∞u 2 ch·ªØ s·ªë th·∫≠p ph√¢n cho c√°c metrics
    for k in k_values:
        metrics["K"].append(k)
        metrics["hit@k"].append(round(hit_k(file_clb_proptit, file_train, embedding, vector_db, reranker, k, use_query_expansion), 2))
        metrics["recall@k"].append(round(recall_k(file_clb_proptit, file_train, embedding, vector_db, reranker, k, use_query_expansion), 2))
        metrics["precision@k"].append(round(precision_k(file_clb_proptit, file_train, embedding, vector_db, reranker, k, use_query_expansion), 2))
        metrics["f1@k"].append(round(f1_k(file_clb_proptit, file_train, embedding, vector_db, reranker, k, use_query_expansion), 2))
        metrics["map@k"].append(round(map_k(file_clb_proptit, file_train, embedding, vector_db, reranker, k, use_query_expansion), 2))
        metrics["mrr@k"].append(round(mrr_k(file_clb_proptit, file_train, embedding, vector_db, reranker, k, use_query_expansion), 2))
        metrics["ndcg@k"].append(round(ndcg_k(file_clb_proptit, file_train, embedding, vector_db, reranker, k, use_query_expansion), 2))
        metrics["context_precision@k"].append(round(context_precision_k(file_clb_proptit, file_train, embedding, vector_db, reranker, k, use_query_expansion), 2))
        metrics["context_recall@k"].append(round(context_recall_k(file_clb_proptit, file_train, embedding, vector_db, reranker, k, use_query_expansion), 2))
        metrics["context_entities_recall@k"].append(round(context_entities_recall_k(file_clb_proptit, file_train, embedding, vector_db, reranker, k, use_query_expansion), 2))
    # Chuy·ªÉn ƒë·ªïi metrics th√†nh DataFrame
    metrics_df = pd.DataFrame(metrics)
    # L∆∞u DataFrame v√†o file csv
    filename_suffix = "_with_query_expansion" if use_query_expansion else "_baseline"
    if train:
        metrics_df.to_csv(f"metrics_retrieval_train{filename_suffix}.csv", index=False)
    else:
        metrics_df.to_csv(f"metrics_retrieval_test{filename_suffix}.csv", index=False)
    return metrics_df

# C√°c h√†m ƒë√°nh gi√° LLM Answer
def get_contexts(query, embedding, vector_db, reranker=None, use_query_expansion=True, k=5):
    """Retrieve contexts using query expansion and optional reranking."""
    expander = QueryExpansion()
    # Expand queries if enabled
    queries = expander.combined_llm_expansion(query) if use_query_expansion else [query]
    all_results = []
    for q in queries:
        emb = embedding.encode(q)
        all_results.extend(vector_db.query("information", emb, limit=k))
    # Rerank passages if reranker is provided
    if reranker:
        passages = [r["information"] for r in all_results]
        _, reranked = reranker(query, passages)
        return [{"information": p} for p in reranked[:k]]
    return all_results[:k]
# H√†m String Presence

def string_presence_k(file_clb_proptit, file_train, embedding, vector_db, k=5, reranker=None, use_query_expansion=True):
    df_clb = pd.read_csv(file_clb_proptit)
    df_train = pd.read_excel(file_train)

    total_presence = 0

    for index, row in df_train.iterrows():
        hits = 0
        query = row['Query']
        # T·∫°o embedding cho c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng
        user_embedding = embedding.encode(query)

        # Retrieve contexts with optional query expansion and reranking
        results = get_contexts(query, embedding, vector_db, reranker, use_query_expansion, k)
        reply = row['Ground truth answer']
        messages = [
            {
                "role": "system",
                "content": """B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n cung c·∫•p th√¥ng tin v·ªÅ C√¢u l·∫°c b·ªô L·∫≠p tr√¨nh ProPTIT.
B·∫°n s·∫Ω nh·∫≠n ƒë∆∞·ª£c d·ªØ li·ªáu ng·ªØ c·∫£nh (context) t·ª´ m·ªôt h·ªá th·ªëng Retrieval-Augmented Generation (RAG) ch·ª©a c√°c th√¥ng tin ch√≠nh x√°c v·ªÅ CLB.

Nguy√™n t·∫Øc tr·∫£ l·ªùi:
1. Ch·ªâ s·ª≠ d·ª•ng th√¥ng tin t·ª´ context ƒë∆∞·ª£c cung c·∫•p ƒë·ªÉ tr·∫£ l·ªùi. Context s·∫Ω ƒë∆∞·ª£c cung c·∫•p ·ªü ƒë·∫ßu m·ªói query c·ªßa ng∆∞·ªùi d√πng. C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng n·∫±m ·ªü cu·ªëi. 
2. Tr√¨nh b√†y c√¢u tr·∫£ l·ªùi r√µ r√†ng, d·ªÖ hi·ªÉu. C√≥ th·ªÉ s·ª≠ d·ª•ng emoij icon khi c·∫ßn.
3. Tuy·ªát ƒë·ªëi kh√¥ng suy ƒëo√°n ho·∫∑c b·ªãa th√¥ng tin.
4. Gi·ªØ phong c√°ch tr·∫£ l·ªùi th√¢n thi·ªán, chuy√™n nghi·ªáp v√† nh·∫•t qu√°n.
5. Trong context c√≥ th·ªÉ ch·ª©a nhi·ªÅu th√¥ng tin kh√°c nhau, h√£y t·∫≠p trung v√†o c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng ƒë·ªÉ tr·∫£ l·ªùi ch√≠nh x√°c nh·∫•t.

Nhi·ªám v·ª• c·ªßa b·∫°n:
- Tr·∫£ l·ªùi c√°c c√¢u h·ªèi v·ªÅ CLB L·∫≠p tr√¨nh ProPTIT: l·ªãch s·ª≠, th√†nh vi√™n, ho·∫°t ƒë·ªông, s·ª± ki·ªán, d·ª± √°n, n·ªôi quy, th√†nh vi√™n ti√™u bi·ªÉu, v√† c√°c th√¥ng tin li√™n quan kh√°c."""
            }
        ]
        context = "Content t·ª´ c√°c t√†i li·ªáu li√™n quan:\n"
        context += "\n".join([result["information"] for result in results])
        # Th√™m context v√†o messages
        messages.append({
            "role": "user",
            "content": context + "\n\nC√¢u h·ªèi: " + query
        })
        # G·ªçi  API ƒë·ªÉ l·∫•y c√¢u tr·∫£ l·ªùi
        response = get_llm_response(messages)
        # Tr√≠ch xu·∫•t c√°c th·ª±c th·ªÉ t·ª´ c√¢u tr·∫£ l·ªùi b·∫±ng LLM
        # NOTE: C√°c em c√≥ th·ªÉ thay ƒë·ªïi message_entities n·∫øu mu·ªën
        messages_entities = [
            {
                "role": "system",
                "content": """B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n tr√≠ch xu·∫•t c√°c th·ª±c th·ªÉ t·ª´ c√¢u tr·∫£ l·ªùi. B·∫°n s·∫Ω ƒë∆∞·ª£c cung c·∫•p m·ªôt c√¢u tr·∫£ l·ªùi v√† nhi·ªám v·ª• c·ªßa b·∫°n l√† tr√≠ch xu·∫•t c√°c th·ª±c th·ªÉ t·ª´ c√¢u tr·∫£ l·ªùi ƒë√≥. C√°c th·ª±c th·ªÉ c√≥ th·ªÉ l√† t√™n ng∆∞·ªùi, ƒë·ªãa ƒëi·ªÉm, t·ªï ch·ª©c, s·ª± ki·ªán, v.v. H√£y tr·∫£ l·ªùi d∆∞·ªõi d·∫°ng m·ªôt danh s√°ch c√°c th·ª±c th·ªÉ.
                V√≠ d·ª•:
                C√¢u tr·∫£ l·ªùi: N·∫øu b·∫°n thu·ªôc ng√†nh kh√°c b·∫°n v·∫´n c√≥ th·ªÉ tham gia CLB ch√∫ng m√¨nh. N·∫øu ƒë·ªãnh h∆∞·ªõng c·ªßa b·∫°n ho√†n to√†n l√† theo CNTT th√¨ CLB ch·∫Øc ch·∫Øn l√† n∆°i ph√π h·ª£p nh·∫•t ƒë·ªÉ c√°c b·∫°n ph√°t tri·ªÉn. Tr·ªü ng·∫°i l·ªõn nh·∫•t s·∫Ω l√† do b·∫°n theo m·ªôt h∆∞·ªõng kh√°c n·ªØa n√™n s·∫Ω ph·∫£i t·∫≠p trung v√†o c·∫£ 2 m·∫£ng n√™n s·∫Ω c·∫ßn c·ªë g·∫Øng nhi·ªÅu h∆°n.
                ["ng√†nh kh√°c", "CLB", "CNTT", "m·∫£ng]
                C√¢u tr·∫£ l·ªùi: C√¢u l·∫°c b·ªô L·∫≠p Tr√¨nh PTIT (Programming PTIT), t√™n vi·∫øt t·∫Øt l√† PROPTIT ƒë∆∞·ª£c th√†nh l·∫≠p ng√†y 9/10/2011. V·ªõi ph∆∞∆°ng ch√¢m ho·∫°t ƒë·ªông "Chia s·∫ª ƒë·ªÉ c√πng nhau ph√°t tri·ªÉn", c√¢u l·∫°c b·ªô l√† n∆°i giao l∆∞u, ƒë√†o t·∫°o c√°c m√¥n l·∫≠p tr√¨nh v√† c√°c m√¥n h·ªçc trong tr∆∞·ªùng, t·∫°o ƒëi·ªÅu ki·ªán ƒë·ªÉ sinh vi√™n trong H·ªçc vi·ªán c√≥ m√¥i tr∆∞·ªùng h·ªçc t·∫≠p nƒÉng ƒë·ªông s√°ng t·∫°o. Slogan: L·∫≠p Tr√¨nh PTIT - L·∫≠p tr√¨nh t·ª´ tr√°i tim.
                ["C√¢u l·∫°c b·ªô L·∫≠p Tr√¨nh PTIT (Programming PTIT)", "PROPTIT", "9/10/2011", "Chia s·∫ª ƒë·ªÉ c√πng nhau ph√°t tri·ªÉn", "sinh vi√™n", "H·ªçc vi·ªán", "L·∫≠p Tr√¨nh PTIT - L·∫≠p tr√¨nh t·ª´ tr√°i tim"]"""
            }
        ]
        # Thay ƒë·ªïi content n·∫øu mu·ªën
        messages_entities.append({
            "role": "user",
            "content": f"C√¢u tr·∫£ l·ªùi: {reply}"
        })
        # G·ªçi  API ƒë·ªÉ tr√≠ch xu·∫•t c√°c th·ª±c th·ªÉ
        entities_str = get_llm_response(messages_entities)
        # Tr√≠ch xu·∫•t danh s√°ch ·ªü ƒë·∫ßu ph·∫£n h·ªìi
        match = re.search(r'\[.*?\]', entities_str)
        if match:
            try:
                entities = ast.literal_eval(match.group())
            except Exception:
                entities = []
        else:
            entities = []
        for entity in entities:
            if entity.strip() in response:
                hits += 1
        hits /= len(entities) if len(entities) > 0 else 0
        print(f"Query {index+1}/{len(df_train)} - Entities found: {hits * len(entities) if len(entities) > 0 else 0} / {len(entities)} - Presence: {hits:.3f}")
        total_presence += hits
    return total_presence / len(df_train) if len(df_train) > 0 else 0


 

# H√†m Rouge-L

from rouge import Rouge
def rouge_l_k(file_clb_proptit, file_train, embedding, vector_db, k=5, reranker=None, use_query_expansion=True):
    df_clb = pd.read_csv(file_clb_proptit)
    df_train = pd.read_excel(file_train)

    rouge = Rouge()
    total_rouge_l = 0

    for index, row in df_train.iterrows():
        query = row['Query']
        # Retrieve contexts with optional reranking and query expansion
        results = get_contexts(query, embedding, vector_db, reranker, use_query_expansion, k)
        reply = row['Ground truth answer']
        messages = [
            {
                "role": "system",
                "content": """B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n cung c·∫•p th√¥ng tin v·ªÅ C√¢u l·∫°c b·ªô L·∫≠p tr√¨nh ProPTIT.
B·∫°n s·∫Ω nh·∫≠n ƒë∆∞·ª£c d·ªØ li·ªáu ng·ªØ c·∫£nh (context) t·ª´ m·ªôt h·ªá th·ªëng Retrieval-Augmented Generation (RAG) ch·ª©a c√°c th√¥ng tin ch√≠nh x√°c v·ªÅ CLB.

Nguy√™n t·∫Øc tr·∫£ l·ªùi:
1. Ch·ªâ s·ª≠ d·ª•ng th√¥ng tin t·ª´ context ƒë∆∞·ª£c cung c·∫•p ƒë·ªÉ tr·∫£ l·ªùi. Context s·∫Ω ƒë∆∞·ª£c cung c·∫•p ·ªü ƒë·∫ßu m·ªói query c·ªßa ng∆∞·ªùi d√πng. C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng n·∫±m ·ªü cu·ªëi. 
2. N·∫øu ng∆∞·ªùi d√πng h·ªèi c√¢u h·ªèi kh√¥ng li√™n quan ƒë·∫øn CLB ProPTIT, h√£y tr·∫£ l·ªùi nh∆∞ b√¨nh th∆∞·ªùng, nh∆∞ng kh√¥ng s·ª≠ d·ª•ng th√¥ng tin t·ª´ context
3. Tr√¨nh b√†y c√¢u tr·∫£ l·ªùi r√µ r√†ng, d·ªÖ hi·ªÉu. C√≥ th·ªÉ s·ª≠ d·ª•ng emoij icon khi c·∫ßn.
4. Tuy·ªát ƒë·ªëi kh√¥ng suy ƒëo√°n ho·∫∑c b·ªãa th√¥ng tin.
5. Gi·ªØ phong c√°ch tr·∫£ l·ªùi th√¢n thi·ªán, chuy√™n nghi·ªáp v√† nh·∫•t qu√°n.
6. Trong context c√≥ th·ªÉ ch·ª©a nhi·ªÅu th√¥ng tin kh√°c nhau, h√£y t·∫≠p trung v√†o c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng ƒë·ªÉ tr·∫£ l·ªùi ch√≠nh x√°c nh·∫•t.

Nhi·ªám v·ª• c·ªßa b·∫°n:
- Tr·∫£ l·ªùi c√°c c√¢u h·ªèi v·ªÅ CLB L·∫≠p tr√¨nh ProPTIT: l·ªãch s·ª≠, th√†nh vi√™n, ho·∫°t ƒë·ªông, s·ª± ki·ªán, d·ª± √°n, n·ªôi quy, th√†nh vi√™n ti√™u bi·ªÉu, v√† c√°c th√¥ng tin li√™n quan kh√°c."""
            }
        ]
        context = "Content t·ª´ c√°c t√†i li·ªáu li√™n quan:\n"
        context += "\n".join([result["information"] for result in results])

        # Th√™m context v√†o messages
        messages.append({
            "role": "user",
            "content": context + "\n\nC√¢u h·ªèi: " + query
        })
        # G·ªçi API ƒë·ªÉ l·∫•y c√¢u tr·∫£ l·ªùi
        response = get_llm_response(messages)
        try:
            scores = rouge.get_scores(response, reply)
            rouge_l = scores[0]['rouge-l']['f']
        except ValueError:
            rouge_l = 0.0
            print(f"Query {index+1}/{len(df_train)} - Error computing Rouge-L, set to {rouge_l:.3f}")
        total_rouge_l += rouge_l
        print(f"Query {index+1}/{len(df_train)} - Rouge-L: {rouge_l:.3f}")
    return total_rouge_l / len(df_train) if len(df_train) > 0 else 0

# H√†m BLEU-4
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
def bleu_4_k(file_clb_proptit, file_train, embedding, vector_db, k=5, reranker=None, use_query_expansion=True):
    df_clb = pd.read_csv(file_clb_proptit)
    df_train = pd.read_excel(file_train)

    total_bleu_4 = 0
    smoothing_function = SmoothingFunction().method1

    for index, row in df_train.iterrows():
        query = row['Query']
        # Retrieve contexts with optional reranking and query expansion
        results = get_contexts(query, embedding, vector_db, reranker, use_query_expansion, k)
        reply = row['Ground truth answer']
        messages = [
            {
                "role": "system",
                "content": """B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n cung c·∫•p th√¥ng tin v·ªÅ C√¢u l·∫°c b·ªô L·∫≠p tr√¨nh ProPTIT.
B·∫°n s·∫Ω nh·∫≠n ƒë∆∞·ª£c d·ªØ li·ªáu ng·ªØ c·∫£nh (context) t·ª´ m·ªôt h·ªá th·ªëng Retrieval-Augmented Generation (RAG) ch·ª©a c√°c th√¥ng tin ch√≠nh x√°c v·ªÅ CLB.

Nguy√™n t·∫Øc tr·∫£ l·ªùi:
1. Ch·ªâ s·ª≠ d·ª•ng th√¥ng tin t·ª´ context ƒë∆∞·ª£c cung c·∫•p ƒë·ªÉ tr·∫£ l·ªùi. Context s·∫Ω ƒë∆∞·ª£c cung c·∫•p ·ªü ƒë·∫ßu m·ªói query c·ªßa ng∆∞·ªùi d√πng. C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng n·∫±m ·ªü cu·ªëi. 
2. N·∫øu ng∆∞·ªùi d√πng h·ªèi c√¢u h·ªèi kh√¥ng li√™n quan ƒë·∫øn CLB ProPTIT, h√£y tr·∫£ l·ªùi nh∆∞ b√¨nh th∆∞·ªùng, nh∆∞ng kh√¥ng s·ª≠ d·ª•ng th√¥ng tin t·ª´ context
3. Tr√¨nh b√†y c√¢u tr·∫£ l·ªùi r√µ r√†ng, d·ªÖ hi·ªÉu. C√≥ th·ªÉ s·ª≠ d·ª•ng emoij icon khi c·∫ßn.
4. Tuy·ªát ƒë·ªëi kh√¥ng suy ƒëo√°n ho·∫∑c b·ªãa th√¥ng tin.
5. Gi·ªØ phong c√°ch tr·∫£ l·ªùi th√¢n thi·ªán, chuy√™n nghi·ªáp v√† nh·∫•t qu√°n.
6. Trong context c√≥ th·ªÉ ch·ª©a nhi·ªÅu th√¥ng tin kh√°c nhau, h√£y t·∫≠p trung v√†o c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng ƒë·ªÉ tr·∫£ l·ªùi ch√≠nh x√°c nh·∫•t.

Nhi·ªám v·ª• c·ªßa b·∫°n:
- Tr·∫£ l·ªùi c√°c c√¢u h·ªèi v·ªÅ CLB L·∫≠p tr√¨nh ProPTIT: l·ªãch s·ª≠, th√†nh vi√™n, ho·∫°t ƒë·ªông, s·ª± ki·ªán, d·ª± √°n, n·ªôi quy, th√†nh vi√™n ti√™u bi·ªÉu, v√† c√°c th√¥ng tin li√™n quan kh√°c."""
            }
        ]
        context = "Content t·ª´ c√°c t√†i li·ªáu li√™n quan:\n"
        context += "\n".join([result["information"] for result in results])

        # S·ª≠a content n·∫øu mu·ªën
        messages.append({
            "role": "user",
            "content": context + "\n\nC√¢u h·ªèi: " + query
        })
        # G·ªçi  API ƒë·ªÉ l·∫•y c√¢u tr·∫£ l·ªùi
        response = get_llm_response(messages)
        reference = reply.split()
        candidate = response.split()
        bleu_4 = sentence_bleu([reference], candidate, smoothing_function=smoothing_function)
        total_bleu_4 += bleu_4
        print(bleu_4)
    return total_bleu_4 / len(df_train) if len(df_train) > 0 else 0

# H√†m Groundedness (LLM Answer - Hallucination Detection)\

def groundedness_k(file_clb_proptit, file_train, embedding, vector_db, k=5, reranker=None, use_query_expansion=True):
    df_clb = pd.read_csv(file_clb_proptit)
    df_train = pd.read_excel(file_train)

    total_groundedness = 0

    for index, row in df_train.iterrows():
        hits = 0
        cnt = 0
        query = row['Query']
        # Retrieve contexts with optional reranking and query expansion
        results = get_contexts(query, embedding, vector_db, reranker, use_query_expansion, k)
        reply = row['Ground truth answer']
        messages = [
            {
                "role": "system",
                "content": """B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n cung c·∫•p th√¥ng tin v·ªÅ C√¢u l·∫°c b·ªô L·∫≠p tr√¨nh ProPTIT.
B·∫°n s·∫Ω nh·∫≠n ƒë∆∞·ª£c d·ªØ li·ªáu ng·ªØ c·∫£nh (context) t·ª´ m·ªôt h·ªá th·ªëng Retrieval-Augmented Generation (RAG) ch·ª©a c√°c th√¥ng tin ch√≠nh x√°c v·ªÅ CLB.

Nguy√™n t·∫Øc tr·∫£ l·ªùi:
1. Ch·ªâ s·ª≠ d·ª•ng th√¥ng tin t·ª´ context ƒë∆∞·ª£c cung c·∫•p ƒë·ªÉ tr·∫£ l·ªùi. Context s·∫Ω ƒë∆∞·ª£c cung c·∫•p ·ªü ƒë·∫ßu m·ªói query c·ªßa ng∆∞·ªùi d√πng. C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng n·∫±m ·ªü cu·ªëi. 
2. N·∫øu ng∆∞·ªùi d√πng h·ªèi c√¢u h·ªèi kh√¥ng li√™n quan ƒë·∫øn CLB ProPTIT, h√£y tr·∫£ l·ªùi nh∆∞ b√¨nh th∆∞·ªùng, nh∆∞ng kh√¥ng s·ª≠ d·ª•ng th√¥ng tin t·ª´ context
3. Tr√¨nh b√†y c√¢u tr·∫£ l·ªùi r√µ r√†ng, d·ªÖ hi·ªÉu. C√≥ th·ªÉ s·ª≠ d·ª•ng emoij icon khi c·∫ßn.
4. Tuy·ªát ƒë·ªëi kh√¥ng suy ƒëo√°n ho·∫∑c b·ªãa th√¥ng tin.
5. Gi·ªØ phong c√°ch tr·∫£ l·ªùi th√¢n thi·ªán, chuy√™n nghi·ªáp v√† nh·∫•t qu√°n.
6. Trong context c√≥ th·ªÉ ch·ª©a nhi·ªÅu th√¥ng tin kh√°c nhau, h√£y t·∫≠p trung v√†o c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng ƒë·ªÉ tr·∫£ l·ªùi ch√≠nh x√°c nh·∫•t.

Nhi·ªám v·ª• c·ªßa b·∫°n:
- Tr·∫£ l·ªùi c√°c c√¢u h·ªèi v·ªÅ CLB L·∫≠p tr√¨nh ProPTIT: l·ªãch s·ª≠, th√†nh vi√™n, ho·∫°t ƒë·ªông, s·ª± ki·ªán, d·ª± √°n, n·ªôi quy, th√†nh vi√™n ti√™u bi·ªÉu, v√† c√°c th√¥ng tin li√™n quan kh√°c."""
            }
        ]
        context = "Content t·ª´ c√°c t√†i li·ªáu li√™n quan:\n"
        context += "\n".join([result["information"] for result in results])

        # Th√™m context v√†o messages, s·ª≠a content n·∫øu mu·ªën
        messages.append({
            "role": "user",
            "content": context + "\n\nC√¢u h·ªèi: " + query
        })
        # G·ªçi  API ƒë·ªÉ l·∫•y c√¢u tr·∫£ l·ªùi
        response = get_llm_response(messages)
      
    
        # T√°ch response th√†nh c√°c c√¢u
        sentences = response.split('. ')
        for sentence in sentences:
            # T·∫°o m·ªôt prompt ƒë·ªÉ ki·ªÉm tra t√≠nh groundedness c·ªßa c√¢u
            # S·ª≠a prompt n·∫øu mu·ªën
            messages_groundedness = [
                {
                    "role": "system",
                    "content": """B·∫°n l√† m·ªôt chuy√™n gia ƒë√°nh gi√° Groundedness trong h·ªá th·ªëng RAG, c√≥ nhi·ªám v·ª• ph√¢n lo·∫°i t·ª´ng c√¢u c·ªßa c√¢u tr·∫£ l·ªùi d·ª±a tr√™n ng·ªØ c·∫£nh ƒë√£ cho.
                    B·∫°n s·∫Ω ƒë∆∞·ª£c cung c·∫•p m·ªôt ng·ªØ c·∫£nh, m·ªôt c√¢u h·ªèi v√† m·ªôt c√¢u trong ph·∫ßn tr·∫£ l·ªùi t·ª´ m√¥ h√¨nh AI. Nhi·ªám v·ª• c·ªßa b·∫°n l√† ƒë√°nh gi√° c√¢u tr·∫£ l·ªùi ƒë√≥ d·ª±a tr√™n ng·ªØ c·∫£nh v√† c√¢u h·ªèi.
                    Input:
                    Question: C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng
                    Contexts: M·ªôt ho·∫∑c nhi·ªÅu ƒëo·∫°n vƒÉn b·∫£n ƒë∆∞·ª£c truy xu·∫•t
                    Answer: Ch·ªâ m·ªôt c√¢u trong ƒëo·∫°n vƒÉn b·∫£n LLM sinh ra
                    B·∫°n h√£y ƒë√°nh gi√° d·ª±a tr√™n c√°c nh√£n sau: 
                    supported: N·ªôi dung c√¢u ƒë∆∞·ª£c ng·ªØ c·∫£nh h·ªó tr·ª£ ho·∫∑c suy ra tr·ª±c ti·∫øp.
                    unsupported: N·ªôi dung c√¢u kh√¥ng ƒë∆∞·ª£c ng·ªØ c·∫£nh h·ªó tr·ª£, v√† kh√¥ng th·ªÉ suy ra t·ª´ ƒë√≥.
                    contradictory: N·ªôi dung c√¢u tr√°i ng∆∞·ª£c ho·∫∑c m√¢u thu·∫´n v·ªõi ng·ªØ c·∫£nh.
                    no_rad: C√¢u kh√¥ng y√™u c·∫ßu ki·ªÉm tra th·ª±c t·∫ø (v√≠ d·ª•: c√¢u ch√†o h·ªèi, √Ω ki·∫øn c√° nh√¢n, c√¢u h·ªèi tu t·ª´, disclaimers).
                    H√£y tr·∫£ l·ªùi b·∫±ng m·ªôt trong c√°c nh√£n tr√™n, kh√¥ng gi·∫£i th√≠ch g√¨ th√™m. Ch·ªâ tr·∫£ l·ªùi m·ªôt t·ª´ duy nh·∫•t l√† nh√£n ƒë√≥.
                    V√≠ d·ª•:
                    Question: B·∫°n c√≥ th·ªÉ cho t√¥i bi·∫øt v·ªÅ l·ªãch s·ª≠ c·ªßa C√¢u l·∫°c b·ªô L·∫≠p tr√¨nh ProPTIT kh√¥ng?
                    Contexts: C√¢u l·∫°c b·ªô L·∫≠p tr√¨nh ProPTIT ƒë∆∞·ª£c ra ƒë·ªùi v√†o nƒÉm 2011, v·ªõi m·ª•c ti√™u t·∫°o ra m·ªôt m√¥i tr∆∞·ªùng h·ªçc t·∫≠p v√† giao l∆∞u cho c√°c sinh vi√™n ƒëam m√™ l·∫≠p tr√¨nh.
                    Answer: C√¢u l·∫°c b·ªô L·∫≠p tr√¨nh ProPTIT ƒë∆∞·ª£c th√†nh l·∫≠p v√†o nƒÉm 2011.
                    supported"""
                }
            ]
            # S·ª≠a content n·∫øu mu·ªën
            messages_groundedness.append({
                "role": "user",
                "content": f"Question: {query}\n\nContexts: {context}\n\nAnswer: {sentence.strip()}"
            })
            # G·ªçi  API ƒë·ªÉ ƒë√°nh gi√° groundedness
            groundedness_reply = get_llm_response(messages_groundedness)

            if groundedness_reply == "supported":
                hits += 1
                cnt += 1
            elif groundedness_reply == "unsupported" or groundedness_reply == "contradictory":
                cnt += 1
        total_groundedness += hits / cnt if cnt > 0 else 1
        print(f"Query {index+1}/{len(df_train)} - Groundedness: {hits}/{cnt} - Total: {total_groundedness:.3f}")
    return total_groundedness / len(df_train) if len(df_train) > 0 else 0 

# H√†m Response Relevancy (LLM Answer - Measures relevance)


def generate_related_questions(response, embedding):
    # S·ª≠a system prompt n·∫øu mu·ªën
    messages_related = [
        {
            "role": "system",
            "content": """B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n t·∫°o ra c√°c c√¢u h·ªèi li√™n quan t·ª´ m·ªôt c√¢u tr·∫£ l·ªùi. B·∫°n s·∫Ω ƒë∆∞·ª£c cung c·∫•p m·ªôt c√¢u tr·∫£ l·ªùi v√† nhi·ªám v·ª• c·ªßa b·∫°n l√† t·∫°o ra c√°c c√¢u h·ªèi li√™n quan ƒë·∫øn c√¢u tr·∫£ l·ªùi ƒë√≥. H√£y t·∫°o ra √≠t nh·∫•t 5 c√¢u h·ªèi li√™n quan, m·ªói c√¢u h·ªèi n√™n ng·∫Øn g·ªçn v√† r√µ r√†ng. Tr·∫£ l·ªùi d∆∞·ªõi d·∫°ng list c√°c c√¢u h·ªèi nh∆∞ ·ªü v√≠ d·ª• d∆∞·ªõi. L∆ØU √ù: Tr·∫£ l·ªùi d∆∞·ªõi d·∫°ng ["c√¢u h·ªèi 1", "c√¢u h·ªèi 2", "c√¢u h·ªèi 3", ...], bao g·ªìm c·∫£ d·∫•u ngo·∫∑c vu√¥ng.
            V√≠ d·ª•:
            C√¢u tr·∫£ l·ªùi: C√¢u l·∫°c b·ªô L·∫≠p Tr√¨nh PTIT (Programming PTIT), t√™n vi·∫øt t·∫Øt l√† PROPTIT ƒë∆∞·ª£c th√†nh l·∫≠p ng√†y 9/10/2011. V·ªõi ph∆∞∆°ng ch√¢m ho·∫°t ƒë·ªông "Chia s·∫ª ƒë·ªÉ c√πng nhau ph√°t tri·ªÉn", c√¢u l·∫°c b·ªô l√† n∆°i giao l∆∞u, ƒë√†o t·∫°o c√°c m√¥n l·∫≠p tr√¨nh v√† c√°c m√¥n h·ªçc trong tr∆∞·ªùng, t·∫°o ƒëi·ªÅu ki·ªán ƒë·ªÉ sinh vi√™n trong H·ªçc vi·ªán c√≥ m√¥i tr∆∞·ªùng h·ªçc t·∫≠p nƒÉng ƒë·ªông s√°ng t·∫°o. Slogan: L·∫≠p Tr√¨nh PTIT - L·∫≠p tr√¨nh t·ª´ tr√°i tim.
            Output c·ªßa b·∫°n: "["CLB L·∫≠p Tr√¨nh PTIT ƒë∆∞·ª£c th√†nh l·∫≠p khi n√†o?", "Slogan c·ªßa CLB l√† g√¨?", "M·ª•c ti√™u c·ªßa CLB l√† g√¨?"]"
            C√¢u tr·∫£ l·ªùi: N·∫øu b·∫°n thu·ªôc ng√†nh kh√°c b·∫°n v·∫´n c√≥ th·ªÉ tham gia CLB ch√∫ng m√¨nh. N·∫øu ƒë·ªãnh h∆∞·ªõng c·ªßa b·∫°n ho√†n to√†n l√† theo CNTT th√¨ CLB ch·∫Øc ch·∫Øn l√† n∆°i ph√π h·ª£p nh·∫•t ƒë·ªÉ c√°c b·∫°n ph√°t tri·ªÉn. Tr·ªü ng·∫°i l·ªõn nh·∫•t s·∫Ω l√† do b·∫°n theo m·ªôt h∆∞·ªõng kh√°c n·ªØa n√™n s·∫Ω ph·∫£i t·∫≠p trung v√†o c·∫£ 2 m·∫£ng n√™n s·∫Ω c·∫ßn c·ªë g·∫Øng nhi·ªÅu h∆°n.
            Output c·ªßa b·∫°n: "["Ng√†nh n√†o c√≥ th·ªÉ tham gia CLB?", "CLB ph√π h·ª£p v·ªõi nh·ªØng ai?", "Tr·ªü ng·∫°i l·ªõn nh·∫•t khi tham gia CLB l√† g√¨?"]"""
        }
    ]
    # S·ª≠a content n·∫øu mu·ªën
    messages_related.append({
        "role": "user",
        "content": f"C√¢u tr·∫£ l·ªùi: {response}"
    })
    # G·ªçi  API ƒë·ªÉ t·∫°o ra c√°c c√¢u h·ªèi li√™n quan
    related_questions = get_llm_response(messages_related)
    return related_questions

def response_relevancy_k(file_clb_proptit, file_train, embedding, vector_db, k=5, reranker=None, use_query_expansion=True):
    df_clb = pd.read_csv(file_clb_proptit)
    df_train = pd.read_excel(file_train)

    total_relevancy = 0

    for index, row in df_train.iterrows():
        hits = 0
        cnt = 0
        query = row['Query']
        # Retrieve contexts with optional reranking and query expansion
        results = get_contexts(query, embedding, vector_db, reranker, use_query_expansion, k)
        reply = row['Ground truth answer']
        messages = [
            {
                "role": "system",
                "content": """B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n cung c·∫•p th√¥ng tin v·ªÅ C√¢u l·∫°c b·ªô L·∫≠p tr√¨nh ProPTIT.
B·∫°n s·∫Ω nh·∫≠n ƒë∆∞·ª£c d·ªØ li·ªáu ng·ªØ c·∫£nh (context) t·ª´ m·ªôt h·ªá th·ªëng Retrieval-Augmented Generation (RAG) ch·ª©a c√°c th√¥ng tin ch√≠nh x√°c v·ªÅ CLB.

Nguy√™n t·∫Øc tr·∫£ l·ªùi:
1. Ch·ªâ s·ª≠ d·ª•ng th√¥ng tin t·ª´ context ƒë∆∞·ª£c cung c·∫•p ƒë·ªÉ tr·∫£ l·ªùi. Context s·∫Ω ƒë∆∞·ª£c cung c·∫•p ·ªü ƒë·∫ßu m·ªói query c·ªßa ng∆∞·ªùi d√πng. C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng n·∫±m ·ªü cu·ªëi. 
2. N·∫øu ng∆∞·ªùi d√πng h·ªèi c√¢u h·ªèi kh√¥ng li√™n quan ƒë·∫øn CLB ProPTIT, h√£y tr·∫£ l·ªùi nh∆∞ b√¨nh th∆∞·ªùng, nh∆∞ng kh√¥ng s·ª≠ d·ª•ng th√¥ng tin t·ª´ context
3. Tr√¨nh b√†y c√¢u tr·∫£ l·ªùi r√µ r√†ng, d·ªÖ hi·ªÉu. C√≥ th·ªÉ s·ª≠ d·ª•ng emoij icon khi c·∫ßn.
4. Tuy·ªát ƒë·ªëi kh√¥ng suy ƒëo√°n ho·∫∑c b·ªãa th√¥ng tin.
5. Gi·ªØ phong c√°ch tr·∫£ l·ªùi th√¢n thi·ªán, chuy√™n nghi·ªáp v√† nh·∫•t qu√°n.
6. Trong context c√≥ th·ªÉ ch·ª©a nhi·ªÅu th√¥ng tin kh√°c nhau, h√£y t·∫≠p trung v√†o c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng ƒë·ªÉ tr·∫£ l·ªùi ch√≠nh x√°c nh·∫•t.

Nhi·ªám v·ª• c·ªßa b·∫°n:
- Tr·∫£ l·ªùi c√°c c√¢u h·ªèi v·ªÅ CLB L·∫≠p tr√¨nh ProPTIT: l·ªãch s·ª≠, th√†nh vi√™n, ho·∫°t ƒë·ªông, s·ª± ki·ªán, d·ª± √°n, n·ªôi quy, th√†nh vi√™n ti√™u bi·ªÉu, v√† c√°c th√¥ng tin li√™n quan kh√°c."""
            }
        ]
        context = "Content t·ª´ c√°c t√†i li·ªáu li√™n quan:\n"
        context += "\n".join([result["information"] for result in results])

        # S·ª≠a content n·∫øu mu·ªën
        messages.append({
            "role": "user",
            "content": context + "\n\nC√¢u h·ªèi: " + query
        })
        # G·ªçi  API ƒë·ªÉ l·∫•y c√¢u tr·∫£ l·ªùi
        user_embedding = embedding.encode(query)
        response = get_llm_response(messages)

        raw_related = generate_related_questions(response, embedding)
        related_questions = []
        if raw_related:
            raw_str = raw_related.strip()
            if raw_str.startswith('[') and raw_str.endswith('"'):
                raw_str = raw_str[:-1]
            try:
                related_questions = ast.literal_eval(raw_str)
            except Exception:
                related_questions = []
        for question in related_questions:
            question_embedding = embedding.encode(question)
            # T√≠nh score relevancy gi·ªØa c√¢u h·ªèi v√† query
            score = similarity(user_embedding, question_embedding)
            hits += score
        print(f"Query {index+1}/{len(df_train)} - Related questions generated: {len(related_questions)} - Total relevancy score: {hits:.3f}")
        total_relevancy += hits / len(related_questions) if len(related_questions) > 0 else 0
    return total_relevancy / len(df_train) if len(df_train) > 0 else 0


# H√†m Noise Sensitivity (LLM Answer - Robustness to Hallucination)

def noise_sensitivity_k(file_clb_proptit, file_train, embedding, vector_db, k=5, reranker=None, use_query_expansion=True):
    df_clb = pd.read_csv(file_clb_proptit)
    df_train = pd.read_excel(file_train)

    total_sensitivity = 0

    for index, row in df_train.iterrows():
        hits = 0
        cnt = 0
        query = row['Query']
        # Retrieve contexts with optional reranking and query expansion
        results = get_contexts(query, embedding, vector_db, reranker, use_query_expansion, k)
        reply = row['Ground truth answer']
        messages = [
            {
                "role": "system",
                "content": """B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n cung c·∫•p th√¥ng tin v·ªÅ C√¢u l·∫°c b·ªô L·∫≠p tr√¨nh ProPTIT.
B·∫°n s·∫Ω nh·∫≠n ƒë∆∞·ª£c d·ªØ li·ªáu ng·ªØ c·∫£nh (context) t·ª´ m·ªôt h·ªá th·ªëng Retrieval-Augmented Generation (RAG) ch·ª©a c√°c th√¥ng tin ch√≠nh x√°c v·ªÅ CLB.

Nguy√™n t·∫Øc tr·∫£ l·ªùi:
1. Ch·ªâ s·ª≠ d·ª•ng th√¥ng tin t·ª´ context ƒë∆∞·ª£c cung c·∫•p ƒë·ªÉ tr·∫£ l·ªùi. Context s·∫Ω ƒë∆∞·ª£c cung c·∫•p ·ªü ƒë·∫ßu m·ªói query c·ªßa ng∆∞·ªùi d√πng. C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng n·∫±m ·ªü cu·ªëi. 
2. N·∫øu ng∆∞·ªùi d√πng h·ªèi c√¢u h·ªèi kh√¥ng li√™n quan ƒë·∫øn CLB ProPTIT, h√£y tr·∫£ l·ªùi nh∆∞ b√¨nh th∆∞·ªùng, nh∆∞ng kh√¥ng s·ª≠ d·ª•ng th√¥ng tin t·ª´ context
3. Tr√¨nh b√†y c√¢u tr·∫£ l·ªùi r√µ r√†ng, d·ªÖ hi·ªÉu. C√≥ th·ªÉ s·ª≠ d·ª•ng emoij icon khi c·∫ßn.
4. Tuy·ªát ƒë·ªëi kh√¥ng suy ƒëo√°n ho·∫∑c b·ªãa th√¥ng tin.
5. Gi·ªØ phong c√°ch tr·∫£ l·ªùi th√¢n thi·ªán, chuy√™n nghi·ªáp v√† nh·∫•t qu√°n.
6. Trong context c√≥ th·ªÉ ch·ª©a nhi·ªÅu th√¥ng tin kh√°c nhau, h√£y t·∫≠p trung v√†o c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng ƒë·ªÉ tr·∫£ l·ªùi ch√≠nh x√°c nh·∫•t.

Nhi·ªám v·ª• c·ªßa b·∫°n:
- Tr·∫£ l·ªùi c√°c c√¢u h·ªèi v·ªÅ CLB L·∫≠p tr√¨nh ProPTIT: l·ªãch s·ª≠, th√†nh vi√™n, ho·∫°t ƒë·ªông, s·ª± ki·ªán, d·ª± √°n, n·ªôi quy, th√†nh vi√™n ti√™u bi·ªÉu, v√† c√°c th√¥ng tin li√™n quan kh√°c."""
            }
        ]
        context =  "Content t·ª´ c√°c t√†i li·ªáu li√™n quan:\n"
        context += "\n".join([result["information"] for result in results])

        # Th√™m context v√†o messages, s·ª≠a content n·∫øu mu·ªën
        messages.append({
            "role": "user",
            "content": context + "\n\nC√¢u h·ªèi: " + query
        })
        # G·ªçi  API ƒë·ªÉ l·∫•y c√¢u tr·∫£ l·ªùi
        response = get_llm_response(messages)

        sentences = response.split('. ')
        for sentence in sentences:
            # S·ª≠a prompt n·∫øu mu·ªën
            messages_sensitivity = [
                {
                    "role": "system",
                    "content": """B·∫°n l√† m·ªôt chuy√™n gia ƒë√°nh gi√° ƒë·ªô nh·∫°y c·∫£m c·ªßa c√¢u tr·∫£ l·ªùi trong h·ªá th·ªëng RAG, c√≥ nhi·ªám v·ª• ph√¢n lo·∫°i t·ª´ng c√¢u c·ªßa c√¢u tr·∫£ l·ªùi d·ª±a tr√™n ng·ªØ c·∫£nh ƒë√£ cho.
                    B·∫°n s·∫Ω ƒë∆∞·ª£c cung c·∫•p m·ªôt ng·ªØ c·∫£nh, m·ªôt c√¢u h·ªèi v√† m·ªôt c√¢u trong ph·∫ßn tr·∫£ l·ªùi t·ª´ m√¥ h√¨nh AI. Nhi·ªám v·ª• c·ªßa b·∫°n l√† ƒë√°nh gi√° c√¢u tr·∫£ l·ªùi ƒë√≥ d·ª±a tr√™n ng·ªØ c·∫£nh v√† c√¢u h·ªèi.
                    Input:
                    Question: C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng
                    Contexts: M·ªôt ho·∫∑c nhi·ªÅu ƒëo·∫°n vƒÉn b·∫£n ƒë∆∞·ª£c truy xu·∫•t
                    Answer: Ch·ªâ m·ªôt c√¢u trong ƒëo·∫°n vƒÉn b·∫£n LLM sinh ra
                    B·∫°n h√£y ƒë√°nh gi√° d·ª±a tr√™n c√°c nh√£n sau: 
                    1: N·ªôi dung c√¢u ƒë∆∞·ª£c ng·ªØ c·∫£nh h·ªó tr·ª£ ho·∫∑c suy ra tr·ª±c ti·∫øp.
                    0: N·ªôi dung c√¢u kh√¥ng ƒë∆∞·ª£c ng·ªØ c·∫£nh h·ªó tr·ª£, v√† kh√¥ng th·ªÉ suy ra t·ª´ ƒë√≥.
                    V√≠ d·ª•:
                    Question: B·∫°n c√≥ th·ªÉ cho t√¥i bi·∫øt v·ªÅ l·ªãch s·ª≠ c·ªßa C√¢u l·∫°c b·ªô L·∫≠p tr√¨nh ProPTIT kh√¥ng?
                    Contexts: C√¢u l·∫°c b·ªô L·∫≠p tr√¨nh ProPTIT ƒë∆∞·ª£c ra ƒë·ªùi v√†o nƒÉm 2011, v·ªõi m·ª•c ti√™u t·∫°o ra m·ªôt m√¥i tr∆∞·ªùng h·ªçc t·∫≠p v√† giao l∆∞u cho c√°c sinh vi√™n ƒëam m√™ l·∫≠p tr√¨nh.
                    Answer: C√¢u l·∫°c b·ªô L·∫≠p tr√¨nh ProPTIT ƒë∆∞·ª£c th√†nh l·∫≠p v√†o nƒÉm 2011.
                    1
                    Question: C√¢u l·∫°c b·ªô L·∫≠p tr√¨nh ProPTIT ƒë∆∞·ª£c th√†nh l·∫≠p v√†o nƒÉm 2011. B·∫°n c√≥ bi·∫øt ng√†y c·ª• th·ªÉ kh√¥ng?
                    Contexts: C√¢u l·∫°c b·ªô L·∫≠p tr√¨nh ProPTIT ƒë∆∞·ª£c ra ƒë·ªùi v√†o nƒÉm 2011, v·ªõi m·ª•c ti√™u t·∫°o ra m·ªôt m√¥i tr∆∞·ªùng h·ªçc t·∫≠p v√† giao l∆∞u cho c√°c sinh vi√™n ƒëam m√™ l·∫≠p tr√¨nh.
                    Answer: C√¢u l·∫°c b·ªô L·∫≠p tr√¨nh ProPTIT l√† CLB thu·ªôc PTIT.
                    0"""
                }
            ]
            # S·ª≠a prompt n·∫øu mu·ªën
            messages_sensitivity.append({
                "role": "user",
                "content": f"Question: {query}\n\nContexts: {context}\n\nAnswer: {sentence.strip()}"
            })
            # G·ªçi  API ƒë·ªÉ ƒë√°nh gi√° ƒë·ªô nh·∫°y c·∫£m
            sensitivity_reply = get_llm_response(messages_sensitivity)
            if sensitivity_reply == "0":
                hits += 1
        print(f"Query {index+1}/{len(df_train)} - Non-supported sentences: {hits} / {len(sentences)} - Noise Sensitivity: {hits / len(sentences) if len(sentences) > 0 else 0:.3f}")
        total_sensitivity += hits / len(sentences) if len(sentences) > 0 else 0
    return total_sensitivity / len(df_train) if len(df_train) > 0 else 0


# H√†m ƒë·ªÉ t√≠nh to√°n to√†n b·ªô metrics trong module LLM Answer

def calculate_metrics_llm_answer(file_clb_proptit, file_train, embedding, vector_db, train, reranker=None, use_query_expansion=True):
    # T·∫°o ra 1 b·∫£ng csv, c·ªôt th·ª© nh·∫•t l√† K value, c√°c c·ªôt c√≤n l·∫°i l√† metrics. S·∫Ω c√≥ 3 h√†ng t∆∞∆°ng tr∆∞ng v·ªõi k = 3, 5, 7
    k_values = [3, 5, 7]
    metrics = {
        "K": [],
        "string_presence@k": [],
        "rouge_l@k": [],
        "bleu_4@k": [],
        "groundedness@k": [],
        "response_relevancy@k": [],
        "noise_sensitivity@k": []
    }
    # L∆∞u 2 ch·ªØ s·ªë th·∫≠p ph√¢n cho c√°c metrics
    for k in k_values:
        metrics["K"].append(k)
        metrics["string_presence@k"].append(round(string_presence_k(file_clb_proptit, file_train, embedding, vector_db, k, reranker, use_query_expansion), 2))
        metrics["rouge_l@k"].append(round(rouge_l_k(file_clb_proptit, file_train, embedding, vector_db, k, reranker, use_query_expansion), 2))
        metrics["bleu_4@k"].append(round(bleu_4_k(file_clb_proptit, file_train, embedding, vector_db, k, reranker, use_query_expansion), 2))
        metrics["groundedness@k"].append(round(groundedness_k(file_clb_proptit, file_train, embedding, vector_db, k, reranker, use_query_expansion), 2))
        metrics["response_relevancy@k"].append(round(response_relevancy_k(file_clb_proptit, file_train, embedding, vector_db, k, reranker, use_query_expansion), 2))
        metrics["noise_sensitivity@k"].append(round(noise_sensitivity_k(file_clb_proptit, file_train, embedding, vector_db, k, reranker, use_query_expansion), 2))
    # Chuy·ªÉn ƒë·ªïi metrics th√†nh DataFrame
    metrics_df = pd.DataFrame(metrics)
    # L∆∞u DataFrame v√†o file csv
    if train:
        metrics_df.to_csv("metrics_llm_answer_train.csv", index=False)
    else:
        metrics_df.to_csv("metrics_llm_answer_test.csv", index=False)
    return metrics_df
