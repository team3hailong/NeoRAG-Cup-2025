import pandas as pd
import re
import os
import ast
from dotenv import load_dotenv
import requests
import time
from query_expansion import QueryExpansion
import torch

# Cache to store retrieval results and avoid redundant retrievals
_RETRIEVAL_CACHE = {}

COMMON_RAG_SYSTEM_PROMPT = """B·∫°n l√† m·ªôt tr·ª£ l√Ω AI th√¢n thi·ªán v√† khuy·∫øn kh√≠ch, r·∫•t hi·ªÉu v·ªÅ C√¢u l·∫°c b·ªô L·∫≠p tr√¨nh ProPTIT.
Nhi·ªám v·ª• c·ªßa b·∫°n l√† tr·∫£ l·ªùi tr·ª±c ti·∫øp c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng v·ªÅ ho·∫°t ƒë·ªông, th√†nh vi√™n, quy tr√¨nh training v√† c√°c quy·ªÅn l·ª£i, nghƒ©a v·ª• trong CLB.
H√£y d·ª±a ho√†n to√†n v√†o th√¥ng tin trong context ƒë√£ ƒë∆∞·ª£c cung c·∫•p, kh√¥ng th√™m ki·∫øn th·ª©c ngo√†i.
Tr·∫£ l·ªùi v·ªõi gi·ªçng ƒëi·ªáu th√¢n thi·ªán, nhi·ªát t√¨nh v√† c·ª• th·ªÉ. V√≠ d·ª•:
    - Cung c·∫•p th√¥ng tin chi ti·∫øt, ch√≠nh x√°c nh∆∞ v√≠ d·ª• m·∫´u.
    - N·∫øu context kh√¥ng c√≥ th√¥ng tin c·∫ßn thi·∫øt, n√≥i: "Th√¥ng tin n√†y kh√¥ng c√≥ trong t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p."
QUAN TR·ªåNG - Quy t·∫Øc tr√≠ch d·∫´n:
- Sao ch√©p CH√çNH X√ÅC c√°c con s·ªë, ng√†y th√°ng, t√™n ri√™ng t·ª´ context
- V√≠ d·ª•: "9/10/2011" KH√îNG vi·∫øt th√†nh "ng√†y 9 th√°ng 10 nƒÉm 2011"
- Gi·ªØ nguy√™n thu·∫≠t ng·ªØ: "PROPTIT" KH√îNG vi·∫øt "Pro PTIT"
- Gi·ªØ format chu·∫©n: "200 th√†nh vi√™n" KH√îNG vi·∫øt "hai trƒÉm th√†nh vi√™n"

V√≠ d·ª• ƒë·ªãnh d·∫°ng tr·∫£ l·ªùi (few-shot):
User Question: "Ti√™u ch√≠ ƒë√°nh gi√° trong giai ƒëo·∫°n training l√† g√¨, v√† n·∫øu em ch∆∞a gi·ªèi l·∫≠p tr√¨nh th√¨ em c√≥ th·ªÉ tham gia c√¢u l·∫°c b·ªô ƒë∆∞·ª£c kh√¥ng ?"
Document: "Trong v√≤ng training, c√°c anh ch·ªã s·∫Ω ƒë√°nh gi√° em v·ªÅ nhi·ªÅu m·∫∑t kh√°c nhau, bao g·ªìm c·∫£ m·∫£ng h·ªçc t·∫≠p, ho·∫°t ƒë·ªông v√† c√°ch giao ti·∫øp gi·ªØa em v·ªõi c√°c th√†nh vi√™n CLB kh√°c. Vi·ªác code ch·ªâ l√† 1 ph·∫ßn trong s·ªë ƒë√≥, em c·ªë g·∫Øng th·ªÉ hi·ªán h·∫øt m√¨nh l√† ƒë∆∞·ª£c nh√©, m·ªçi n·ªó l·ª±c em l√†m ƒë·ªÅu s·∫Ω ƒë∆∞·ª£c anh ch·ªã ghi nh·∫≠n v√† ƒë√°nh gi√°. Anh ch·ªã ƒë√°nh gi√° r·∫•t cao s·ª± ti·∫øn b·ªô c·ªßa c√°c em trong qu√° tr√¨nh training."
Answer: "Ch√†o em, trong v√≤ng training, c√°c anh ch·ªã s·∫Ω ƒë√°nh gi√° em v·ªÅ nhi·ªÅu m·∫∑t kh√°c nhau, bao g·ªìm c·∫£ m·∫£ng h·ªçc t·∫≠p, ho·∫°t ƒë·ªông v√† c√°ch giao ti·∫øp gi·ªØa em v·ªõi c√°c th√†nh vi√™n CLB kh√°c. Vi·ªác code ch·ªâ l√† m·ªôt ph·∫ßn trong s·ªë ƒë√≥ th√¥i, quan tr·ªçng l√† em c·ªë g·∫Øng th·ªÉ hi·ªán h·∫øt m√¨nh. M·ªçi n·ªó l·ª±c c·ªßa em ƒë·ªÅu s·∫Ω ƒë∆∞·ª£c anh ch·ªã ghi nh·∫≠n v√† ƒë√°nh gi√° cao. CLB r·∫•t mong ch·ªù s·ª± ti·∫øn b·ªô c·ªßa c√°c em trong qu√° tr√¨nh n√†y nh√©!"

User Question: "Khi tham gia CLB, th√†nh vi√™n s·∫Ω ƒë∆∞·ª£c h∆∞·ªüng nh·ªØng quy·ªÅn l·ª£i g√¨ v√† c·∫ßn th·ª±c hi·ªán nh·ªØng nghƒ©a v·ª• g√¨?"
Document: "Quy·ªÅn l·ª£i g·ªìm tham gia ho·∫°t ƒë·ªông h·ªçc t·∫≠p, d·ª± √°n, ·ª©ng c·ª≠ ‚Äì ƒë·ªÅ c·ª≠, v√† h·ªçc h·ªèi k·ªπ nƒÉng. Nghƒ©a v·ª• g·ªìm tham gia ƒë·∫ßy ƒë·ªß, ch·∫•p h√†nh n·ªôi quy, ho√†n th√†nh nhi·ªám v·ª•, ƒë√≥ng ph√≠ ƒë√∫ng h·∫°n v√† ƒë√≥ng g√≥p √Ω ki·∫øn x√¢y d·ª±ng CLB."
Answer: "Khi tham gia CLB, em s·∫Ω c√≥ r·∫•t nhi·ªÅu quy·ªÅn l·ª£i h·∫•p d·∫´n nh∆∞ ƒë∆∞·ª£c tham gia c√°c ho·∫°t ƒë·ªông h·ªçc t·∫≠p, d·ª± √°n, c√≥ c∆° h·ªôi ·ª©ng c·ª≠ - ƒë·ªÅ c·ª≠ v√†o c√°c v·ªã tr√≠ l√£nh ƒë·∫°o, v√† ƒë∆∞·ª£c h·ªçc h·ªèi th√™m nhi·ªÅu k·ªπ nƒÉng m·ªõi. B√™n c·∫°nh ƒë√≥, ƒë·ªÉ CLB ng√†y c√†ng ph√°t tri·ªÉn, em c≈©ng c·∫ßn th·ª±c hi·ªán m·ªôt s·ªë nghƒ©a v·ª• nh∆∞ tham gia ƒë·∫ßy ƒë·ªß c√°c bu·ªïi sinh ho·∫°t, ch·∫•p h√†nh n·ªôi quy, ho√†n th√†nh t·ªët nhi·ªám v·ª• ƒë∆∞·ª£c giao, ƒë√≥ng ph√≠ ƒë√∫ng h·∫°n v√† t√≠ch c·ª±c ƒë√≥ng g√≥p √Ω ki·∫øn x√¢y d·ª±ng CLB nh√©!"

User Question: "CLB c√≥ ho·∫°t ƒë·ªông mentoring cho th√†nh vi√™n m·ªõi kh√¥ng?"
Document: "C√≥, mentor s·∫Ω h∆∞·ªõng d·∫´n k·ªπ thu·∫≠t, gi·∫£i ƒë√°p th·∫Øc m·∫Øc v√† gi√∫p th√†nh vi√™n m·ªõi l√†m quen v·ªõi d·ª± √°n."
Answer: "Ch√†o em, CLB c√≥ ho·∫°t ƒë·ªông mentoring r·∫•t chu ƒë√°o cho th√†nh vi√™n m·ªõi ƒë√≥! Mentor c·ªßa CLB s·∫Ω h∆∞·ªõng d·∫´n em v·ªÅ k·ªπ thu·∫≠t, gi·∫£i ƒë√°p m·ªçi th·∫Øc m·∫Øc v√† gi√∫p em l√†m quen v·ªõi c√°c d·ª± √°n m·ªôt c√°ch nhanh ch√≥ng nh·∫•t."
"""

def retrieve_and_rerank(query, embedding, vector_db, reranker, k, use_query_expansion=True):
    cache_key = (query, k, use_query_expansion, bool(reranker))
    if cache_key in _RETRIEVAL_CACHE:
        return _RETRIEVAL_CACHE[cache_key]

    # Ch·ªçn retrieval function
    use_m3 = hasattr(embedding, 'use_colbert') and embedding.use_colbert
    if use_m3:
        from m3_retriever.m3_hybrid_retriever import create_m3_hybrid_retriever
        hybrid = create_m3_hybrid_retriever(embedding, vector_db)
        def retrieve_single(q, limit):
            return hybrid.retrieve_hybrid(q, limit)
    else:
        def retrieve_single(q, limit):
            emb = embedding.encode(q)
            vec = emb.get('dense_vecs') if isinstance(emb, dict) else emb
            return vector_db.query('information', vec, limit=limit, embedding_model=embedding)

    # M·ªü r·ªông truy v·∫•n n·∫øu c·∫ßn
    if use_query_expansion and k > 5:
        expander = QueryExpansion()
        queries = expander.expand_query(query, techniques=['synonym'], max_expansions=1)
    else:
        queries = [query]

    # G·ªôp k·∫øt qu·∫£ v·ªõi ƒëi·ªÉm s·ªë k·∫øt h·ª£p
    all_results = []
    seen = set()
    for i, q in enumerate(queries):
        weight = 1.0 if i == 0 else max(0.0, 0.7 - (i-1)*0.1)
        limit = (k if i == 0 else max(1, k//5)) * (2 if reranker else 1)
        candidates = retrieve_single(q, limit)
        for j, doc in enumerate(candidates):
            doc_id = doc.get('title', str(hash(doc.get('information', ''))))
            if doc_id in seen:
                continue
            seen.add(doc_id)
            score = doc.get('score', 1.0)
            combined = weight * (1.0 / (j+1)) * score
            doc.update({'combined_score': combined, 'expansion_weight': weight, 'source_query': q})
            all_results.append(doc)
    # S·∫Øp x·∫øp v√† l·∫•y top-k (ho·∫∑c top-2k n·∫øu reranker)
    all_results.sort(key=lambda x: x.get('combined_score', 0), reverse=True)
    pre_k = all_results[:k*2] if reranker else all_results[:k]

    # Rerank c√°c k·∫øt qu·∫£ n·∫øu c√≥ reranker
    if reranker and pre_k:
        passages = [doc['information'] for doc in pre_k]
        _, reranked_passages = reranker(query, passages)
        results = [doc for rp in reranked_passages[:k] for doc in pre_k if doc['information'] == rp]
    else:
        results = pre_k

    _RETRIEVAL_CACHE[cache_key] = results
    return results

load_dotenv()

from llm_config import get_llm_response, get_config_info

config_info = get_config_info()
print(f"ü§ñ LLM: {config_info['model']} ({config_info['provider']})\n")

# N√™n ch·∫°y t·ª´ng h√†m t·ª´ ƒëo·∫°n n√†y ƒë·ªÉ test

def hit_k(file_clb_proptit, file_train_data_proptit, embedding, vector_db, reranker=None, k=5, use_query_expansion=True):
    df_clb = pd.read_csv(file_clb_proptit)
    df_train = pd.read_excel(file_train_data_proptit)

    hits = 0
    total_queries = len(df_train)

    for index, row in df_train.iterrows():
        query = row['Query']
        ground_truth_doc = row['Ground truth document']
        # TODO: N·∫øu c√°c em d√πng Text2SQL RAG hay c√°c ph∆∞∆°ng ph√°p s·ª≠ d·ª•ng ng√¥n ng·ªØ truy v·∫•n, c√≥ th·ªÉ b·ªè qua bi·∫øn user_embedding
        # C√°c em c√≥ th·ªÉ d√πng c√°c kƒ© thu·∫≠t ƒë·ªÉ vi·∫øt l·∫°i c√¢u query, Reranking, ... ·ªü ƒëo·∫°n n√†y.
        # Retrieve top-k (with optional reranking and query expansion) using fast retrieval
        results = retrieve_and_rerank(query, embedding, vector_db, reranker, k, use_query_expansion)
         
        # L·∫•y danh s√°ch t√†i li·ªáu ƒë∆∞·ª£c truy su·∫•t
        retrieved_docs = [int(result['title'].split(' ')[-1]) for result in results]
        ground_truth_docs =  []
        if type(ground_truth_doc) is str:
            for doc in ground_truth_doc.split(","):
                ground_truth_docs.append(int(doc))
        else:
            ground_truth_docs.append(int(ground_truth_doc))
        if any(doc in retrieved_docs for doc in ground_truth_docs):
            hits += 1
    return hits / total_queries if total_queries > 0 else 0


# H√†m recall@k
def recall_k(file_clb_proptit, file_train, embedding, vector_db, reranker=None, k=5, use_query_expansion=True):
    df_clb = pd.read_csv(file_clb_proptit)
    df_train = pd.read_excel(file_train)

    
    ans = 0

    for index, row in df_train.iterrows():
        hits = 0
        query = row['Query']
        ground_truth_doc = row['Ground truth document']
        # TODO: N·∫øu c√°c em d√πng Text2SQL RAG hay c√°c ph∆∞∆°ng ph√°p s·ª≠ d·ª•ng ng√¥n ng·ªØ truy v·∫•n, c√≥ th·ªÉ b·ªè qua bi·∫øn user_embedding
        # C√°c em c√≥ th·ªÉ d√πng c√°c kƒ© thu·∫≠t ƒë·ªÉ vi·∫øt l·∫°i c√¢u query, Reranking, ... ·ªü ƒëo·∫°n n√†y.
        # Embedding c√¢u query
        # Retrieve top-k (with optional reranking and query expansion) using fast retrieval
        results = retrieve_and_rerank(query, embedding, vector_db, reranker, k, use_query_expansion)

        # L·∫•y danh s√°ch t√†i li·ªáu ƒë∆∞·ª£c truy su·∫•t
        retrieved_docs = [int(result['title'].split(' ')[-1]) for result in results]
        ground_truth_docs =  []
        if type(ground_truth_doc) is str:
            for doc in ground_truth_doc.split(","):
                ground_truth_docs.append(int(doc))
        else:
            ground_truth_docs.append(int(ground_truth_doc))
        if any(doc in retrieved_docs for doc in ground_truth_docs):
            hits = len([doc for doc in ground_truth_docs if doc in retrieved_docs])
        ans += hits / len(ground_truth_docs) 
    return ans / len(df_train)


# H√†m precision@k
def precision_k(file_clb_proptit, file_train, embedding, vector_db, reranker=None, k=5, use_query_expansion=True):
    df_clb = pd.read_csv(file_clb_proptit)
    df_train = pd.read_excel(file_train)

    ans = 0

    for index, row in df_train.iterrows():
        hits = 0
        query = row['Query']
        ground_truth_doc = row['Ground truth document']
        # T·∫°o embedding cho c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng
        user_embedding = embedding.encode(query)

        # Retrieve top-k (with optional reranking and query expansion)
        results = retrieve_and_rerank(query, embedding, vector_db, reranker, k, use_query_expansion)

        # L·∫•y danh s√°ch t√†i li·ªáu ƒë∆∞·ª£c truy su·∫•t
        retrieved_docs = [int(result['title'].split(' ')[-1]) for result in results]
        # print(f"Retrieved documents: {retrieved_docs}")
        ground_truth_docs =  []
        if type(ground_truth_doc) is str:
            for doc in ground_truth_doc.split(","):
                ground_truth_docs.append(int(doc))
        else:
            ground_truth_docs.append(int(ground_truth_doc))
        # print("Ground truth documents:", ground_truth_docs)
        # Ki·ªÉm tra xem c√≥ √≠t nh·∫•t m·ªôt t√†i li·ªáu ƒë√∫ng trong k·∫øt qu·∫£ t√¨m ki·∫øm
        if any(doc in retrieved_docs for doc in ground_truth_docs):
            hits = len([doc for doc in retrieved_docs if doc in ground_truth_docs])
        ans += hits / k 
        # print("Hits / k for this query:", hits / k)
    return ans / len(df_train)


# H√†m f1@k
def f1_k(file_clb_proptit, file_train, embedding, vector_db, reranker=None, k=5, use_query_expansion=True):
    precision = precision_k(file_clb_proptit, file_train, embedding, vector_db, reranker, k, use_query_expansion)
    recall = recall_k(file_clb_proptit, file_train, embedding, vector_db, reranker, k, use_query_expansion)
    if precision + recall == 0:
        return 0
    return 2 * (precision * recall) / (precision + recall)

# H√†m MAP@k

def map_k(file_clb_proptit, file_train, embedding, vector_db, reranker=None, k=5, use_query_expansion=True):
    df_clb = pd.read_csv(file_clb_proptit)
    df_train = pd.read_excel(file_train)

    total_map = 0

    for index, row in df_train.iterrows():
        hits = 0
        ap = 0
        query = row['Query']
        ground_truth_doc = row['Ground truth document']
        # T·∫°o embedding cho c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng
        # Retrieve top-k (with optional reranking and query expansion)
        results = retrieve_and_rerank(query, embedding, vector_db, reranker, k, use_query_expansion)

        # L·∫•y danh s√°ch t√†i li·ªáu ƒë∆∞·ª£c truy su·∫•t
        retrieved_docs = [int(result['title'].split(' ')[-1]) for result in results]
        # print(f"Retrieved documents: {retrieved_docs}")
        ground_truth_docs =  []
        if type(ground_truth_doc) is str:
            for doc in ground_truth_doc.split(","):
                ground_truth_docs.append(int(doc))
        else:
            ground_truth_docs.append(int(ground_truth_doc))
        # print("Ground truth documents:", ground_truth_docs)
        
        # T√≠nh MAP cho 1 truy v·∫•n
        for i, doc in enumerate(retrieved_docs):
            if doc in ground_truth_docs:
                hits += 1
                ap += hits / (i + 1)
        if hits > 0:
            ap /= hits
        # print(f"Average Precision for this query: {ap}")
        total_map += ap 
    return total_map / len(df_train)

# H√†m MRR@k
def mrr_k(file_clb_proptit, file_train, embedding, vector_db, reranker=None, k=5, use_query_expansion=True):
    df_clb = pd.read_csv(file_clb_proptit)
    df_train = pd.read_excel(file_train)

    total_mrr = 0

    for index, row in df_train.iterrows():
        query = row['Query']
        ground_truth_doc = row['Ground truth document']
        # T·∫°o embedding cho c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng
        # Retrieve top-k (with optional reranking and query expansion)
        results = retrieve_and_rerank(query, embedding, vector_db, reranker, k, use_query_expansion)

        # L·∫•y danh s√°ch t√†i li·ªáu ƒë∆∞·ª£c truy su·∫•t
        retrieved_docs = [int(result['title'].split(' ')[-1]) for result in results]
        # print(f"Retrieved documents: {retrieved_docs}")
        ground_truth_docs =  []
        if type(ground_truth_doc) is str:
            for doc in ground_truth_doc.split(","):
                ground_truth_docs.append(int(doc))
        else:
            ground_truth_docs.append(int(ground_truth_doc))
        # print("Ground truth documents:", ground_truth_docs)
        
        # T√≠nh MRR cho 1 truy v·∫•n
        for i, doc in enumerate(retrieved_docs):
            if doc in ground_truth_docs:
                total_mrr += 1 / (i + 1)
                break
    return total_mrr / len(df_train) if len(df_train) > 0 else 0

# H√†m NDCG@k
import numpy as np
import torch
def dcg_at_k(relevances, k):
    relevances = np.array(relevances)[:k]
    return np.sum((2**relevances - 1) / np.log2(np.arange(2, len(relevances) + 2)))

def ndcg_at_k(relevances, k):
    dcg_max = dcg_at_k(sorted(relevances, reverse=True), k)
    if dcg_max == 0:
        return 0.0
    return dcg_at_k(relevances, k) / dcg_max

def similarity(embedding1, embedding2):
    # Use torch cosine similarity on appropriate device for performance
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    a = torch.tensor(embedding1, device=device, dtype=torch.float)
    b = torch.tensor(embedding2, device=device, dtype=torch.float)
    norm1 = torch.norm(a)
    norm2 = torch.norm(b)
    if norm1.item() == 0 or norm2.item() == 0:
        return 0.0
    cos_sim = torch.dot(a, b) / (norm1 * norm2)
    # Normalize to [0,1]
    return ((cos_sim + 1) / 2).item()


def ndcg_k(file_clb_proptit, file_train, embedding, vector_db, reranker=None, k=5, use_query_expansion=True):
    df_clb = pd.read_csv(file_clb_proptit)
    df_train = pd.read_excel(file_train)

    total_ndcg = 0

    for index, row in df_train.iterrows():
        query = row['Query']
        ground_truth_doc = row['Ground truth document']
        query_embeddings = embedding.encode(query)
        if isinstance(query_embeddings, dict) and 'dense_vecs' in query_embeddings:
            user_embedding = query_embeddings['dense_vecs']
        else:
            user_embedding = query_embeddings

        # Retrieve top-k (with optional reranking and query expansion)
        results = retrieve_and_rerank(query, embedding, vector_db, reranker, k, use_query_expansion)


        retrieved_docs = [int(result['title'].split(' ')[-1]) for result in results]

        ground_truth_docs = []
        if type(ground_truth_doc) is str:
            for doc in ground_truth_doc.split(","):
                ground_truth_docs.append(int(doc))
        else:
            ground_truth_docs.append(int(ground_truth_doc))


        # N·∫øu ƒëi·ªÉm t∆∞∆°ng ƒë·ªìng > 0.9 th√¨ g√°n 3, n·∫øu > 0.7 th√¨ g√°n 2, n·∫øu > 0.5 th√¨ g√°n 1, c√≤n l·∫°i th√¨ g√°n 0 
        relevances = []
        for doc in retrieved_docs:
            if doc in ground_truth_docs:
                # Gi·∫£ s·ª≠ ta c√≥ m·ªôt h√†m ƒë·ªÉ t√≠nh ƒë·ªô t∆∞∆°ng ƒë·ªìng gi·ªØa c√¢u h·ªèi v√† t√†i li·ªáu, doc l√† s·ªë th·ª© t·ª± c·ªßa t√†i li·ªáu trong file CLB_PROPTIT.csv
                doc_result = [r for r in results if int(r['title'].split(' ')[-1]) == doc][0]
                doc_embeddings = embedding.encode(doc_result['information'])
                # Extract dense vectors for similarity calculation
                if isinstance(doc_embeddings, dict) and 'dense_vecs' in doc_embeddings:
                    doc_embedding = doc_embeddings['dense_vecs']
                else:
                    doc_embedding = doc_embeddings
                similarity_score = similarity(user_embedding, doc_embedding)
                if similarity_score > 0.9:
                    relevances.append(3)
                elif similarity_score > 0.7:
                    relevances.append(2)
                elif similarity_score > 0.5:
                    relevances.append(1)
                else:
                    relevances.append(0)
            else:
                relevances.append(0)
        ndcg = ndcg_at_k(relevances, k)
        # print(f"NDCG for this query: {ndcg}")
        total_ndcg += ndcg

    return total_ndcg / len(df_train) if len(df_train) > 0 else 0

# H√†m Context Precision@k (LLM Judged)
def context_precision_k(file_clb_proptit, file_train, embedding, vector_db, reranker=None, k=5, use_query_expansion=True):
    df_clb = pd.read_csv(file_clb_proptit)
    df_train = pd.read_excel(file_train)

    # Ch·ªâ l·∫•y 30 h√†ng ƒë·ªÉ test nhanh
    # sample_size = 30
    # df_train = df_train.tail(sample_size)
    # print(f"Testing with {len(df_train)} queries out of total {sample_size * 5} queries")

    total_precision = 0

    for index, row in df_train.iterrows():
        print(f"Processing query {index+1}/{len(df_train)}: {row['Query'][:50]}...")
        # TODO: T·∫°o ra LLM Answer, c√°c em h√£y t·ª± vi·∫øt ph·∫ßn system prompt
        messages = [
            {
                "role": "system",
                "content": COMMON_RAG_SYSTEM_PROMPT
            }
        ]
        hits = 0
        query = row['Query']

        # T√¨m ki·∫øm th√¥ng tin li√™n quan trong c∆° s·ªü d·ªØ li·ªáu v·ªõi optional reranking/query expansion
        results = retrieve_and_rerank(query, embedding, vector_db, reranker, k, use_query_expansion)
         
        # TODO: vi·∫øt c√¢u query c·ªßa ng∆∞·ªùi d√πng (bao g·ªìm document retrieval v√† c√¢u query)
        context = "Content t·ª´ c√°c t√†i li·ªáu li√™n quan:\n"
        context += "\n".join([result["information"] for result in results])

        # Th√™m context v√†o messages
        messages.append({
            "role": "user",
            "content": context + "\n\nC√¢u h·ªèi: " + query
        })
        # G·ªçi  API ƒë·ªÉ l·∫•y c√¢u tr·∫£ l·ªùi
        reply = get_llm_response(messages)

        system_judge = {
            "role": "system",
            "content": "B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n ƒë√°nh gi√° ƒë·ªô ch√≠nh x√°c c·ªßa c√°c c√¢u tr·∫£ l·ªùi d·ª±a tr√™n ng·ªØ c·∫£nh ƒë∆∞·ª£c cung c·∫•p. "
                      "B·∫°n s·∫Ω ƒë∆∞·ª£c cung c·∫•p m·ªôt c√¢u h·ªèi, m·ªôt c√¢u tr·∫£ l·ªùi, v√† danh s√°ch ng·ªØ c·∫£nh. "
                      "Nhi·ªám v·ª•: ƒë√°nh gi√° m·ª©c ƒë·ªô li√™n quan c·ªßa m·ªói ng·ªØ c·∫£nh v·ªõi c√¢u tr·∫£ l·ªùi. "
                      f"Tr·∫£ v·ªÅ m·ªôt chu·ªói li·ªÅn m·∫°ch g·ªìm {k} k√Ω t·ª±, m·ªói k√Ω t·ª± l√† 1 n·∫øu ng·ªØ c·∫£nh t∆∞∆°ng ·ª©ng li√™n quan, 0 n·∫øu kh√¥ng, s·∫Øp x·∫øp theo ƒë√∫ng th·ª© t·ª± c√°c ng·ªØ c·∫£nh v√† gi·∫£i th√≠ch ng·∫Øn g·ªçn b·∫±ng duy nh·∫•t 1 c√¢u ·ªü b√™n d∆∞·ªõi."
        }
        
        context_sections = results
        user_content = f"C√¢u h·ªèi: {query}\nC√¢u tr·∫£ l·ªùi: {reply}\n\nNg·ªØ c·∫£nh:\n"
        for idx, res in enumerate(context_sections, 1):
            user_content += f"{idx}. {res['information']}\n"
            # print(f"Context {idx}: {res['information'][:50]}...")
        user_content += f"\nH√£y ƒë√°nh gi√° m·ª©c ƒë·ªô li√™n quan cho m·ªói ng·ªØ c·∫£nh, tr·∫£ l·ªùi chu·ªói g·ªìm {k} k√Ω t·ª± 1 ho·∫∑c 0 theo th·ª© t·ª± tr√™n, kh√¥ng gi·∫£i th√≠ch."
        messages_judged = [system_judge, {"role": "user", "content": user_content}]
        judged_reply = get_llm_response(messages_judged)
        raw = str(judged_reply)
        # D√πng regex ƒë·ªÉ t√¨m chu·ªói ƒë√∫ng ƒë·ªãnh d·∫°ng
        match = re.search(rf'[01]{{{k}}}', raw)
        if match:
            flags = match.group(0)
        else:
            # Fallback: ƒëi·ªÅn 0 b√π
            tmp = ''.join(c for c in raw if c in '01')
            flags = (tmp + '0' * k)[:k]
        hits = flags.count('1')
        # Debug
        print(f"Judged reply: {judged_reply}")
        print(f"Flags: {flags}, Hits: {hits}")

        precision = hits / k if k > 0 else 0
        total_precision += precision
        time.sleep(1)
    return total_precision / len(df_train) if len(df_train) > 0 else 0


# H√†m Context Recall@k (LLM Judged)
def context_recall_k(file_clb_proptit, file_train, embedding, vector_db, reranker=None, k=5, use_query_expansion=True):
    df_clb = pd.read_csv(file_clb_proptit)
    df_train = pd.read_excel(file_train)
    # Sample a subset for faster testing / stable evaluation (same behavior as context_precision_k)
    # sample_size = 20
    # df_train = df_train.head(sample_size)
    # print(f"Testing context_recall_k with {len(df_train)} queries (sample_size={sample_size})")

    total_recall = 0

    for index, row in df_train.iterrows():
        query = row['Query']

        # T√¨m ki·∫øm th√¥ng tin li√™n quan trong c∆° s·ªü d·ªØ li·ªáu
        results = retrieve_and_rerank(query, embedding, vector_db, reranker, k, use_query_expansion)
        reply = row['Ground truth answer']
        

        # Build a single judge prompt that asks for a k-length 0/1 string (no explanation)
        system_judge = {
            "role": "system",
            "content": (
                "B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n ƒë√°nh gi√° ƒë·ªô ch√≠nh x√°c c·ªßa c√°c c√¢u tr·∫£ l·ªùi d·ª±a tr√™n ng·ªØ c·∫£nh ƒë∆∞·ª£c cung c·∫•p. "
                "B·∫°n s·∫Ω ƒë∆∞·ª£c cung c·∫•p m·ªôt c√¢u h·ªèi, m·ªôt c√¢u tr·∫£ l·ªùi ch√≠nh x√°c (ground-truth), v√† m·ªôt danh s√°ch ng·ªØ c·∫£nh. "
                "Nhi·ªám v·ª•: cho bi·∫øt m·ªói ng·ªØ c·∫£nh c√≥ ƒë·ªß th√¥ng tin (ho·∫∑c m·ªôt ph·∫ßn th√¥ng tin) ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n c√¢u tr·∫£ l·ªùi ch√≠nh x√°c hay kh√¥ng. "
                "Tr·∫£ v·ªÅ m·ªôt chu·ªói g·ªìm ch√≠nh x√°c {k} k√Ω t·ª±, m·ªói k√Ω t·ª± l√† 1 n·∫øu ng·ªØ c·∫£nh t∆∞∆°ng ·ª©ng li√™n quan/useful, 0 n·∫øu kh√¥ng, theo ƒë√∫ng th·ª© t·ª± c√°c ng·ªØ c·∫£nh. "
                "KH√îNG gi·∫£i th√≠ch th√™m, ch·ªâ tr·∫£ v·ªÅ chu·ªói 0/1."
            )
        }

        user_content = f"C√¢u h·ªèi: {query}\nC√¢u tr·∫£ l·ªùi ch√≠nh x√°c: {reply}\n\nNg·ªØ c·∫£nh:\n"
        for idx, res in enumerate(results, 1):
            user_content += f"{idx}. {res['information']}\n"
            # print(f"Context {idx}: {res['information'][:50]}...")

        messages_judged = [system_judge, {"role": "user", "content": user_content}]
        judged_reply = get_llm_response(messages_judged)

        raw = str(judged_reply)
        # Try to extract a k-length binary string
        match = re.search(rf'[01]{{{k}}}', raw)
        if match:
            flags = match.group(0)
        else:
            tmp = ''.join(c for c in raw if c in '01')
            flags = (tmp + '0' * k)[:k]

        hits = flags.count('1')
        recall = hits / k if k > 0 else 0
        total_recall += recall
        # debug
        print(f"Query {index+1}/{len(df_train)} - Flags: {flags} - Hits: {hits} - Recall: {recall:.3f}")
        time.sleep(1)

    return total_recall / len(df_train) if len(df_train) > 0 else 0

# H√†m Context Entities Recall@k (LLM Judged)
def context_entities_recall_k(file_clb_proptit, file_train, embedding, vector_db, reranker=None, k=5, use_query_expansion=True):
    df_clb = pd.read_csv(file_clb_proptit)
    df_train = pd.read_excel(file_train)
    # Sample subset for faster testing
    # sample_size = 20
    # df_train = df_train.head(sample_size)
    # print(f"Testing context_entities_recall_k with {len(df_train)} queries (sample_size={sample_size})")

    total_recall = 0
    for index, row in df_train.iterrows():
        hits = 0
        query = row['Query']

        # T√¨m ki·∫øm th√¥ng tin li√™n quan trong c∆° s·ªü d·ªØ li·ªáu
        results = retrieve_and_rerank(query, embedding, vector_db, reranker, k, use_query_expansion)
        reply = row['Ground truth answer']
        # Tr√≠ch xu·∫•t c√°c th·ª±c th·ªÉ t·ª´ Ground truth answer b·∫±ng LLM
        # NOTE: C√°c em c√≥ th·ªÉ thay ƒë·ªïi messages_entities n·∫øu mu·ªën
        messages_entities = [
            {
                "role": "system", 
                "content": """Tr√≠ch xu·∫•t t·∫•t c·∫£ th·ª±c th·ªÉ quan tr·ªçng t·ª´ c√¢u tr·∫£ l·ªùi. Th·ª±c th·ªÉ bao g·ªìm: t√™n t·ªï ch·ª©c, t√™n ng∆∞·ªùi, ng√†y th√°ng, ƒë·ªãa ƒëi·ªÉm, kh√°i ni·ªám, thu·∫≠t ng·ªØ, s·ªë li·ªáu.
CH·ªà TRA V·ªÄ danh s√°ch Python h·ª£p l·ªá, KH√îNG gi·∫£i th√≠ch g√¨ th√™m.

V√≠ d·ª•:
Input: C√¢u l·∫°c b·ªô L·∫≠p Tr√¨nh PTIT ƒë∆∞·ª£c th√†nh l·∫≠p ng√†y 9/10/2011 v·ªõi slogan "L·∫≠p tr√¨nh t·ª´ tr√°i tim".
Output: ["C√¢u l·∫°c b·ªô L·∫≠p Tr√¨nh PTIT", "PTIT", "9/10/2011", "L·∫≠p tr√¨nh t·ª´ tr√°i tim"]

Input: CLB c√≥ 200 th√†nh vi√™n ƒëang h·ªçc t·∫°i H·ªçc vi·ªán PTIT.
Output: ["CLB", "200 th√†nh vi√™n", "H·ªçc vi·ªán PTIT"]"""
            }
        ]
        # NOTE: C√°c em c√≥ th·ªÉ thay ƒë·ªïi content n·∫øu mu·ªën
        messages_entities.append({
            "role": "user",
            "content": f"C√¢u tr·∫£ l·ªùi: {reply}"
        })
        # G·ªçi  API ƒë·ªÉ tr√≠ch xu·∫•t c√°c th·ª±c th·ªÉ
        entities_str = get_llm_response(messages_entities)
        try:
            entities = ast.literal_eval(entities_str.strip())
            if not isinstance(entities, list):
                entities = []
        except Exception:
            # Fallback: extract first bracketed list and parse
            match = re.search(r'\[.*?\]', entities_str, re.DOTALL)
            if match:
                try:
                    entities = ast.literal_eval(match.group(0))
                except Exception:
                    entities = []
            else:
                entities = []
        tmp = len(entities)
        for result in results:
            context = result['information']
            for entity in entities[:]:  
                entity_clean = entity.strip()
                if entity_clean.lower() in context.lower() or any(word in context.lower() for word in entity_clean.lower().split()):
                    hits += 1
                    entities.remove(entity.strip())
        total_recall += hits / tmp if tmp > 0 else 0
        print(f"Query {index+1}/{len(df_train)} - Total Recall: {total_recall:.3f}")
        time.sleep(1)

    return total_recall / len(df_train) if len(df_train) > 0 else 0



# H√†m t√≠nh to√°n t·∫•t c·∫£ metrics li√™n quan ƒë·∫øn Retrieval

def calculate_metrics_retrieval(file_clb_proptit, file_train , embedding, vector_db, train, reranker=None, use_query_expansion=True):
    # T·∫°o ra 1 b·∫£ng csv, c·ªôt th·ª© nh·∫•t l√† K value, c√°c c·ªôt c√≤n l·∫°i l√† metrics. S·∫Ω c√≥ 3 h√†ng t∆∞∆°ng tr∆∞ng v·ªõi k = 3, 5, 7
    k_values = [3, 5, 7]
    metrics = {
        "K": [],
        "hit@k": [],
        "recall@k": [],
        "precision@k": [],
        "f1@k": [],
        "map@k": [],
        "mrr@k": [],
        "ndcg@k": [],
        "context_precision@k": [],
        "context_recall@k": [],
        "context_entities_recall@k": []
    }
    # L∆∞u 2 ch·ªØ s·ªë th·∫≠p ph√¢n cho c√°c metrics
    for k in k_values:
        metrics["K"].append(k)
        metrics["hit@k"].append(round(hit_k(file_clb_proptit, file_train, embedding, vector_db, reranker, k, use_query_expansion), 2))
        metrics["recall@k"].append(round(recall_k(file_clb_proptit, file_train, embedding, vector_db, reranker, k, use_query_expansion), 2))
        metrics["precision@k"].append(round(precision_k(file_clb_proptit, file_train, embedding, vector_db, reranker, k, use_query_expansion), 2))
        metrics["f1@k"].append(round(f1_k(file_clb_proptit, file_train, embedding, vector_db, reranker, k, use_query_expansion), 2))
        metrics["map@k"].append(round(map_k(file_clb_proptit, file_train, embedding, vector_db, reranker, k, use_query_expansion), 2))
        metrics["mrr@k"].append(round(mrr_k(file_clb_proptit, file_train, embedding, vector_db, reranker, k, use_query_expansion), 2))
        metrics["ndcg@k"].append(round(ndcg_k(file_clb_proptit, file_train, embedding, vector_db, reranker, k, use_query_expansion), 2))
        metrics["context_precision@k"].append(round(context_precision_k(file_clb_proptit, file_train, embedding, vector_db, reranker, k, use_query_expansion), 2))
        metrics["context_recall@k"].append(round(context_recall_k(file_clb_proptit, file_train, embedding, vector_db, reranker, k, use_query_expansion), 2))
        metrics["context_entities_recall@k"].append(round(context_entities_recall_k(file_clb_proptit, file_train, embedding, vector_db, reranker, k, use_query_expansion), 2))
    # Chuy·ªÉn ƒë·ªïi metrics th√†nh DataFrame
    metrics_df = pd.DataFrame(metrics)
    # L∆∞u DataFrame v√†o file csv
    filename_suffix = "_with_query_expansion" if use_query_expansion else "_baseline"
    if train:
        metrics_df.to_csv(f"metrics_retrieval_train{filename_suffix}.csv", index=False)
    else:
        metrics_df.to_csv(f"metrics_retrieval_test{filename_suffix}.csv", index=False)
    return metrics_df

# C√°c h√†m ƒë√°nh gi√° LLM Answer
def get_contexts(query, embedding, vector_db, reranker=None, use_query_expansion=True, k=5):
    return retrieve_and_rerank(query, embedding, vector_db, reranker, k, use_query_expansion)
# H√†m String Presence

def string_presence_k(file_clb_proptit, file_train, embedding, vector_db, k=5, reranker=None, use_query_expansion=True):
    df_clb = pd.read_csv(file_clb_proptit)
    df_train = pd.read_excel(file_train)

    total_presence = 0

    for index, row in df_train.iterrows():
        hits = 0
        query = row['Query']
        # T·∫°o embedding cho c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng
        user_embedding = embedding.encode(query)

        # Retrieve contexts with optional query expansion and reranking
        results = get_contexts(query, embedding, vector_db, reranker, use_query_expansion, k)
        reply = row['Ground truth answer']
        messages = [
            {
                "role": "system",
                "content": COMMON_RAG_SYSTEM_PROMPT
            }
        ]
        context = "Content t·ª´ c√°c t√†i li·ªáu li√™n quan:\n"
        context += "\n".join([result["information"] for result in results])
        # Th√™m context v√†o messages
        messages.append({
            "role": "user",
            "content": context + "\n\nC√¢u h·ªèi: " + query
        })
        # G·ªçi  API ƒë·ªÉ l·∫•y c√¢u tr·∫£ l·ªùi
        response = get_llm_response(messages)
        # Tr√≠ch xu·∫•t c√°c th·ª±c th·ªÉ t·ª´ c√¢u tr·∫£ l·ªùi b·∫±ng LLM
        # NOTE: C√°c em c√≥ th·ªÉ thay ƒë·ªïi message_entities n·∫øu mu·ªën
        messages_entities = [
            {
                "role": "system",
                "content": """Tr√≠ch xu·∫•t t·∫•t c·∫£ th·ª±c th·ªÉ quan tr·ªçng t·ª´ c√¢u tr·∫£ l·ªùi. Th·ª±c th·ªÉ bao g·ªìm: t√™n t·ªï ch·ª©c, t√™n ng∆∞·ªùi, ng√†y th√°ng, ƒë·ªãa ƒëi·ªÉm, kh√°i ni·ªám, thu·∫≠t ng·ªØ, s·ªë li·ªáu. 
CH·ªà TRA V·ªÄ danh s√°ch Python h·ª£p l·ªá, KH√îNG gi·∫£i th√≠ch g√¨ th√™m.

V√≠ d·ª•:
Input: C√¢u l·∫°c b·ªô L·∫≠p Tr√¨nh PTIT ƒë∆∞·ª£c th√†nh l·∫≠p ng√†y 9/10/2011 v·ªõi slogan "L·∫≠p tr√¨nh t·ª´ tr√°i tim".
Output: ["C√¢u l·∫°c b·ªô L·∫≠p Tr√¨nh PTIT", "PTIT", "9/10/2011", "L·∫≠p tr√¨nh t·ª´ tr√°i tim"]

Input: CLB c√≥ 200 th√†nh vi√™n ƒëang h·ªçc t·∫°i H·ªçc vi·ªán PTIT.  
Output: ["CLB", "200 th√†nh vi√™n", "H·ªçc vi·ªán PTIT"]"""
            }
        ]
        # Thay ƒë·ªïi content n·∫øu mu·ªën
        messages_entities.append({
            "role": "user",
            "content": f"C√¢u tr·∫£ l·ªùi: {reply}"
        })
        # G·ªçi  API ƒë·ªÉ tr√≠ch xu·∫•t c√°c th·ª±c th·ªÉ
        entities_str = get_llm_response(messages_entities)
        # Tr√≠ch xu·∫•t danh s√°ch ·ªü ƒë·∫ßu ph·∫£n h·ªìi
        match = re.search(r'\[.*?\]', entities_str)
        if match:
            try:
                entities = ast.literal_eval(match.group())
            except Exception:
                entities = []
        else:
            entities = []
        for entity in entities:
            entity_clean = entity.strip()
            if entity_clean.lower() in response.lower() or any(word in response.lower() for word in entity_clean.lower().split()):
                hits += 1
        
        if len(entities) > 0:
            hits /= len(entities)
        else:
            hits = 0
        
        print(f"Query {index+1}/{len(df_train)} - Entities found: {hits * len(entities) if len(entities) > 0 else 0} / {len(entities)} - Presence: {hits:.3f}")
        total_presence += hits
    return total_presence / len(df_train) if len(df_train) > 0 else 0


 

# H√†m Rouge-L

from rouge import Rouge
def rouge_l_k(file_clb_proptit, file_train, embedding, vector_db, k=5, reranker=None, use_query_expansion=True):
    df_clb = pd.read_csv(file_clb_proptit)
    df_train = pd.read_excel(file_train)

    rouge = Rouge()
    total_rouge_l = 0

    for index, row in df_train.iterrows():
        query = row['Query']
        # Retrieve contexts with optional reranking and query expansion
        results = get_contexts(query, embedding, vector_db, reranker, use_query_expansion, k)
        reply = row['Ground truth answer']
        messages = [
            {
                "role": "system",
                "content": COMMON_RAG_SYSTEM_PROMPT
            }
        ]
        context = "Content t·ª´ c√°c t√†i li·ªáu li√™n quan:\n"
        context += "\n".join([result["information"] for result in results])

        # Th√™m context v√†o messages
        messages.append({
            "role": "user",
            "content": context + "\n\nC√¢u h·ªèi: " + query
        })
        # G·ªçi API ƒë·ªÉ l·∫•y c√¢u tr·∫£ l·ªùi
        response = get_llm_response(messages)
        try:
            scores = rouge.get_scores(response, reply)
            rouge_l = scores[0]['rouge-l']['f']
        except ValueError:
            rouge_l = 0.0
            print(f"Query {index+1}/{len(df_train)} - Error computing Rouge-L, set to {rouge_l:.3f}")
        total_rouge_l += rouge_l
        print(f"Query {index+1}/{len(df_train)} - Rouge-L: {rouge_l:.3f}")
    return total_rouge_l / len(df_train) if len(df_train) > 0 else 0

# H√†m BLEU-4
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
def bleu_4_k(file_clb_proptit, file_train, embedding, vector_db, k=5, reranker=None, use_query_expansion=True):
    df_clb = pd.read_csv(file_clb_proptit)
    df_train = pd.read_excel(file_train)

    total_bleu_4 = 0
    smoothing_function = SmoothingFunction().method1

    for index, row in df_train.iterrows():
        query = row['Query']
        # Retrieve contexts with optional reranking and query expansion
        results = get_contexts(query, embedding, vector_db, reranker, use_query_expansion, k)
        reply = row['Ground truth answer']
        messages = [
            {
                "role": "system",
                "content": COMMON_RAG_SYSTEM_PROMPT
            }
        ]
        context = "Content t·ª´ c√°c t√†i li·ªáu li√™n quan:\n"
        context += "\n".join([result["information"] for result in results])

        # S·ª≠a content n·∫øu mu·ªën
        messages.append({
            "role": "user",
            "content": context + "\n\nC√¢u h·ªèi: " + query
        })
        # G·ªçi  API ƒë·ªÉ l·∫•y c√¢u tr·∫£ l·ªùi
        response = get_llm_response(messages)
        reference = reply.split()
        candidate = response.split()
        bleu_4 = sentence_bleu([reference], candidate, smoothing_function=smoothing_function)
        total_bleu_4 += bleu_4
        print(f"Query {index+1}/{len(df_train)} - BLEU-4: {bleu_4:.3f}")
    return total_bleu_4 / len(df_train) if len(df_train) > 0 else 0

# H√†m Groundedness (LLM Answer - Hallucination Detection)

def groundedness_k(file_clb_proptit, file_train, embedding, vector_db, k=5, reranker=None, use_query_expansion=True):
    df_clb = pd.read_csv(file_clb_proptit)
    df_train = pd.read_excel(file_train)

    total_groundedness = 0

    for index, row in df_train.iterrows():
        hits = 0
        cnt = 0
        query = row['Query']
        # Retrieve contexts with optional reranking and query expansion
        results = get_contexts(query, embedding, vector_db, reranker, use_query_expansion, k)
        reply = row['Ground truth answer']
        messages = [
            {
                "role": "system",
                "content": COMMON_RAG_SYSTEM_PROMPT
            }
        ]
        context = "Content t·ª´ c√°c t√†i li·ªáu li√™n quan:\n"
        context += "\n".join([result["information"] for result in results])

        # Th√™m context v√†o messages, s·ª≠a content n·∫øu mu·ªën
        messages.append({
            "role": "user",
            "content": context + "\n\nC√¢u h·ªèi: " + query
        })
        # G·ªçi  API ƒë·ªÉ l·∫•y c√¢u tr·∫£ l·ªùi
        response = get_llm_response(messages)
      
    
        # T√°ch response th√†nh c√°c c√¢u
        import re
        sentences = re.split(r'(?<=[.!?])\s+', response.strip())
        sentences = [s.strip() for s in sentences if len(s.strip()) > 3]
        
        for sentence in sentences:
            # T·∫°o m·ªôt prompt ƒë·ªÉ ki·ªÉm tra t√≠nh groundedness c·ªßa c√¢u
            # S·ª≠a prompt n·∫øu mu·ªën
            messages_groundedness = [
                {
                    "role": "system",
                    "content": """B·∫°n l√† chuy√™n gia ƒë√°nh gi√° Groundedness trong h·ªá th·ªëng RAG. Nhi·ªám v·ª•: ƒë√°nh gi√° t·ª´ng c√¢u trong response c√≥ ƒë∆∞·ª£c h·ªó tr·ª£ b·ªüi context ƒë√£ cung c·∫•p hay kh√¥ng.

NGUY√äN T·∫ÆC ƒê√ÅNH GI√Å:
- supported: C√¢u ƒë∆∞·ª£c context h·ªó tr·ª£ tr·ª±c ti·∫øp HO·∫∂C c√≥ th·ªÉ suy ra h·ª£p l√Ω t·ª´ context
- unsupported: C√¢u ch·ª©a th√¥ng tin ho√†n to√†n kh√¥ng c√≥ trong context v√† kh√¥ng th·ªÉ suy ra
- contradictory: C√¢u m√¢u thu·∫´n tr·ª±c ti·∫øp v·ªõi th√¥ng tin trong context  
- no_rag: C√¢u kh√¥ng c·∫ßn ki·ªÉm tra factual (c√¢u ch√†o h·ªèi, disclaimer, c√¢u chuy·ªÉn ti·∫øp th√¥ng th∆∞·ªùng)

QUY T·∫ÆC ƒê·∫∂C BI·ªÜT:
1. C√¢u m√¥ t·∫£ chung v·ªÅ quy ƒë·ªãnh/ch√≠nh s√°ch (nh∆∞ "B·∫°n s·∫Ω ƒë∆∞·ª£c tham gia khi...") ‚Üí supported n·∫øu context ƒë·ªÅ c·∫≠p v·ªÅ ƒëi·ªÅu ki·ªán tham gia
2. Danh s√°ch ƒë∆∞·ª£c li·ªát k√™ m·ªôt ph·∫ßn t·ª´ context ‚Üí supported  
3. Th√¥ng tin t·ªïng h·ª£p t·ª´ nhi·ªÅu ph·∫ßn context ‚Üí supported
4. C√¢u "Th√¥ng tin n√†y kh√¥ng c√≥ trong t√†i li·ªáu" ‚Üí supported
5. Ch·ªâ ƒë√°nh gi√° unsupported khi c√¢u ch·ª©a facts sai l·ªách

Ch·ªâ tr·∫£ l·ªùi m·ªôt t·ª´: supported/unsupported/contradictory/no_rag"""
                }
            ]
            # S·ª≠a content n·∫øu mu·ªën
            messages_groundedness.append({
                "role": "user",
                "content": f"Question: {query}\n\nContexts: {context}\n\nAnswer: {sentence.strip()}"
            })
            # G·ªçi  API ƒë·ªÉ ƒë√°nh gi√° groundedness
            groundedness_reply = get_llm_response(messages_groundedness).lower().strip()
            words = groundedness_reply.split()

            if "supported" in words:
                hits += 1
                cnt += 1
            elif "unsupported" in words or "contradictory" in words:
                cnt += 1
        total_groundedness += hits / cnt if cnt > 0 else 0
        print(f"Query {index+1}/{len(df_train)} - Groundedness: {hits}/{cnt} - Total: {total_groundedness:.3f}")
    return total_groundedness / len(df_train) if len(df_train) > 0 else 0 

# H√†m Response Relevancy (LLM Answer - Measures relevance)


def generate_related_questions(response, embedding):
    # S·ª≠a system prompt n·∫øu mu·ªën
    messages_related = [
        {
            "role": "system",
            "content": """B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n t·∫°o ra c√°c c√¢u h·ªèi li√™n quan t·ª´ m·ªôt c√¢u tr·∫£ l·ªùi. B·∫°n s·∫Ω ƒë∆∞·ª£c cung c·∫•p m·ªôt c√¢u tr·∫£ l·ªùi v√† nhi·ªám v·ª• c·ªßa b·∫°n l√† t·∫°o ra c√°c c√¢u h·ªèi li√™n quan ƒë·∫øn c√¢u tr·∫£ l·ªùi ƒë√≥. H√£y t·∫°o ra √≠t nh·∫•t 5 c√¢u h·ªèi li√™n quan, m·ªói c√¢u h·ªèi n√™n ng·∫Øn g·ªçn v√† r√µ r√†ng. Tr·∫£ l·ªùi d∆∞·ªõi d·∫°ng list c√°c c√¢u h·ªèi nh∆∞ ·ªü v√≠ d·ª• d∆∞·ªõi. L∆ØU √ù: Tr·∫£ l·ªùi d∆∞·ªõi d·∫°ng ["c√¢u h·ªèi 1", "c√¢u h·ªèi 2", "c√¢u h·ªèi 3", ...], bao g·ªìm c·∫£ d·∫•u ngo·∫∑c vu√¥ng.
            V√≠ d·ª•:
            C√¢u tr·∫£ l·ªùi: C√¢u l·∫°c b·ªô L·∫≠p Tr√¨nh PTIT (Programming PTIT), t√™n vi·∫øt t·∫Øt l√† PROPTIT ƒë∆∞·ª£c th√†nh l·∫≠p ng√†y 9/10/2011. V·ªõi ph∆∞∆°ng ch√¢m ho·∫°t ƒë·ªông "Chia s·∫ª ƒë·ªÉ c√πng nhau ph√°t tri·ªÉn", c√¢u l·∫°c b·ªô l√† n∆°i giao l∆∞u, ƒë√†o t·∫°o c√°c m√¥n l·∫≠p tr√¨nh v√† c√°c m√¥n h·ªçc trong tr∆∞·ªùng, t·∫°o ƒëi·ªÅu ki·ªán ƒë·ªÉ sinh vi√™n trong H·ªçc vi·ªán c√≥ m√¥i tr∆∞·ªùng h·ªçc t·∫≠p nƒÉng ƒë·ªông s√°ng t·∫°o. Slogan: L·∫≠p Tr√¨nh PTIT - L·∫≠p tr√¨nh t·ª´ tr√°i tim.
            Output c·ªßa b·∫°n: "["CLB L·∫≠p Tr√¨nh PTIT ƒë∆∞·ª£c th√†nh l·∫≠p khi n√†o?", "Slogan c·ªßa CLB l√† g√¨?", "M·ª•c ti√™u c·ªßa CLB l√† g√¨?"]"
            C√¢u tr·∫£ l·ªùi: N·∫øu b·∫°n thu·ªôc ng√†nh kh√°c b·∫°n v·∫´n c√≥ th·ªÉ tham gia CLB ch√∫ng m√¨nh. N·∫øu ƒë·ªãnh h∆∞·ªõng c·ªßa b·∫°n ho√†n to√†n l√† theo CNTT th√¨ CLB ch·∫Øc ch·∫Øn l√† n∆°i ph√π h·ª£p nh·∫•t ƒë·ªÉ c√°c b·∫°n ph√°t tri·ªÉn. Tr·ªü ng·∫°i l·ªõn nh·∫•t s·∫Ω l√† do b·∫°n theo m·ªôt h∆∞·ªõng kh√°c n·ªØa n√™n s·∫Ω ph·∫£i t·∫≠p trung v√†o c·∫£ 2 m·∫£ng n√™n s·∫Ω c·∫ßn c·ªë g·∫Øng nhi·ªÅu h∆°n.
            Output c·ªßa b·∫°n: "["Ng√†nh n√†o c√≥ th·ªÉ tham gia CLB?", "CLB ph√π h·ª£p v·ªõi nh·ªØng ai?", "Tr·ªü ng·∫°i l·ªõn nh·∫•t khi tham gia CLB l√† g√¨?"]"""
        }
    ]
    # S·ª≠a content n·∫øu mu·ªën
    messages_related.append({
        "role": "user",
        "content": f"C√¢u tr·∫£ l·ªùi: {response}"
    })
    # G·ªçi  API ƒë·ªÉ t·∫°o ra c√°c c√¢u h·ªèi li√™n quan
    related_questions = get_llm_response(messages_related)
    return related_questions

def response_relevancy_k(file_clb_proptit, file_train, embedding, vector_db, k=5, reranker=None, use_query_expansion=True):
    df_clb = pd.read_csv(file_clb_proptit)
    df_train = pd.read_excel(file_train)

    total_relevancy = 0

    for index, row in df_train.iterrows():
        hits = 0
        cnt = 0
        query = row['Query']
        # Retrieve contexts with optional reranking and query expansion
        results = get_contexts(query, embedding, vector_db, reranker, use_query_expansion, k)
        reply = row['Ground truth answer']
        messages = [
            {
                "role": "system",
                "content": COMMON_RAG_SYSTEM_PROMPT
            }
        ]
        context = "Content t·ª´ c√°c t√†i li·ªáu li√™n quan:\n"
        context += "\n".join([result["information"] for result in results])

        # S·ª≠a content n·∫øu mu·ªën
        messages.append({
            "role": "user",
            "content": context + "\n\nC√¢u h·ªèi: " + query
        })
        # G·ªçi  API ƒë·ªÉ l·∫•y c√¢u tr·∫£ l·ªùi
        user_embedding_raw = embedding.encode(query)
        # Extract dense vectors for similarity calculation
        if isinstance(user_embedding_raw, dict) and 'dense_vecs' in user_embedding_raw:
            user_embedding = user_embedding_raw['dense_vecs']
        else:
            user_embedding = user_embedding_raw
            
        response = get_llm_response(messages)

        raw_related = generate_related_questions(response, embedding)
        related_questions = []
        if raw_related:
            raw_str = raw_related.strip()
            if raw_str.startswith('[') and raw_str.endswith('"'):
                raw_str = raw_str[:-1]
            try:
                related_questions = ast.literal_eval(raw_str)
            except Exception:
                related_questions = []
        for question in related_questions:
            question_embedding_raw = embedding.encode(question)
            # Extract dense vectors for similarity calculation
            if isinstance(question_embedding_raw, dict) and 'dense_vecs' in question_embedding_raw:
                question_embedding = question_embedding_raw['dense_vecs']
            else:
                question_embedding = question_embedding_raw
            # T√≠nh score relevancy gi·ªØa c√¢u h·ªèi v√† query
            score = similarity(user_embedding, question_embedding)
            hits += score
        print(f"Query {index+1}/{len(df_train)} - Related questions generated: {len(related_questions)} - Total relevancy score: {hits:.3f}")
        total_relevancy += hits / len(related_questions) if len(related_questions) > 0 else 0
    return total_relevancy / len(df_train) if len(df_train) > 0 else 0


# H√†m Noise Sensitivity (LLM Answer - Robustness to Hallucination)

def noise_sensitivity_k(file_clb_proptit, file_train, embedding, vector_db, k=5, reranker=None, use_query_expansion=True):
    df_clb = pd.read_csv(file_clb_proptit)
    df_train = pd.read_excel(file_train)

    total_sensitivity = 0

    for index, row in df_train.iterrows():
        hits = 0
        cnt = 0
        query = row['Query']
        # Retrieve contexts with optional reranking and query expansion
        results = get_contexts(query, embedding, vector_db, reranker, use_query_expansion, k)
        reply = row['Ground truth answer']
        messages = [
            {
                "role": "system",
                "content": COMMON_RAG_SYSTEM_PROMPT
            }
        ]
        context =  "Content t·ª´ c√°c t√†i li·ªáu li√™n quan:\n"
        context += "\n".join([result["information"] for result in results])

        # Th√™m context v√†o messages, s·ª≠a content n·∫øu mu·ªën
        messages.append({
            "role": "user",
            "content": context + "\n\nC√¢u h·ªèi: " + query
        })
        # G·ªçi  API ƒë·ªÉ l·∫•y c√¢u tr·∫£ l·ªùi
        response = get_llm_response(messages)

        sentences = re.split(r'(?<=[.!?])\s+', response.strip())
        sentences = [s for s in sentences if len(s.strip()) > 5]
        for sentence in sentences:
            # S·ª≠a prompt n·∫øu mu·ªën
            messages_sensitivity = [
                {
                    "role": "system",
                    "content": """B·∫°n l√† m·ªôt chuy√™n gia ƒë√°nh gi√° ƒë·ªô nh·∫°y c·∫£m c·ªßa c√¢u tr·∫£ l·ªùi trong h·ªá th·ªëng RAG. Nhi·ªám v·ª•: ƒë√°nh gi√° t·ª´ng c√¢u c·ªßa response c√≥ ƒë∆∞·ª£c h·ªó tr·ª£ b·ªüi context hay kh√¥ng.

Quy t·∫Øc ƒë√°nh gi√° NGHI√äM NG·∫∂T:
- 1: C√¢u c√≥ th·ªÉ truy v·∫øt tr·ª±c ti·∫øp t·ª´ context (t·ª´, c·ª•m t·ª´, √Ω nghƒ©a xu·∫•t hi·ªán trong context)
- 0: C√¢u KH√îNG c√≥ trong context ho·∫∑c kh√¥ng th·ªÉ suy ra t·ª´ context

L∆∞u √Ω ƒë·∫∑c bi·ªát:
- C√¢u ch√†o h·ªèi l·ªãch s·ª±, c√¢u c·∫£m th√°n ƒë∆°n gi·∫£n -> 1
- C√¢u "Th√¥ng tin n√†y kh√¥ng c√≥ trong t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p." -> 1 
- B·∫•t k·ª≥ th√¥ng tin c·ª• th·ªÉ n√†o kh√¥ng xu·∫•t hi·ªán trong context -> 0
- Suy lu·∫≠n qu√° xa so v·ªõi context -> 0

Ch·ªâ tr·∫£ l·ªùi: 1 ho·∫∑c 0"""
                }
            ]
            # S·ª≠a prompt n·∫øu mu·ªën
            messages_sensitivity.append({
                "role": "user",
                "content": f"Question: {query}\n\nContexts: {context}\n\nAnswer: {sentence.strip()}"
            })
            # G·ªçi  API ƒë·ªÉ ƒë√°nh gi√° ƒë·ªô nh·∫°y c·∫£m
            sensitivity_reply = get_llm_response(messages_sensitivity)
            if sensitivity_reply == "0":
                hits += 1
        print(f"Query {index+1}/{len(df_train)} - Non-supported sentences: {hits} / {len(sentences)} - Noise Sensitivity: {hits / len(sentences) if len(sentences) > 0 else 0:.3f}")
        total_sensitivity += hits / len(sentences) if len(sentences) > 0 else 0
    return total_sensitivity / len(df_train) if len(df_train) > 0 else 0


# H√†m ƒë·ªÉ t√≠nh to√°n to√†n b·ªô metrics trong module LLM Answer

def calculate_metrics_llm_answer(file_clb_proptit, file_train, embedding, vector_db, train, reranker=None, use_query_expansion=True):
    # T·∫°o ra 1 b·∫£ng csv, c·ªôt th·ª© nh·∫•t l√† K value, c√°c c·ªôt c√≤n l·∫°i l√† metrics. S·∫Ω c√≥ 3 h√†ng t∆∞∆°ng tr∆∞ng v·ªõi k = 3, 5, 7
    k_values = [3, 5, 7]
    metrics = {
        "K": [],
        "string_presence@k": [],
        "rouge_l@k": [],
        "bleu_4@k": [],
        "groundedness@k": [],
        "response_relevancy@k": [],
        "noise_sensitivity@k": []
    }
    # L∆∞u 2 ch·ªØ s·ªë th·∫≠p ph√¢n cho c√°c metrics
    for k in k_values:
        metrics["K"].append(k)
        metrics["string_presence@k"].append(round(string_presence_k(file_clb_proptit, file_train, embedding, vector_db, k, reranker, use_query_expansion), 2))
        metrics["rouge_l@k"].append(round(rouge_l_k(file_clb_proptit, file_train, embedding, vector_db, k, reranker, use_query_expansion), 2))
        metrics["bleu_4@k"].append(round(bleu_4_k(file_clb_proptit, file_train, embedding, vector_db, k, reranker, use_query_expansion), 2))
        metrics["groundedness@k"].append(round(groundedness_k(file_clb_proptit, file_train, embedding, vector_db, k, reranker, use_query_expansion), 2))
        metrics["response_relevancy@k"].append(round(response_relevancy_k(file_clb_proptit, file_train, embedding, vector_db, k, reranker, use_query_expansion), 2))
        metrics["noise_sensitivity@k"].append(round(noise_sensitivity_k(file_clb_proptit, file_train, embedding, vector_db, k, reranker, use_query_expansion), 2))
    # Chuy·ªÉn ƒë·ªïi metrics th√†nh DataFrame
    metrics_df = pd.DataFrame(metrics)
    # L∆∞u DataFrame v√†o file csv
    if train:
        metrics_df.to_csv("metrics_llm_answer_train.csv", index=False)
    else:
        metrics_df.to_csv("metrics_llm_answer_test.csv", index=False)
    return metrics_df
