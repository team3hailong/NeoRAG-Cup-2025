import pandas as pd
import re
import openai
import os
import ast
from dotenv import load_dotenv
import requests
import time
from google import genai
from rerank import Reranker

# Helper to retrieve and optionally rerank results
def retrieve_and_rerank(query, embedding, vector_db, reranker, k):
    user_embedding = embedding.encode(query)
    initial_limit = k * 2 if reranker else k
    results = vector_db.query("information", user_embedding, limit=initial_limit)
    if reranker and results:
        passages = [res['information'] for res in results]
        ranked_scores, ranked_passages = reranker(query, passages)
        reranked_results = []
        for rp in ranked_passages[:k]:
            for res in results:
                if res['information'] == rp:
                    reranked_results.append(res)
                    break
        return reranked_results
    return results[:k]

load_dotenv()

# Option 1: Ollama (local)
# client = openai.OpenAI(
#     base_url=os.getenv("OLLAMA_BASE_URL", "http://localhost:11434/v1"),
#     api_key=os.getenv("OLLAMA_API_KEY", "ollama")
# )
# # Default model name for Ollama; change via environment variable OLLAMA_MODEL if needed.
# MODEL_NAME = os.getenv("OLLAMA_MODEL", "deepseek-r1:8b")

# Option 2: Gemini Pro 
# client = genai.Client(api_key=os.getenv("GEMINI_API_KEY"))
# MODEL_NAME = "gemini-1.5-flash"

from groq import Groq

client = Groq(
    api_key=os.getenv("GROQ_API_KEY")
)
MODEL_NAME="meta-llama/llama-4-maverick-17b-128e-instruct"

# üîß HELPER FUNCTION: Wrapper ƒë·ªÉ h·ªó tr·ª£ c·∫£ OpenAI v√† Gemini, c√≥ th·ªÉ thay ƒë·ªïi temperature, max_tokens
def get_llm_response(messages, model_name=MODEL_NAME):
    max_retries = 3
    backoff = 1
    for attempt in range(1, max_retries + 1):
        try:
            response = client.chat.completions.create(
                model=model_name,
                messages=messages,
                temperature=0.35,
                max_completion_tokens=848,
                top_p=1,
                stream=False,
                stop=None
            )
            return response.choices[0].message.content
        except Exception as e:
            print(f"Error calling LLM (attempt {attempt}/{max_retries}): {e}")
            if attempt < max_retries:
                time.sleep(backoff)
                backoff *= 2
    return ""

# N√™n ch·∫°y t·ª´ng h√†m t·ª´ ƒëo·∫°n n√†y ƒë·ªÉ test

def hit_k(file_clb_proptit, file_train_data_proptit, embedding, vector_db, reranker=None, k=5):
    df_clb = pd.read_csv(file_clb_proptit)
    df_train = pd.read_excel(file_train_data_proptit)

    hits = 0
    total_queries = len(df_train)

    for index, row in df_train.iterrows():
        query = row['Query']
        ground_truth_doc = row['Ground truth document']
        # TODO: N·∫øu c√°c em d√πng Text2SQL RAG hay c√°c ph∆∞∆°ng ph√°p s·ª≠ d·ª•ng ng√¥n ng·ªØ truy v·∫•n, c√≥ th·ªÉ b·ªè qua bi·∫øn user_embedding
        # C√°c em c√≥ th·ªÉ d√πng c√°c kƒ© thu·∫≠t ƒë·ªÉ vi·∫øt l·∫°i c√¢u query, Reranking, ... ·ªü ƒëo·∫°n n√†y.
        # Retrieve top-k (with optional reranking)
        results = retrieve_and_rerank(query, embedding, vector_db, reranker, k)
         
        # L·∫•y danh s√°ch t√†i li·ªáu ƒë∆∞·ª£c truy su·∫•t
        retrieved_docs = [int(result['title'].split(' ')[-1]) for result in results]
        ground_truth_docs =  []
        if type(ground_truth_doc) is str:
            for doc in ground_truth_doc.split(","):
                ground_truth_docs.append(int(doc))
        else:
            ground_truth_docs.append(int(ground_truth_doc))
        if any(doc in retrieved_docs for doc in ground_truth_docs):
            hits += 1
    return hits / total_queries if total_queries > 0 else 0


# H√†m recall@k
def recall_k(file_clb_proptit, file_train, embedding, vector_db, reranker=None, k=5):
    df_clb = pd.read_csv(file_clb_proptit)
    df_train = pd.read_excel(file_train)

    
    ans = 0

    for index, row in df_train.iterrows():
        hits = 0
        query = row['Query']
        ground_truth_doc = row['Ground truth document']
        # TODO: N·∫øu c√°c em d√πng Text2SQL RAG hay c√°c ph∆∞∆°ng ph√°p s·ª≠ d·ª•ng ng√¥n ng·ªØ truy v·∫•n, c√≥ th·ªÉ b·ªè qua bi·∫øn user_embedding
        # C√°c em c√≥ th·ªÉ d√πng c√°c kƒ© thu·∫≠t ƒë·ªÉ vi·∫øt l·∫°i c√¢u query, Reranking, ... ·ªü ƒëo·∫°n n√†y.
        # Embedding c√¢u query
        # Retrieve top-k (with optional reranking)
        results = retrieve_and_rerank(query, embedding, vector_db, reranker, k)

        # L·∫•y danh s√°ch t√†i li·ªáu ƒë∆∞·ª£c truy su·∫•t
        retrieved_docs = [int(result['title'].split(' ')[-1]) for result in results]
        ground_truth_docs =  []
        if type(ground_truth_doc) is str:
            for doc in ground_truth_doc.split(","):
                ground_truth_docs.append(int(doc))
        else:
            ground_truth_docs.append(int(ground_truth_doc))
        if any(doc in retrieved_docs for doc in ground_truth_docs):
            hits = len([doc for doc in ground_truth_docs if doc in retrieved_docs])
        ans += hits / len(ground_truth_docs) 
    return ans / len(df_train)


# H√†m precision@k
def precision_k(file_clb_proptit, file_train, embedding, vector_db, reranker=None, k=5):
    df_clb = pd.read_csv(file_clb_proptit)
    df_train = pd.read_excel(file_train)

    ans = 0

    for index, row in df_train.iterrows():
        hits = 0
        query = row['Query']
        ground_truth_doc = row['Ground truth document']
        # T·∫°o embedding cho c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng
        user_embedding = embedding.encode(query)

        # Retrieve top-k (with optional reranking)
        results = retrieve_and_rerank(query, embedding, vector_db, reranker, k)

        # L·∫•y danh s√°ch t√†i li·ªáu ƒë∆∞·ª£c truy su·∫•t
        retrieved_docs = [int(result['title'].split(' ')[-1]) for result in results]
        # print(f"Retrieved documents: {retrieved_docs}")
        ground_truth_docs =  []
        if type(ground_truth_doc) is str:
            for doc in ground_truth_doc.split(","):
                ground_truth_docs.append(int(doc))
        else:
            ground_truth_docs.append(int(ground_truth_doc))
        # print("Ground truth documents:", ground_truth_docs)
        # Ki·ªÉm tra xem c√≥ √≠t nh·∫•t m·ªôt t√†i li·ªáu ƒë√∫ng trong k·∫øt qu·∫£ t√¨m ki·∫øm
        if any(doc in retrieved_docs for doc in ground_truth_docs):
            hits = len([doc for doc in retrieved_docs if doc in ground_truth_docs])
        ans += hits / k 
        # print("Hits / k for this query:", hits / k)
    return ans / len(df_train)


# H√†m f1@k
def f1_k(file_clb_proptit, file_train, embedding, vector_db, reranker=None, k=5):
    precision = precision_k(file_clb_proptit, file_train, embedding, vector_db, reranker, k)
    recall = recall_k(file_clb_proptit, file_train, embedding, vector_db, reranker, k)
    if precision + recall == 0:
        return 0
    return 2 * (precision * recall) / (precision + recall)

# H√†m MAP@k

def map_k(file_clb_proptit, file_train, embedding, vector_db, reranker=None, k=5):
    df_clb = pd.read_csv(file_clb_proptit)
    df_train = pd.read_excel(file_train)

    total_map = 0

    for index, row in df_train.iterrows():
        hits = 0
        ap = 0
        query = row['Query']
        ground_truth_doc = row['Ground truth document']
        # T·∫°o embedding cho c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng
        # Retrieve top-k (with optional reranking)
        results = retrieve_and_rerank(query, embedding, vector_db, reranker, k)

        # L·∫•y danh s√°ch t√†i li·ªáu ƒë∆∞·ª£c truy su·∫•t
        retrieved_docs = [int(result['title'].split(' ')[-1]) for result in results]
        # print(f"Retrieved documents: {retrieved_docs}")
        ground_truth_docs =  []
        if type(ground_truth_doc) is str:
            for doc in ground_truth_doc.split(","):
                ground_truth_docs.append(int(doc))
        else:
            ground_truth_docs.append(int(ground_truth_doc))
        # print("Ground truth documents:", ground_truth_docs)
        
        # T√≠nh MAP cho 1 truy v·∫•n
        for i, doc in enumerate(retrieved_docs):
            if doc in ground_truth_docs:
                hits += 1
                ap += hits / (i + 1)
        if hits > 0:
            ap /= hits
        # print(f"Average Precision for this query: {ap}")
        total_map += ap 
    return total_map / len(df_train)

# H√†m MRR@k
def mrr_k(file_clb_proptit, file_train, embedding, vector_db, reranker=None, k=5):
    df_clb = pd.read_csv(file_clb_proptit)
    df_train = pd.read_excel(file_train)

    total_mrr = 0

    for index, row in df_train.iterrows():
        query = row['Query']
        ground_truth_doc = row['Ground truth document']
        # T·∫°o embedding cho c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng
        # Retrieve top-k (with optional reranking)
        results = retrieve_and_rerank(query, embedding, vector_db, reranker, k)

        # L·∫•y danh s√°ch t√†i li·ªáu ƒë∆∞·ª£c truy su·∫•t
        retrieved_docs = [int(result['title'].split(' ')[-1]) for result in results]
        # print(f"Retrieved documents: {retrieved_docs}")
        ground_truth_docs =  []
        if type(ground_truth_doc) is str:
            for doc in ground_truth_doc.split(","):
                ground_truth_docs.append(int(doc))
        else:
            ground_truth_docs.append(int(ground_truth_doc))
        # print("Ground truth documents:", ground_truth_docs)
        
        # T√≠nh MRR cho 1 truy v·∫•n
        for i, doc in enumerate(retrieved_docs):
            if doc in ground_truth_docs:
                total_mrr += 1 / (i + 1)
                break
    return total_mrr / len(df_train) if len(df_train) > 0 else 0

# H√†m NDCG@k
import numpy as np
import torch
def dcg_at_k(relevances, k):
    relevances = np.array(relevances)[:k]
    return np.sum((2**relevances - 1) / np.log2(np.arange(2, len(relevances) + 2)))

def ndcg_at_k(relevances, k):
    dcg_max = dcg_at_k(sorted(relevances, reverse=True), k)
    if dcg_max == 0:
        return 0.0
    return dcg_at_k(relevances, k) / dcg_max

def similarity(embedding1, embedding2):
    # Use torch cosine similarity on appropriate device for performance
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    a = torch.tensor(embedding1, device=device, dtype=torch.float)
    b = torch.tensor(embedding2, device=device, dtype=torch.float)
    norm1 = torch.norm(a)
    norm2 = torch.norm(b)
    if norm1.item() == 0 or norm2.item() == 0:
        return 0.0
    cos_sim = torch.dot(a, b) / (norm1 * norm2)
    # Normalize to [0,1]
    return ((cos_sim + 1) / 2).item()


def ndcg_k(file_clb_proptit, file_train, embedding, vector_db, reranker=None, k=5):
    df_clb = pd.read_csv(file_clb_proptit)
    df_train = pd.read_excel(file_train)

    total_ndcg = 0

    for index, row in df_train.iterrows():
        query = row['Query']
        ground_truth_doc = row['Ground truth document']
        # T·∫°o embedding cho c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng
        user_embedding = embedding.encode(query)

        # Retrieve top-k (with optional reranking)
        results = retrieve_and_rerank(query, embedding, vector_db, reranker, k)


        retrieved_docs = [int(result['title'].split(' ')[-1]) for result in results]

        ground_truth_docs = []
        if type(ground_truth_doc) is str:
            for doc in ground_truth_doc.split(","):
                ground_truth_docs.append(int(doc))
        else:
            ground_truth_docs.append(int(ground_truth_doc))


        # N·∫øu ƒëi·ªÉm t∆∞∆°ng ƒë·ªìng > 0.9 th√¨ g√°n 3, n·∫øu > 0.7 th√¨ g√°n 2, n·∫øu > 0.5 th√¨ g√°n 1, c√≤n l·∫°i th√¨ g√°n 0 
        relevances = []
        for doc in retrieved_docs:
            if doc in ground_truth_docs:
                # Gi·∫£ s·ª≠ ta c√≥ m·ªôt h√†m ƒë·ªÉ t√≠nh ƒë·ªô t∆∞∆°ng ƒë·ªìng gi·ªØa c√¢u h·ªèi v√† t√†i li·ªáu, doc l√† s·ªë th·ª© t·ª± c·ªßa t√†i li·ªáu trong file CLB_PROPTIT.csv
                doc_result = [r for r in results if int(r['title'].split(' ')[-1]) == doc][0]
                doc_embedding = embedding.encode(doc_result['information'])
                similarity_score = similarity(user_embedding, doc_embedding)
                if similarity_score > 0.9:
                    relevances.append(3)
                elif similarity_score > 0.7:
                    relevances.append(2)
                elif similarity_score > 0.5:
                    relevances.append(1)
                else:
                    relevances.append(0)
            else:
                relevances.append(0)
        ndcg = ndcg_at_k(relevances, k)
        # print(f"NDCG for this query: {ndcg}")
        total_ndcg += ndcg

    return total_ndcg / len(df_train) if len(df_train) > 0 else 0

# H√†m Context Precision@k (LLM Judged)
def context_precision_k(file_clb_proptit, file_train, embedding, vector_db, k=5):
    df_clb = pd.read_csv(file_clb_proptit)
    df_train = pd.read_excel(file_train)
    
    # Ch·ªâ l·∫•y 20 h√†ng ƒë·ªÉ test nhanh
    sample_size = 20
    df_train = df_train.head(sample_size)
    print(f"Testing with {len(df_train)} queries out of total {sample_size * 5} queries")

    total_precision = 0

    for index, row in df_train.iterrows():
        print(f"Processing query {index+1}/{len(df_train)}: {row['Query'][:50]}...")
        # TODO: T·∫°o ra LLM Answer, c√°c em h√£y t·ª± vi·∫øt ph·∫ßn system prompt
        messages = [
            {
                "role": "system",
                "content": """B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n cung c·∫•p th√¥ng tin v·ªÅ C√¢u l·∫°c b·ªô L·∫≠p tr√¨nh ProPTIT.
B·∫°n s·∫Ω nh·∫≠n ƒë∆∞·ª£c d·ªØ li·ªáu ng·ªØ c·∫£nh (context) t·ª´ m·ªôt h·ªá th·ªëng Retrieval-Augmented Generation (RAG) ch·ª©a c√°c th√¥ng tin ch√≠nh x√°c v·ªÅ CLB.

1. Ch·ªâ s·ª≠ d·ª•ng th√¥ng tin t·ª´ context ƒë∆∞·ª£c cung c·∫•p ƒë·ªÉ tr·∫£ l·ªùi. Context s·∫Ω ƒë∆∞·ª£c cung c·∫•p ·ªü ƒë·∫ßu m·ªói query c·ªßa ng∆∞·ªùi d√πng. C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng n·∫±m ·ªü cu·ªëi. 
2. N·∫øu ng∆∞·ªùi d√πng h·ªèi c√¢u h·ªèi kh√¥ng li√™n quan ƒë·∫øn CLB ProPTIT, h√£y tr·∫£ l·ªùi nh∆∞ b√¨nh th∆∞·ªùng, nh∆∞ng kh√¥ng s·ª≠ d·ª•ng th√¥ng tin t·ª´ context
3. Tr√¨nh b√†y c√¢u tr·∫£ l·ªùi r√µ r√†ng, d·ªÖ hi·ªÉu. C√≥ th·ªÉ s·ª≠ d·ª•ng emoij icon khi c·∫ßn.
4. Tuy·ªát ƒë·ªëi kh√¥ng suy ƒëo√°n ho·∫∑c b·ªãa th√¥ng tin.
5. Gi·ªØ phong c√°ch tr·∫£ l·ªùi th√¢n thi·ªán, chuy√™n nghi·ªáp v√† nh·∫•t qu√°n.
6. Trong context c√≥ th·ªÉ ch·ª©a nhi·ªÅu th√¥ng tin kh√°c nhau, h√£y t·∫≠p trung v√†o c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng ƒë·ªÉ tr·∫£ l·ªùi ch√≠nh x√°c nh·∫•t.

Nhi·ªám v·ª• c·ªßa b·∫°n:
- Tr·∫£ l·ªùi c√°c c√¢u h·ªèi v·ªÅ CLB L·∫≠p tr√¨nh ProPTIT: l·ªãch s·ª≠, th√†nh vi√™n, ho·∫°t ƒë·ªông, s·ª± ki·ªán, d·ª± √°n, n·ªôi quy, th√†nh vi√™n ti√™u bi·ªÉu, v√† c√°c th√¥ng tin li√™n quan kh√°c."""
            }
        ]
        hits = 0
        query = row['Query']
        ground_truth_doc = row['Ground truth document']
        # T·∫°o embedding cho c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng
        user_embedding = embedding.encode(query)

        # T√¨m ki·∫øm th√¥ng tin li√™n quan trong c∆° s·ªü d·ªØ li·ªáu
        results = vector_db.query("information", user_embedding, limit=k)
        
        # TODO: vi·∫øt c√¢u query c·ªßa ng∆∞·ªùi d√πng (bao g·ªìm document retrieval v√† c√¢u query)
        context = "Content t·ª´ c√°c t√†i li·ªáu li√™n quan:\n"
        context += "\n".join([result["information"] for result in results])

        # Th√™m context v√†o messages
        messages.append({
            "role": "user",
            "content": context + "\n\nC√¢u h·ªèi: " + query
        })
        # G·ªçi  API ƒë·ªÉ l·∫•y c√¢u tr·∫£ l·ªùi
        reply = get_llm_response(messages)

        system_judge = {
            "role": "system",
            "content": "B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n ƒë√°nh gi√° ƒë·ªô ch√≠nh x√°c c·ªßa c√°c c√¢u tr·∫£ l·ªùi d·ª±a tr√™n ng·ªØ c·∫£nh ƒë∆∞·ª£c cung c·∫•p. "
                      "B·∫°n s·∫Ω ƒë∆∞·ª£c cung c·∫•p m·ªôt c√¢u h·ªèi, m·ªôt c√¢u tr·∫£ l·ªùi, v√† danh s√°ch ng·ªØ c·∫£nh. "
                      "Nhi·ªám v·ª•: ƒë√°nh gi√° m·ª©c ƒë·ªô li√™n quan c·ªßa m·ªói ng·ªØ c·∫£nh v·ªõi c√¢u tr·∫£ l·ªùi. "
                      f"Tr·∫£ v·ªÅ m·ªôt chu·ªói li·ªÅn m·∫°ch g·ªìm {k} k√Ω t·ª±, m·ªói k√Ω t·ª± l√† 1 n·∫øu ng·ªØ c·∫£nh t∆∞∆°ng ·ª©ng li√™n quan, 0 n·∫øu kh√¥ng, s·∫Øp x·∫øp theo ƒë√∫ng th·ª© t·ª± c√°c ng·ªØ c·∫£nh v√† gi·∫£i th√≠ch ng·∫Øn g·ªçn b·∫±ng duy nh·∫•t 1 c√¢u ·ªü b√™n d∆∞·ªõi."
        }
        
        context_sections = results
        user_content = f"C√¢u h·ªèi: {query}\nC√¢u tr·∫£ l·ªùi: {reply}\n\nNg·ªØ c·∫£nh:\n"
        for idx, res in enumerate(context_sections, 1):
            user_content += f"{idx}. {res['information']}\n"
        user_content += f"\nH√£y ƒë√°nh gi√° m·ª©c ƒë·ªô li√™n quan cho m·ªói ng·ªØ c·∫£nh, tr·∫£ l·ªùi chu·ªói g·ªìm {k} k√Ω t·ª± 1 ho·∫∑c 0 theo th·ª© t·ª± tr√™n, kh√¥ng gi·∫£i th√≠ch."
        messages_judged = [system_judge, {"role": "user", "content": user_content}]
        judged_reply = get_llm_response(messages_judged)
        raw = str(judged_reply)
        # D√πng regex ƒë·ªÉ t√¨m chu·ªói ƒë√∫ng ƒë·ªãnh d·∫°ng
        match = re.search(rf'[01]{{{k}}}', raw)
        if match:
            flags = match.group(0)
        else:
            # Fallback: ƒëi·ªÅn 0 b√π
            tmp = ''.join(c for c in raw if c in '01')
            flags = (tmp + '0' * k)[:k]
        hits = flags.count('1')
        # Debug
        print(f"Judged reply: {judged_reply}")
        print(f"Flags: {flags}, Hits: {hits}")

        precision = hits / k if k > 0 else 0
        total_precision += precision
        time.sleep(1)
    return total_precision / len(df_train) if len(df_train) > 0 else 0


# H√†m Context Recall@k (LLM Judged)
def context_recall_k(file_clb_proptit, file_train, embedding, vector_db, k=5):
    df_clb = pd.read_csv(file_clb_proptit)
    df_train = pd.read_excel(file_train)
    # Sample a subset for faster testing / stable evaluation (same behavior as context_precision_k)
    sample_size = 20
    df_train = df_train.head(sample_size)
    print(f"Testing context_recall_k with {len(df_train)} queries (sample_size={sample_size})")

    total_recall = 0

    for index, row in df_train.iterrows():
        query = row['Query']
        # T·∫°o embedding cho c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng
        user_embedding = embedding.encode(query)

        # T√¨m ki·∫øm th√¥ng tin li√™n quan trong c∆° s·ªü d·ªØ li·ªáu
        results = vector_db.query("information", user_embedding, limit=k)
        reply = row['Ground truth answer']
        

        # Build a single judge prompt that asks for a k-length 0/1 string (no explanation)
        system_judge = {
            "role": "system",
            "content": (
                "B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n ƒë√°nh gi√° ƒë·ªô ch√≠nh x√°c c·ªßa c√°c c√¢u tr·∫£ l·ªùi d·ª±a tr√™n ng·ªØ c·∫£nh ƒë∆∞·ª£c cung c·∫•p. "
                "B·∫°n s·∫Ω ƒë∆∞·ª£c cung c·∫•p m·ªôt c√¢u h·ªèi, m·ªôt c√¢u tr·∫£ l·ªùi ch√≠nh x√°c (ground-truth), v√† m·ªôt danh s√°ch ng·ªØ c·∫£nh. "
                "Nhi·ªám v·ª•: cho bi·∫øt m·ªói ng·ªØ c·∫£nh c√≥ ƒë·ªß th√¥ng tin (ho·∫∑c m·ªôt ph·∫ßn th√¥ng tin) ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n c√¢u tr·∫£ l·ªùi ch√≠nh x√°c hay kh√¥ng. "
                "Tr·∫£ v·ªÅ m·ªôt chu·ªói g·ªìm ch√≠nh x√°c {k} k√Ω t·ª±, m·ªói k√Ω t·ª± l√† 1 n·∫øu ng·ªØ c·∫£nh t∆∞∆°ng ·ª©ng li√™n quan/useful, 0 n·∫øu kh√¥ng, theo ƒë√∫ng th·ª© t·ª± c√°c ng·ªØ c·∫£nh. "
                "KH√îNG gi·∫£i th√≠ch th√™m, ch·ªâ tr·∫£ v·ªÅ chu·ªói 0/1."
            )
        }

        user_content = f"C√¢u h·ªèi: {query}\nC√¢u tr·∫£ l·ªùi ch√≠nh x√°c: {reply}\n\nNg·ªØ c·∫£nh:\n"
        for idx, res in enumerate(results, 1):
            user_content += f"{idx}. {res['information']}\n"

        messages_judged = [system_judge, {"role": "user", "content": user_content}]
        judged_reply = get_llm_response(messages_judged)

        raw = str(judged_reply)
        # Try to extract a k-length binary string
        match = re.search(rf'[01]{{{k}}}', raw)
        if match:
            flags = match.group(0)
        else:
            tmp = ''.join(c for c in raw if c in '01')
            flags = (tmp + '0' * k)[:k]

        hits = flags.count('1')
        recall = hits / k if k > 0 else 0
        total_recall += recall
        # debug
        print(f"Query {index+1}/{len(df_train)} - Flags: {flags} - Hits: {hits} - Recall: {recall:.3f}")
        time.sleep(1)

    return total_recall / len(df_train) if len(df_train) > 0 else 0

# H√†m Context Entities Recall@k (LLM Judged)
def context_entities_recall_k(file_clb_proptit, file_train, embedding, vector_db, k=5):
    df_clb = pd.read_csv(file_clb_proptit)
    df_train = pd.read_excel(file_train)
    # Sample subset for faster testing
    sample_size = 20
    df_train = df_train.head(sample_size)
    print(f"Testing context_entities_recall_k with {len(df_train)} queries (sample_size={sample_size})")

    total_recall = 0
    for index, row in df_train.iterrows():
        hits = 0
        query = row['Query']
        # T·∫°o embedding cho c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng
        user_embedding = embedding.encode(query)

        # T√¨m ki·∫øm th√¥ng tin li√™n quan trong c∆° s·ªü d·ªØ li·ªáu
        results = vector_db.query("information", user_embedding, limit=k)
        reply = row['Ground truth answer']
        # Tr√≠ch xu·∫•t c√°c th·ª±c th·ªÉ t·ª´ Ground truth answer b·∫±ng LLM
        # NOTE: C√°c em c√≥ th·ªÉ thay ƒë·ªïi messages_entities n·∫øu mu·ªën
        messages_entities = [
            {
                "role": "system",
                "content": """B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n tr√≠ch xu·∫•t c√°c th·ª±c th·ªÉ t·ª´ c√¢u tr·∫£ l·ªùi. B·∫°n s·∫Ω ƒë∆∞·ª£c cung c·∫•p m·ªôt c√¢u tr·∫£ l·ªùi v√† nhi·ªám v·ª• c·ªßa b·∫°n l√† tr√≠ch xu·∫•t c√°c th·ª±c th·ªÉ t·ª´ c√¢u tr·∫£ l·ªùi ƒë√≥. C√°c th·ª±c th·ªÉ c√≥ th·ªÉ l√† t√™n ng∆∞·ªùi, ƒë·ªãa ƒëi·ªÉm, t·ªï ch·ª©c, s·ª± ki·ªán, v.v. H√£y tr·∫£ l·ªùi d∆∞·ªõi d·∫°ng m·ªôt danh s√°ch c√°c th·ª±c th·ªÉ.
                V√≠ d·ª•:
                C√¢u tr·∫£ l·ªùi: N·∫øu b·∫°n thu·ªôc ng√†nh kh√°c b·∫°n v·∫´n c√≥ th·ªÉ tham gia CLB ch√∫ng m√¨nh. N·∫øu ƒë·ªãnh h∆∞·ªõng c·ªßa b·∫°n ho√†n to√†n l√† theo CNTT th√¨ CLB ch·∫Øc ch·∫Øn l√† n∆°i ph√π h·ª£p nh·∫•t ƒë·ªÉ c√°c b·∫°n ph√°t tri·ªÉn. Tr·ªü ng·∫°i l·ªõn nh·∫•t s·∫Ω l√† do b·∫°n theo m·ªôt h∆∞·ªõng kh√°c n·ªØa n√™n s·∫Ω ph·∫£i t·∫≠p trung v√†o c·∫£ 2 m·∫£ng n√™n s·∫Ω c·∫ßn c·ªë g·∫Øng nhi·ªÅu h∆°n.
                ["ng√†nh kh√°c", "CLB", "CNTT", "m·∫£ng]
                C√¢u tr·∫£ l·ªùi: C√¢u l·∫°c b·ªô L·∫≠p Tr√¨nh PTIT (Programming PTIT), t√™n vi·∫øt t·∫Øt l√† PROPTIT ƒë∆∞·ª£c th√†nh l·∫≠p ng√†y 9/10/2011. V·ªõi ph∆∞∆°ng ch√¢m ho·∫°t ƒë·ªông "Chia s·∫ª ƒë·ªÉ c√πng nhau ph√°t tri·ªÉn", c√¢u l·∫°c b·ªô l√† n∆°i giao l∆∞u, ƒë√†o t·∫°o c√°c m√¥n l·∫≠p tr√¨nh v√† c√°c m√¥n h·ªçc trong tr∆∞·ªùng, t·∫°o ƒëi·ªÅu ki·ªán ƒë·ªÉ sinh vi√™n trong H·ªçc vi·ªán c√≥ m√¥i tr∆∞·ªùng h·ªçc t·∫≠p nƒÉng ƒë·ªông s√°ng t·∫°o. Slogan: L·∫≠p Tr√¨nh PTIT - L·∫≠p tr√¨nh t·ª´ tr√°i tim.
                ["C√¢u l·∫°c b·ªô L·∫≠p Tr√¨nh PTIT (Programming PTIT)", "PROPTIT", "9/10/2011", "Chia s·∫ª ƒë·ªÉ c√πng nhau ph√°t tri·ªÉn", "sinh vi√™n", "H·ªçc vi·ªán", "L·∫≠p Tr√¨nh PTIT - L·∫≠p tr√¨nh t·ª´ tr√°i tim"]"""
            }
        ]
        # NOTE: C√°c em c√≥ th·ªÉ thay ƒë·ªïi content n·∫øu mu·ªën
        messages_entities.append({
            "role": "user",
            "content": f"C√¢u tr·∫£ l·ªùi: {reply}"
        })
        # G·ªçi  API ƒë·ªÉ tr√≠ch xu·∫•t c√°c th·ª±c th·ªÉ
        entities = get_llm_response(messages_entities)
        entities = eval(entities) if entities.startswith('[') else []
        tmp = len(entities)
        for result in results:
            context = result['information']
            for entity in entities:
                if entity.strip() in context:
                    hits += 1
                    entities.remove(entity.strip())
        total_recall += hits / tmp if tmp > 0 else 0
        print(f"Query {index+1}/{len(df_train)} - Entities extracted: {tmp} - Hits: {hits}")
        time.sleep(1)

    return total_recall / len(df_train) if len(df_train) > 0 else 0



# H√†m t√≠nh to√°n t·∫•t c·∫£ metrics li√™n quan ƒë·∫øn Retrieval

def calculate_metrics_retrieval(file_clb_proptit, file_train , embedding, vector_db, train, reranker=None):
    # T·∫°o ra 1 b·∫£ng csv, c·ªôt th·ª© nh·∫•t l√† K value, c√°c c·ªôt c√≤n l·∫°i l√† metrics. S·∫Ω c√≥ 3 h√†ng t∆∞∆°ng tr∆∞ng v·ªõi k = 3, 5, 7
    k_values = [3, 5, 7]
    metrics = {
        "K": [],
        "hit@k": [],
        "recall@k": [],
        "precision@k": [],
        "f1@k": [],
        "map@k": [],
        "mrr@k": [],
        "ndcg@k": [],
        "context_precision@k": [],
        "context_recall@k": [],
        "context_entities_recall@k": []
    }
    # L∆∞u 2 ch·ªØ s·ªë th·∫≠p ph√¢n cho c√°c metrics
    for k in k_values:
        metrics["K"].append(k)
        metrics["hit@k"].append(round(hit_k(file_clb_proptit, file_train, embedding, vector_db, reranker, k), 2))
        metrics["recall@k"].append(round(recall_k(file_clb_proptit, file_train, embedding, vector_db, reranker, k), 2))
        metrics["precision@k"].append(round(precision_k(file_clb_proptit, file_train, embedding, vector_db, reranker, k), 2))
        metrics["f1@k"].append(round(f1_k(file_clb_proptit, file_train, embedding, vector_db, reranker, k), 2))
        metrics["map@k"].append(round(map_k(file_clb_proptit, file_train, embedding, vector_db, reranker, k), 2))
        metrics["mrr@k"].append(round(mrr_k(file_clb_proptit, file_train, embedding, vector_db, reranker, k), 2))
        metrics["ndcg@k"].append(round(ndcg_k(file_clb_proptit, file_train, embedding, vector_db, reranker, k), 2))
        metrics["context_precision@k"].append(round(context_precision_k(file_clb_proptit, file_train, embedding, vector_db, k), 2))
        metrics["context_recall@k"].append(round(context_recall_k(file_clb_proptit, file_train, embedding, vector_db, k), 2))
        metrics["context_entities_recall@k"].append(round(context_entities_recall_k(file_clb_proptit, file_train, embedding, vector_db, k), 2))
    # Chuy·ªÉn ƒë·ªïi metrics th√†nh DataFrame
    metrics_df = pd.DataFrame(metrics)
    # L∆∞u DataFrame v√†o file csv
    if train:
        metrics_df.to_csv("metrics_retrieval_train.csv", index=False)
    else:
        metrics_df.to_csv("metrics_retrieval_test.csv", index=False)
    return metrics_df

# C√°c h√†m ƒë√°nh gi√° LLM Answer

# H√†m String Presence

def string_presence_k(file_clb_proptit, file_train, embedding, vector_db,  k=5):
    df_clb = pd.read_csv(file_clb_proptit)
    df_train = pd.read_excel(file_train)

    total_presence = 0

    for index, row in df_train.iterrows():
        hits = 0
        query = row['Query']
        # T·∫°o embedding cho c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng
        user_embedding = embedding.encode(query)

        # T√¨m ki·∫øm th√¥ng tin li√™n quan trong c∆° s·ªü d·ªØ li·ªáu
        results = vector_db.query("information", user_embedding, limit=k)
        reply = row['Ground truth answer']
        messages = [
            {
                "role": "system",
                "content": """B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n cung c·∫•p th√¥ng tin v·ªÅ C√¢u l·∫°c b·ªô L·∫≠p tr√¨nh ProPTIT.
B·∫°n s·∫Ω nh·∫≠n ƒë∆∞·ª£c d·ªØ li·ªáu ng·ªØ c·∫£nh (context) t·ª´ m·ªôt h·ªá th·ªëng Retrieval-Augmented Generation (RAG) ch·ª©a c√°c th√¥ng tin ch√≠nh x√°c v·ªÅ CLB.

Nguy√™n t·∫Øc tr·∫£ l·ªùi:
1. Ch·ªâ s·ª≠ d·ª•ng th√¥ng tin t·ª´ context ƒë∆∞·ª£c cung c·∫•p ƒë·ªÉ tr·∫£ l·ªùi. Context s·∫Ω ƒë∆∞·ª£c cung c·∫•p ·ªü ƒë·∫ßu m·ªói query c·ªßa ng∆∞·ªùi d√πng. C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng n·∫±m ·ªü cu·ªëi. 
2. Tr√¨nh b√†y c√¢u tr·∫£ l·ªùi r√µ r√†ng, d·ªÖ hi·ªÉu. C√≥ th·ªÉ s·ª≠ d·ª•ng emoij icon khi c·∫ßn.
3. Tuy·ªát ƒë·ªëi kh√¥ng suy ƒëo√°n ho·∫∑c b·ªãa th√¥ng tin.
4. Gi·ªØ phong c√°ch tr·∫£ l·ªùi th√¢n thi·ªán, chuy√™n nghi·ªáp v√† nh·∫•t qu√°n.
5. Trong context c√≥ th·ªÉ ch·ª©a nhi·ªÅu th√¥ng tin kh√°c nhau, h√£y t·∫≠p trung v√†o c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng ƒë·ªÉ tr·∫£ l·ªùi ch√≠nh x√°c nh·∫•t.

Nhi·ªám v·ª• c·ªßa b·∫°n:
- Tr·∫£ l·ªùi c√°c c√¢u h·ªèi v·ªÅ CLB L·∫≠p tr√¨nh ProPTIT: l·ªãch s·ª≠, th√†nh vi√™n, ho·∫°t ƒë·ªông, s·ª± ki·ªán, d·ª± √°n, n·ªôi quy, th√†nh vi√™n ti√™u bi·ªÉu, v√† c√°c th√¥ng tin li√™n quan kh√°c."""
            }
        ]
        context = "Content t·ª´ c√°c t√†i li·ªáu li√™n quan:\n"
        context += "\n".join([result["information"] for result in results])
        # Th√™m context v√†o messages
        messages.append({
            "role": "user",
            "content": context + "\n\nC√¢u h·ªèi: " + query
        })
        # G·ªçi  API ƒë·ªÉ l·∫•y c√¢u tr·∫£ l·ªùi
        response = get_llm_response(messages)
        # Tr√≠ch xu·∫•t c√°c th·ª±c th·ªÉ t·ª´ c√¢u tr·∫£ l·ªùi b·∫±ng LLM
        # NOTE: C√°c em c√≥ th·ªÉ thay ƒë·ªïi message_entities n·∫øu mu·ªën
        messages_entities = [
            {
                "role": "system",
                "content": """B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n tr√≠ch xu·∫•t c√°c th·ª±c th·ªÉ t·ª´ c√¢u tr·∫£ l·ªùi. B·∫°n s·∫Ω ƒë∆∞·ª£c cung c·∫•p m·ªôt c√¢u tr·∫£ l·ªùi v√† nhi·ªám v·ª• c·ªßa b·∫°n l√† tr√≠ch xu·∫•t c√°c th·ª±c th·ªÉ t·ª´ c√¢u tr·∫£ l·ªùi ƒë√≥. C√°c th·ª±c th·ªÉ c√≥ th·ªÉ l√† t√™n ng∆∞·ªùi, ƒë·ªãa ƒëi·ªÉm, t·ªï ch·ª©c, s·ª± ki·ªán, v.v. H√£y tr·∫£ l·ªùi d∆∞·ªõi d·∫°ng m·ªôt danh s√°ch c√°c th·ª±c th·ªÉ.
                V√≠ d·ª•:
                C√¢u tr·∫£ l·ªùi: N·∫øu b·∫°n thu·ªôc ng√†nh kh√°c b·∫°n v·∫´n c√≥ th·ªÉ tham gia CLB ch√∫ng m√¨nh. N·∫øu ƒë·ªãnh h∆∞·ªõng c·ªßa b·∫°n ho√†n to√†n l√† theo CNTT th√¨ CLB ch·∫Øc ch·∫Øn l√† n∆°i ph√π h·ª£p nh·∫•t ƒë·ªÉ c√°c b·∫°n ph√°t tri·ªÉn. Tr·ªü ng·∫°i l·ªõn nh·∫•t s·∫Ω l√† do b·∫°n theo m·ªôt h∆∞·ªõng kh√°c n·ªØa n√™n s·∫Ω ph·∫£i t·∫≠p trung v√†o c·∫£ 2 m·∫£ng n√™n s·∫Ω c·∫ßn c·ªë g·∫Øng nhi·ªÅu h∆°n.
                ["ng√†nh kh√°c", "CLB", "CNTT", "m·∫£ng]
                C√¢u tr·∫£ l·ªùi: C√¢u l·∫°c b·ªô L·∫≠p Tr√¨nh PTIT (Programming PTIT), t√™n vi·∫øt t·∫Øt l√† PROPTIT ƒë∆∞·ª£c th√†nh l·∫≠p ng√†y 9/10/2011. V·ªõi ph∆∞∆°ng ch√¢m ho·∫°t ƒë·ªông "Chia s·∫ª ƒë·ªÉ c√πng nhau ph√°t tri·ªÉn", c√¢u l·∫°c b·ªô l√† n∆°i giao l∆∞u, ƒë√†o t·∫°o c√°c m√¥n l·∫≠p tr√¨nh v√† c√°c m√¥n h·ªçc trong tr∆∞·ªùng, t·∫°o ƒëi·ªÅu ki·ªán ƒë·ªÉ sinh vi√™n trong H·ªçc vi·ªán c√≥ m√¥i tr∆∞·ªùng h·ªçc t·∫≠p nƒÉng ƒë·ªông s√°ng t·∫°o. Slogan: L·∫≠p Tr√¨nh PTIT - L·∫≠p tr√¨nh t·ª´ tr√°i tim.
                ["C√¢u l·∫°c b·ªô L·∫≠p Tr√¨nh PTIT (Programming PTIT)", "PROPTIT", "9/10/2011", "Chia s·∫ª ƒë·ªÉ c√πng nhau ph√°t tri·ªÉn", "sinh vi√™n", "H·ªçc vi·ªán", "L·∫≠p Tr√¨nh PTIT - L·∫≠p tr√¨nh t·ª´ tr√°i tim"]"""
            }
        ]
        # Thay ƒë·ªïi content n·∫øu mu·ªën
        messages_entities.append({
            "role": "user",
            "content": f"C√¢u tr·∫£ l·ªùi: {reply}"
        })
        # G·ªçi  API ƒë·ªÉ tr√≠ch xu·∫•t c√°c th·ª±c th·ªÉ
        entities = get_llm_response(messages_entities)
        entities = eval(entities) if entities.startswith('[') else []
        for entity in entities:
            if entity.strip() in response:
                hits += 1
                # print(f"Entity '{entity.strip()}' found in response.")
        hits /= len(entities) if len(entities) > 0 else 0
        total_presence += hits
    return total_presence / len(df_train) if len(df_train) > 0 else 0


 

# H√†m Rouge-L

from rouge import Rouge
def rouge_l_k(file_clb_proptit, file_train, embedding, vector_db, k=5):
    df_clb = pd.read_csv(file_clb_proptit)
    df_train = pd.read_excel(file_train)

    rouge = Rouge()
    total_rouge_l = 0

    for index, row in df_train.iterrows():
        query = row['Query']
        # T·∫°o embedding cho c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng
        user_embedding = embedding.encode(query)

        # T√¨m ki·∫øm th√¥ng tin li√™n quan trong c∆° s·ªü d·ªØ li·ªáu
        results = vector_db.query("information", user_embedding, limit=k)
        reply = row['Ground truth answer']
        messages = [
            {
                "role": "system",
                "content": """B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n cung c·∫•p th√¥ng tin v·ªÅ C√¢u l·∫°c b·ªô L·∫≠p tr√¨nh ProPTIT.
B·∫°n s·∫Ω nh·∫≠n ƒë∆∞·ª£c d·ªØ li·ªáu ng·ªØ c·∫£nh (context) t·ª´ m·ªôt h·ªá th·ªëng Retrieval-Augmented Generation (RAG) ch·ª©a c√°c th√¥ng tin ch√≠nh x√°c v·ªÅ CLB.

Nguy√™n t·∫Øc tr·∫£ l·ªùi:
1. Ch·ªâ s·ª≠ d·ª•ng th√¥ng tin t·ª´ context ƒë∆∞·ª£c cung c·∫•p ƒë·ªÉ tr·∫£ l·ªùi. Context s·∫Ω ƒë∆∞·ª£c cung c·∫•p ·ªü ƒë·∫ßu m·ªói query c·ªßa ng∆∞·ªùi d√πng. C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng n·∫±m ·ªü cu·ªëi. 
2. N·∫øu ng∆∞·ªùi d√πng h·ªèi c√¢u h·ªèi kh√¥ng li√™n quan ƒë·∫øn CLB ProPTIT, h√£y tr·∫£ l·ªùi nh∆∞ b√¨nh th∆∞·ªùng, nh∆∞ng kh√¥ng s·ª≠ d·ª•ng th√¥ng tin t·ª´ context
3. Tr√¨nh b√†y c√¢u tr·∫£ l·ªùi r√µ r√†ng, d·ªÖ hi·ªÉu. C√≥ th·ªÉ s·ª≠ d·ª•ng emoij icon khi c·∫ßn.
4. Tuy·ªát ƒë·ªëi kh√¥ng suy ƒëo√°n ho·∫∑c b·ªãa th√¥ng tin.
5. Gi·ªØ phong c√°ch tr·∫£ l·ªùi th√¢n thi·ªán, chuy√™n nghi·ªáp v√† nh·∫•t qu√°n.
6. Trong context c√≥ th·ªÉ ch·ª©a nhi·ªÅu th√¥ng tin kh√°c nhau, h√£y t·∫≠p trung v√†o c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng ƒë·ªÉ tr·∫£ l·ªùi ch√≠nh x√°c nh·∫•t.

Nhi·ªám v·ª• c·ªßa b·∫°n:
- Tr·∫£ l·ªùi c√°c c√¢u h·ªèi v·ªÅ CLB L·∫≠p tr√¨nh ProPTIT: l·ªãch s·ª≠, th√†nh vi√™n, ho·∫°t ƒë·ªông, s·ª± ki·ªán, d·ª± √°n, n·ªôi quy, th√†nh vi√™n ti√™u bi·ªÉu, v√† c√°c th√¥ng tin li√™n quan kh√°c."""
            }
        ]
        context = "Content t·ª´ c√°c t√†i li·ªáu li√™n quan:\n"
        context += "\n".join([result["information"] for result in results])

        # Th√™m context v√†o messages
        messages.append({
            "role": "user",
            "content": context + "\n\nC√¢u h·ªèi: " + query
        })
        # G·ªçi API ƒë·ªÉ l·∫•y c√¢u tr·∫£ l·ªùi
        response = get_llm_response(messages)
        scores = rouge.get_scores(response, reply)
        rouge_l = scores[0]['rouge-l']['f']
        total_rouge_l += rouge_l
    return total_rouge_l / len(df_train) if len(df_train) > 0 else 0

# H√†m BLEU-4
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
def bleu_4_k(file_clb_proptit, file_train, embedding, vector_db, k=5):
    df_clb = pd.read_csv(file_clb_proptit)
    df_train = pd.read_excel(file_train)

    total_bleu_4 = 0
    smoothing_function = SmoothingFunction().method1

    for index, row in df_train.iterrows():
        query = row['Query']
        # T·∫°o embedding cho c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng
        user_embedding = embedding.encode(query)

        # T√¨m ki·∫øm th√¥ng tin li√™n quan trong c∆° s·ªü d·ªØ li·ªáu
        results = vector_db.query("information", user_embedding, limit=k)
        reply = row['Ground truth answer']
        messages = [
            {
                "role": "system",
                "content": """B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n cung c·∫•p th√¥ng tin v·ªÅ C√¢u l·∫°c b·ªô L·∫≠p tr√¨nh ProPTIT.
B·∫°n s·∫Ω nh·∫≠n ƒë∆∞·ª£c d·ªØ li·ªáu ng·ªØ c·∫£nh (context) t·ª´ m·ªôt h·ªá th·ªëng Retrieval-Augmented Generation (RAG) ch·ª©a c√°c th√¥ng tin ch√≠nh x√°c v·ªÅ CLB.

Nguy√™n t·∫Øc tr·∫£ l·ªùi:
1. Ch·ªâ s·ª≠ d·ª•ng th√¥ng tin t·ª´ context ƒë∆∞·ª£c cung c·∫•p ƒë·ªÉ tr·∫£ l·ªùi. Context s·∫Ω ƒë∆∞·ª£c cung c·∫•p ·ªü ƒë·∫ßu m·ªói query c·ªßa ng∆∞·ªùi d√πng. C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng n·∫±m ·ªü cu·ªëi. 
2. N·∫øu ng∆∞·ªùi d√πng h·ªèi c√¢u h·ªèi kh√¥ng li√™n quan ƒë·∫øn CLB ProPTIT, h√£y tr·∫£ l·ªùi nh∆∞ b√¨nh th∆∞·ªùng, nh∆∞ng kh√¥ng s·ª≠ d·ª•ng th√¥ng tin t·ª´ context
3. Tr√¨nh b√†y c√¢u tr·∫£ l·ªùi r√µ r√†ng, d·ªÖ hi·ªÉu. C√≥ th·ªÉ s·ª≠ d·ª•ng emoij icon khi c·∫ßn.
4. Tuy·ªát ƒë·ªëi kh√¥ng suy ƒëo√°n ho·∫∑c b·ªãa th√¥ng tin.
5. Gi·ªØ phong c√°ch tr·∫£ l·ªùi th√¢n thi·ªán, chuy√™n nghi·ªáp v√† nh·∫•t qu√°n.
6. Trong context c√≥ th·ªÉ ch·ª©a nhi·ªÅu th√¥ng tin kh√°c nhau, h√£y t·∫≠p trung v√†o c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng ƒë·ªÉ tr·∫£ l·ªùi ch√≠nh x√°c nh·∫•t.

Nhi·ªám v·ª• c·ªßa b·∫°n:
- Tr·∫£ l·ªùi c√°c c√¢u h·ªèi v·ªÅ CLB L·∫≠p tr√¨nh ProPTIT: l·ªãch s·ª≠, th√†nh vi√™n, ho·∫°t ƒë·ªông, s·ª± ki·ªán, d·ª± √°n, n·ªôi quy, th√†nh vi√™n ti√™u bi·ªÉu, v√† c√°c th√¥ng tin li√™n quan kh√°c."""
            }
        ]
        context = "Content t·ª´ c√°c t√†i li·ªáu li√™n quan:\n"
        context += "\n".join([result["information"] for result in results])

        # S·ª≠a content n·∫øu mu·ªën
        messages.append({
            "role": "user",
            "content": context + "\n\nC√¢u h·ªèi: " + query
        })
        # G·ªçi  API ƒë·ªÉ l·∫•y c√¢u tr·∫£ l·ªùi
        response = get_llm_response(messages)
        reference = reply.split()
        candidate = response.split()
        bleu_4 = sentence_bleu([reference], candidate, smoothing_function=smoothing_function)
        total_bleu_4 += bleu_4
    return total_bleu_4 / len(df_train) if len(df_train) > 0 else 0

# H√†m Groundedness (LLM Answer - Hallucination Detection)\

def groundedness_k(file_clb_proptit, file_train, embedding, vector_db, k=5):
    df_clb = pd.read_csv(file_clb_proptit)
    df_train = pd.read_excel(file_train)

    total_groundedness = 0

    for index, row in df_train.iterrows():
        hits = 0
        cnt = 0
        query = row['Query']
        # T·∫°o embedding cho c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng
        user_embedding = embedding.encode(query)

        # T√¨m ki·∫øm th√¥ng tin li√™n quan trong c∆° s·ªü d·ªØ li·ªáu
        results = vector_db.query("information", user_embedding, limit=k)
        reply = row['Ground truth answer']
        messages = [
            {
                "role": "system",
                "content": """B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n cung c·∫•p th√¥ng tin v·ªÅ C√¢u l·∫°c b·ªô L·∫≠p tr√¨nh ProPTIT.
B·∫°n s·∫Ω nh·∫≠n ƒë∆∞·ª£c d·ªØ li·ªáu ng·ªØ c·∫£nh (context) t·ª´ m·ªôt h·ªá th·ªëng Retrieval-Augmented Generation (RAG) ch·ª©a c√°c th√¥ng tin ch√≠nh x√°c v·ªÅ CLB.

Nguy√™n t·∫Øc tr·∫£ l·ªùi:
1. Ch·ªâ s·ª≠ d·ª•ng th√¥ng tin t·ª´ context ƒë∆∞·ª£c cung c·∫•p ƒë·ªÉ tr·∫£ l·ªùi. Context s·∫Ω ƒë∆∞·ª£c cung c·∫•p ·ªü ƒë·∫ßu m·ªói query c·ªßa ng∆∞·ªùi d√πng. C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng n·∫±m ·ªü cu·ªëi. 
2. N·∫øu ng∆∞·ªùi d√πng h·ªèi c√¢u h·ªèi kh√¥ng li√™n quan ƒë·∫øn CLB ProPTIT, h√£y tr·∫£ l·ªùi nh∆∞ b√¨nh th∆∞·ªùng, nh∆∞ng kh√¥ng s·ª≠ d·ª•ng th√¥ng tin t·ª´ context
3. Tr√¨nh b√†y c√¢u tr·∫£ l·ªùi r√µ r√†ng, d·ªÖ hi·ªÉu. C√≥ th·ªÉ s·ª≠ d·ª•ng emoij icon khi c·∫ßn.
4. Tuy·ªát ƒë·ªëi kh√¥ng suy ƒëo√°n ho·∫∑c b·ªãa th√¥ng tin.
5. Gi·ªØ phong c√°ch tr·∫£ l·ªùi th√¢n thi·ªán, chuy√™n nghi·ªáp v√† nh·∫•t qu√°n.
6. Trong context c√≥ th·ªÉ ch·ª©a nhi·ªÅu th√¥ng tin kh√°c nhau, h√£y t·∫≠p trung v√†o c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng ƒë·ªÉ tr·∫£ l·ªùi ch√≠nh x√°c nh·∫•t.

Nhi·ªám v·ª• c·ªßa b·∫°n:
- Tr·∫£ l·ªùi c√°c c√¢u h·ªèi v·ªÅ CLB L·∫≠p tr√¨nh ProPTIT: l·ªãch s·ª≠, th√†nh vi√™n, ho·∫°t ƒë·ªông, s·ª± ki·ªán, d·ª± √°n, n·ªôi quy, th√†nh vi√™n ti√™u bi·ªÉu, v√† c√°c th√¥ng tin li√™n quan kh√°c."""
            }
        ]
        context = "Content t·ª´ c√°c t√†i li·ªáu li√™n quan:\n"
        context += "\n".join([result["information"] for result in results])

        # Th√™m context v√†o messages, s·ª≠a content n·∫øu mu·ªën
        messages.append({
            "role": "user",
            "content": context + "\n\nC√¢u h·ªèi: " + query
        })
        # G·ªçi  API ƒë·ªÉ l·∫•y c√¢u tr·∫£ l·ªùi
        response = get_llm_response(messages)
      
    
        # T√°ch response th√†nh c√°c c√¢u
        sentences = response.split('. ')
        for sentence in sentences:
            # T·∫°o m·ªôt prompt ƒë·ªÉ ki·ªÉm tra t√≠nh groundedness c·ªßa c√¢u
            # S·ª≠a prompt n·∫øu mu·ªën
            messages_groundedness = [
                {
                    "role": "system",
                    "content": """B·∫°n l√† m·ªôt chuy√™n gia ƒë√°nh gi√° Groundedness trong h·ªá th·ªëng RAG, c√≥ nhi·ªám v·ª• ph√¢n lo·∫°i t·ª´ng c√¢u c·ªßa c√¢u tr·∫£ l·ªùi d·ª±a tr√™n ng·ªØ c·∫£nh ƒë√£ cho.
                    B·∫°n s·∫Ω ƒë∆∞·ª£c cung c·∫•p m·ªôt ng·ªØ c·∫£nh, m·ªôt c√¢u h·ªèi v√† m·ªôt c√¢u trong ph·∫ßn tr·∫£ l·ªùi t·ª´ m√¥ h√¨nh AI. Nhi·ªám v·ª• c·ªßa b·∫°n l√† ƒë√°nh gi√° c√¢u tr·∫£ l·ªùi ƒë√≥ d·ª±a tr√™n ng·ªØ c·∫£nh v√† c√¢u h·ªèi.
                    Input:
                    Question: C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng
                    Contexts: M·ªôt ho·∫∑c nhi·ªÅu ƒëo·∫°n vƒÉn b·∫£n ƒë∆∞·ª£c truy xu·∫•t
                    Answer: Ch·ªâ m·ªôt c√¢u trong ƒëo·∫°n vƒÉn b·∫£n LLM sinh ra
                    B·∫°n h√£y ƒë√°nh gi√° d·ª±a tr√™n c√°c nh√£n sau: 
                    supported: N·ªôi dung c√¢u ƒë∆∞·ª£c ng·ªØ c·∫£nh h·ªó tr·ª£ ho·∫∑c suy ra tr·ª±c ti·∫øp.
                    unsupported: N·ªôi dung c√¢u kh√¥ng ƒë∆∞·ª£c ng·ªØ c·∫£nh h·ªó tr·ª£, v√† kh√¥ng th·ªÉ suy ra t·ª´ ƒë√≥.
                    contradictory: N·ªôi dung c√¢u tr√°i ng∆∞·ª£c ho·∫∑c m√¢u thu·∫´n v·ªõi ng·ªØ c·∫£nh.
                    no_rad: C√¢u kh√¥ng y√™u c·∫ßu ki·ªÉm tra th·ª±c t·∫ø (v√≠ d·ª•: c√¢u ch√†o h·ªèi, √Ω ki·∫øn c√° nh√¢n, c√¢u h·ªèi tu t·ª´, disclaimers).
                    H√£y tr·∫£ l·ªùi b·∫±ng m·ªôt trong c√°c nh√£n tr√™n, kh√¥ng gi·∫£i th√≠ch g√¨ th√™m. Ch·ªâ tr·∫£ l·ªùi m·ªôt t·ª´ duy nh·∫•t l√† nh√£n ƒë√≥.
                    V√≠ d·ª•:
                    Question: B·∫°n c√≥ th·ªÉ cho t√¥i bi·∫øt v·ªÅ l·ªãch s·ª≠ c·ªßa C√¢u l·∫°c b·ªô L·∫≠p tr√¨nh ProPTIT kh√¥ng?
                    Contexts: C√¢u l·∫°c b·ªô L·∫≠p tr√¨nh ProPTIT ƒë∆∞·ª£c ra ƒë·ªùi v√†o nƒÉm 2011, v·ªõi m·ª•c ti√™u t·∫°o ra m·ªôt m√¥i tr∆∞·ªùng h·ªçc t·∫≠p v√† giao l∆∞u cho c√°c sinh vi√™n ƒëam m√™ l·∫≠p tr√¨nh.
                    Answer: C√¢u l·∫°c b·ªô L·∫≠p tr√¨nh ProPTIT ƒë∆∞·ª£c th√†nh l·∫≠p v√†o nƒÉm 2011.
                    supported"""
                }
            ]
            # S·ª≠a content n·∫øu mu·ªën
            messages_groundedness.append({
                "role": "user",
                "content": f"Question: {query}\n\nContexts: {context}\n\nAnswer: {sentence.strip()}"
            })
            # G·ªçi  API ƒë·ªÉ ƒë√°nh gi√° groundedness
            groundedness_reply = get_llm_response(messages_groundedness)

            if groundedness_reply == "supported":
                hits += 1
                cnt += 1
            elif groundedness_reply == "unsupported" or groundedness_reply == "contradictory":
                cnt += 1
        total_groundedness += hits / cnt if cnt > 0 else 0
    return total_groundedness / len(df_train) if len(df_train) > 0 else 0 

# H√†m Response Relevancy (LLM Answer - Measures relevance)


def generate_related_questions(response, embedding):
    # S·ª≠a system prompt n·∫øu mu·ªën
    messages_related = [
        {
            "role": "system",
            "content": """B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n t·∫°o ra c√°c c√¢u h·ªèi li√™n quan t·ª´ m·ªôt c√¢u tr·∫£ l·ªùi. B·∫°n s·∫Ω ƒë∆∞·ª£c cung c·∫•p m·ªôt c√¢u tr·∫£ l·ªùi v√† nhi·ªám v·ª• c·ªßa b·∫°n l√† t·∫°o ra c√°c c√¢u h·ªèi li√™n quan ƒë·∫øn c√¢u tr·∫£ l·ªùi ƒë√≥. H√£y t·∫°o ra √≠t nh·∫•t 5 c√¢u h·ªèi li√™n quan, m·ªói c√¢u h·ªèi n√™n ng·∫Øn g·ªçn v√† r√µ r√†ng. Tr·∫£ l·ªùi d∆∞·ªõi d·∫°ng list c√°c c√¢u h·ªèi nh∆∞ ·ªü v√≠ d·ª• d∆∞·ªõi. L∆ØU √ù: Tr·∫£ l·ªùi d∆∞·ªõi d·∫°ng ["c√¢u h·ªèi 1", "c√¢u h·ªèi 2", "c√¢u h·ªèi 3", ...], bao g·ªìm c·∫£ d·∫•u ngo·∫∑c vu√¥ng.
            V√≠ d·ª•:
            C√¢u tr·∫£ l·ªùi: C√¢u l·∫°c b·ªô L·∫≠p Tr√¨nh PTIT (Programming PTIT), t√™n vi·∫øt t·∫Øt l√† PROPTIT ƒë∆∞·ª£c th√†nh l·∫≠p ng√†y 9/10/2011. V·ªõi ph∆∞∆°ng ch√¢m ho·∫°t ƒë·ªông "Chia s·∫ª ƒë·ªÉ c√πng nhau ph√°t tri·ªÉn", c√¢u l·∫°c b·ªô l√† n∆°i giao l∆∞u, ƒë√†o t·∫°o c√°c m√¥n l·∫≠p tr√¨nh v√† c√°c m√¥n h·ªçc trong tr∆∞·ªùng, t·∫°o ƒëi·ªÅu ki·ªán ƒë·ªÉ sinh vi√™n trong H·ªçc vi·ªán c√≥ m√¥i tr∆∞·ªùng h·ªçc t·∫≠p nƒÉng ƒë·ªông s√°ng t·∫°o. Slogan: L·∫≠p Tr√¨nh PTIT - L·∫≠p tr√¨nh t·ª´ tr√°i tim.
            Output c·ªßa b·∫°n: "["CLB L·∫≠p Tr√¨nh PTIT ƒë∆∞·ª£c th√†nh l·∫≠p khi n√†o?", "Slogan c·ªßa CLB l√† g√¨?", "M·ª•c ti√™u c·ªßa CLB l√† g√¨?"]"
            C√¢u tr·∫£ l·ªùi: N·∫øu b·∫°n thu·ªôc ng√†nh kh√°c b·∫°n v·∫´n c√≥ th·ªÉ tham gia CLB ch√∫ng m√¨nh. N·∫øu ƒë·ªãnh h∆∞·ªõng c·ªßa b·∫°n ho√†n to√†n l√† theo CNTT th√¨ CLB ch·∫Øc ch·∫Øn l√† n∆°i ph√π h·ª£p nh·∫•t ƒë·ªÉ c√°c b·∫°n ph√°t tri·ªÉn. Tr·ªü ng·∫°i l·ªõn nh·∫•t s·∫Ω l√† do b·∫°n theo m·ªôt h∆∞·ªõng kh√°c n·ªØa n√™n s·∫Ω ph·∫£i t·∫≠p trung v√†o c·∫£ 2 m·∫£ng n√™n s·∫Ω c·∫ßn c·ªë g·∫Øng nhi·ªÅu h∆°n.
            Output c·ªßa b·∫°n: "["Ng√†nh n√†o c√≥ th·ªÉ tham gia CLB?", "CLB ph√π h·ª£p v·ªõi nh·ªØng ai?", "Tr·ªü ng·∫°i l·ªõn nh·∫•t khi tham gia CLB l√† g√¨?"]"""
        }
    ]
    # S·ª≠a content n·∫øu mu·ªën
    messages_related.append({
        "role": "user",
        "content": f"C√¢u tr·∫£ l·ªùi: {response}"
    })
    # G·ªçi  API ƒë·ªÉ t·∫°o ra c√°c c√¢u h·ªèi li√™n quan
    related_questions = get_llm_response(messages_related)
    return related_questions

def response_relevancy_k(file_clb_proptit, file_train, embedding, vector_db, k=5):
    df_clb = pd.read_csv(file_clb_proptit)
    df_train = pd.read_excel(file_train)

    total_relevancy = 0

    for index, row in df_train.iterrows():
        hits = 0
        cnt = 0
        query = row['Query']
        # T·∫°o embedding cho c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng
        user_embedding = embedding.encode(query)

        # T√¨m ki·∫øm th√¥ng tin li√™n quan trong c∆° s·ªü d·ªØ li·ªáu
        results = vector_db.query("information", user_embedding, limit=k)
        reply = row['Ground truth answer']
        messages = [
            {
                "role": "system",
                "content": """B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n cung c·∫•p th√¥ng tin v·ªÅ C√¢u l·∫°c b·ªô L·∫≠p tr√¨nh ProPTIT.
B·∫°n s·∫Ω nh·∫≠n ƒë∆∞·ª£c d·ªØ li·ªáu ng·ªØ c·∫£nh (context) t·ª´ m·ªôt h·ªá th·ªëng Retrieval-Augmented Generation (RAG) ch·ª©a c√°c th√¥ng tin ch√≠nh x√°c v·ªÅ CLB.

Nguy√™n t·∫Øc tr·∫£ l·ªùi:
1. Ch·ªâ s·ª≠ d·ª•ng th√¥ng tin t·ª´ context ƒë∆∞·ª£c cung c·∫•p ƒë·ªÉ tr·∫£ l·ªùi. Context s·∫Ω ƒë∆∞·ª£c cung c·∫•p ·ªü ƒë·∫ßu m·ªói query c·ªßa ng∆∞·ªùi d√πng. C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng n·∫±m ·ªü cu·ªëi. 
2. N·∫øu ng∆∞·ªùi d√πng h·ªèi c√¢u h·ªèi kh√¥ng li√™n quan ƒë·∫øn CLB ProPTIT, h√£y tr·∫£ l·ªùi nh∆∞ b√¨nh th∆∞·ªùng, nh∆∞ng kh√¥ng s·ª≠ d·ª•ng th√¥ng tin t·ª´ context
3. Tr√¨nh b√†y c√¢u tr·∫£ l·ªùi r√µ r√†ng, d·ªÖ hi·ªÉu. C√≥ th·ªÉ s·ª≠ d·ª•ng emoij icon khi c·∫ßn.
4. Tuy·ªát ƒë·ªëi kh√¥ng suy ƒëo√°n ho·∫∑c b·ªãa th√¥ng tin.
5. Gi·ªØ phong c√°ch tr·∫£ l·ªùi th√¢n thi·ªán, chuy√™n nghi·ªáp v√† nh·∫•t qu√°n.
6. Trong context c√≥ th·ªÉ ch·ª©a nhi·ªÅu th√¥ng tin kh√°c nhau, h√£y t·∫≠p trung v√†o c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng ƒë·ªÉ tr·∫£ l·ªùi ch√≠nh x√°c nh·∫•t.

Nhi·ªám v·ª• c·ªßa b·∫°n:
- Tr·∫£ l·ªùi c√°c c√¢u h·ªèi v·ªÅ CLB L·∫≠p tr√¨nh ProPTIT: l·ªãch s·ª≠, th√†nh vi√™n, ho·∫°t ƒë·ªông, s·ª± ki·ªán, d·ª± √°n, n·ªôi quy, th√†nh vi√™n ti√™u bi·ªÉu, v√† c√°c th√¥ng tin li√™n quan kh√°c."""
            }
        ]
        context = "Content t·ª´ c√°c t√†i li·ªáu li√™n quan:\n"
        context += "\n".join([result["information"] for result in results])

        # S·ª≠a content n·∫øu mu·ªën
        messages.append({
            "role": "user",
            "content": context + "\n\nC√¢u h·ªèi: " + query
        })
        # G·ªçi  API ƒë·ªÉ l·∫•y c√¢u tr·∫£ l·ªùi
        response = get_llm_response(messages)

        # D√πng c√¢u tr·∫£ l·ªùi c·ªßa LLM ƒë·ªÉ sinh ra c√°c c√¢u h·ªèi li√™n quan
        related_questions = generate_related_questions(response, embedding) # "["CLB L·∫≠p Tr√¨nh PTIT ƒë∆∞·ª£c th√†nh l·∫≠p khi n√†o?", "Slogan c·ªßa CLB l√† g√¨?", "M·ª•c ti√™u c·ªßa CLB l√† g√¨?"]"
        related_questions = eval(related_questions) if related_questions else []  # Chuy·ªÉn ƒë·ªïi chu·ªói th√†nh danh s√°ch
        for question in related_questions:
            question_embedding = embedding.encode(question)
            # T√≠nh score relevancy gi·ªØa c√¢u h·ªèi v√† query
            score = similarity(user_embedding, question_embedding)
            hits += score
        total_relevancy += hits / len(related_questions) if len(related_questions) > 0 else 0
    return total_relevancy / len(df_train) if len(df_train) > 0 else 0


# H√†m Noise Sensitivity (LLM Answer - Robustness to Hallucination)

def noise_sensitivity_k(file_clb_proptit, file_train, embedding, vector_db, k=5):
    df_clb = pd.read_csv(file_clb_proptit)
    df_train = pd.read_excel(file_train)

    total_sensitivity = 0

    for index, row in df_train.iterrows():
        hits = 0
        cnt = 0
        query = row['Query']
        # T·∫°o embedding cho c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng
        user_embedding = embedding.encode(query)

        # T√¨m ki·∫øm th√¥ng tin li√™n quan trong c∆° s·ªü d·ªØ li·ªáu
        results = vector_db.query("information", user_embedding, limit=k)
        reply = row['Ground truth answer']
        messages = [
            {
                "role": "system",
                "content": """B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n cung c·∫•p th√¥ng tin v·ªÅ C√¢u l·∫°c b·ªô L·∫≠p tr√¨nh ProPTIT.
B·∫°n s·∫Ω nh·∫≠n ƒë∆∞·ª£c d·ªØ li·ªáu ng·ªØ c·∫£nh (context) t·ª´ m·ªôt h·ªá th·ªëng Retrieval-Augmented Generation (RAG) ch·ª©a c√°c th√¥ng tin ch√≠nh x√°c v·ªÅ CLB.

Nguy√™n t·∫Øc tr·∫£ l·ªùi:
1. Ch·ªâ s·ª≠ d·ª•ng th√¥ng tin t·ª´ context ƒë∆∞·ª£c cung c·∫•p ƒë·ªÉ tr·∫£ l·ªùi. Context s·∫Ω ƒë∆∞·ª£c cung c·∫•p ·ªü ƒë·∫ßu m·ªói query c·ªßa ng∆∞·ªùi d√πng. C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng n·∫±m ·ªü cu·ªëi. 
2. N·∫øu ng∆∞·ªùi d√πng h·ªèi c√¢u h·ªèi kh√¥ng li√™n quan ƒë·∫øn CLB ProPTIT, h√£y tr·∫£ l·ªùi nh∆∞ b√¨nh th∆∞·ªùng, nh∆∞ng kh√¥ng s·ª≠ d·ª•ng th√¥ng tin t·ª´ context
3. Tr√¨nh b√†y c√¢u tr·∫£ l·ªùi r√µ r√†ng, d·ªÖ hi·ªÉu. C√≥ th·ªÉ s·ª≠ d·ª•ng emoij icon khi c·∫ßn.
4. Tuy·ªát ƒë·ªëi kh√¥ng suy ƒëo√°n ho·∫∑c b·ªãa th√¥ng tin.
5. Gi·ªØ phong c√°ch tr·∫£ l·ªùi th√¢n thi·ªán, chuy√™n nghi·ªáp v√† nh·∫•t qu√°n.
6. Trong context c√≥ th·ªÉ ch·ª©a nhi·ªÅu th√¥ng tin kh√°c nhau, h√£y t·∫≠p trung v√†o c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng ƒë·ªÉ tr·∫£ l·ªùi ch√≠nh x√°c nh·∫•t.

Nhi·ªám v·ª• c·ªßa b·∫°n:
- Tr·∫£ l·ªùi c√°c c√¢u h·ªèi v·ªÅ CLB L·∫≠p tr√¨nh ProPTIT: l·ªãch s·ª≠, th√†nh vi√™n, ho·∫°t ƒë·ªông, s·ª± ki·ªán, d·ª± √°n, n·ªôi quy, th√†nh vi√™n ti√™u bi·ªÉu, v√† c√°c th√¥ng tin li√™n quan kh√°c."""
            }
        ]
        context =  "Content t·ª´ c√°c t√†i li·ªáu li√™n quan:\n"
        context += "\n".join([result["information"] for result in results])

        # Th√™m context v√†o messages, s·ª≠a content n·∫øu mu·ªën
        messages.append({
            "role": "user",
            "content": context + "\n\nC√¢u h·ªèi: " + query
        })
        # G·ªçi  API ƒë·ªÉ l·∫•y c√¢u tr·∫£ l·ªùi
        response = get_llm_response(messages)

        sentences = response.split('. ')
        for sentence in sentences:
            # S·ª≠a prompt n·∫øu mu·ªën
            messages_sensitivity = [
                {
                    "role": "system",
                    "content": """B·∫°n l√† m·ªôt chuy√™n gia ƒë√°nh gi√° ƒë·ªô nh·∫°y c·∫£m c·ªßa c√¢u tr·∫£ l·ªùi trong h·ªá th·ªëng RAG, c√≥ nhi·ªám v·ª• ph√¢n lo·∫°i t·ª´ng c√¢u c·ªßa c√¢u tr·∫£ l·ªùi d·ª±a tr√™n ng·ªØ c·∫£nh ƒë√£ cho.
                    B·∫°n s·∫Ω ƒë∆∞·ª£c cung c·∫•p m·ªôt ng·ªØ c·∫£nh, m·ªôt c√¢u h·ªèi v√† m·ªôt c√¢u trong ph·∫ßn tr·∫£ l·ªùi t·ª´ m√¥ h√¨nh AI. Nhi·ªám v·ª• c·ªßa b·∫°n l√† ƒë√°nh gi√° c√¢u tr·∫£ l·ªùi ƒë√≥ d·ª±a tr√™n ng·ªØ c·∫£nh v√† c√¢u h·ªèi.
                    Input:
                    Question: C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng
                    Contexts: M·ªôt ho·∫∑c nhi·ªÅu ƒëo·∫°n vƒÉn b·∫£n ƒë∆∞·ª£c truy xu·∫•t
                    Answer: Ch·ªâ m·ªôt c√¢u trong ƒëo·∫°n vƒÉn b·∫£n LLM sinh ra
                    B·∫°n h√£y ƒë√°nh gi√° d·ª±a tr√™n c√°c nh√£n sau: 
                    1: N·ªôi dung c√¢u ƒë∆∞·ª£c ng·ªØ c·∫£nh h·ªó tr·ª£ ho·∫∑c suy ra tr·ª±c ti·∫øp.
                    0: N·ªôi dung c√¢u kh√¥ng ƒë∆∞·ª£c ng·ªØ c·∫£nh h·ªó tr·ª£, v√† kh√¥ng th·ªÉ suy ra t·ª´ ƒë√≥.
                    V√≠ d·ª•:
                    Question: B·∫°n c√≥ th·ªÉ cho t√¥i bi·∫øt v·ªÅ l·ªãch s·ª≠ c·ªßa C√¢u l·∫°c b·ªô L·∫≠p tr√¨nh ProPTIT kh√¥ng?
                    Contexts: C√¢u l·∫°c b·ªô L·∫≠p tr√¨nh ProPTIT ƒë∆∞·ª£c ra ƒë·ªùi v√†o nƒÉm 2011, v·ªõi m·ª•c ti√™u t·∫°o ra m·ªôt m√¥i tr∆∞·ªùng h·ªçc t·∫≠p v√† giao l∆∞u cho c√°c sinh vi√™n ƒëam m√™ l·∫≠p tr√¨nh.
                    Answer: C√¢u l·∫°c b·ªô L·∫≠p tr√¨nh ProPTIT ƒë∆∞·ª£c th√†nh l·∫≠p v√†o nƒÉm 2011.
                    1
                    Question: C√¢u l·∫°c b·ªô L·∫≠p tr√¨nh ProPTIT ƒë∆∞·ª£c th√†nh l·∫≠p v√†o nƒÉm 2011. B·∫°n c√≥ bi·∫øt ng√†y c·ª• th·ªÉ kh√¥ng?
                    Contexts: C√¢u l·∫°c b·ªô L·∫≠p tr√¨nh ProPTIT ƒë∆∞·ª£c ra ƒë·ªùi v√†o nƒÉm 2011, v·ªõi m·ª•c ti√™u t·∫°o ra m·ªôt m√¥i tr∆∞·ªùng h·ªçc t·∫≠p v√† giao l∆∞u cho c√°c sinh vi√™n ƒëam m√™ l·∫≠p tr√¨nh.
                    Answer: C√¢u l·∫°c b·ªô L·∫≠p tr√¨nh ProPTIT l√† CLB thu·ªôc PTIT.
                    0"""
                }
            ]
            # S·ª≠a prompt n·∫øu mu·ªën
            messages_sensitivity.append({
                "role": "user",
                "content": f"Question: {query}\n\nContexts: {context}\n\nAnswer: {sentence.strip()}"
            })
            # G·ªçi  API ƒë·ªÉ ƒë√°nh gi√° ƒë·ªô nh·∫°y c·∫£m
            sensitivity_reply = get_llm_response(messages_sensitivity)
            if sensitivity_reply == "0":
                hits += 1
        total_sensitivity += hits / len(sentences) if len(sentences) > 0 else 0
    return total_sensitivity / len(df_train) if len(df_train) > 0 else 0


# H√†m ƒë·ªÉ t√≠nh to√°n to√†n b·ªô metrics trong module LLM Answer

def calculate_metrics_llm_answer(file_clb_proptit, file_train, embedding, vector_db, train, reranker=None):
    # T·∫°o ra 1 b·∫£ng csv, c·ªôt th·ª© nh·∫•t l√† K value, c√°c c·ªôt c√≤n l·∫°i l√† metrics. S·∫Ω c√≥ 3 h√†ng t∆∞∆°ng tr∆∞ng v·ªõi k = 3, 5, 7
    k_values = [3, 5, 7]
    metrics = {
        "K": [],
        "string_presence@k": [],
        "rouge_l@k": [],
        "bleu_4@k": [],
        "groundedness@k": [],
        "response_relevancy@k": [],
        "noise_sensitivity@k": []
    }
    # L∆∞u 2 ch·ªØ s·ªë th·∫≠p ph√¢n cho c√°c metrics
    for k in k_values:
        metrics["K"].append(k)
        metrics["string_presence@k"].append(round(string_presence_k(file_clb_proptit, file_train, embedding, vector_db, k), 2))
        metrics["rouge_l@k"].append(round(rouge_l_k(file_clb_proptit, file_train, embedding, vector_db, k), 2))
        metrics["bleu_4@k"].append(round(bleu_4_k(file_clb_proptit, file_train, embedding, vector_db, k), 2))
        metrics["groundedness@k"].append(round(groundedness_k(file_clb_proptit, file_train, embedding, vector_db, k), 2))
        metrics["response_relevancy@k"].append(round(response_relevancy_k(file_clb_proptit, file_train, embedding, vector_db, k), 2))
        metrics["noise_sensitivity@k"].append(round(noise_sensitivity_k(file_clb_proptit, file_train, embedding, vector_db, k), 2))
    # Chuy·ªÉn ƒë·ªïi metrics th√†nh DataFrame
    metrics_df = pd.DataFrame(metrics)
    # L∆∞u DataFrame v√†o file csv
    if train:
        metrics_df.to_csv("metrics_llm_answer_train.csv", index=False)
    else:
        metrics_df.to_csv("metrics_llm_answer_test.csv", index=False)
    return metrics_df

