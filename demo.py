import streamlit as st
import pandas as pd
import numpy as np
import json
import time
from docx import Document
from ingest_utils import build_collection_from_docx
import plotly.express as px
import plotly.graph_objects as go
from embeddings import Embeddings
from vector_db import VectorDatabase
from rerank import Reranker
from metrics_rag import (
    calculate_metrics_retrieval, 
    calculate_metrics_llm_answer,
    hit_k, recall_k, precision_k, context_precision_k, 
    context_recall_k, context_entities_recall_k, ndcg_k,
    retrieve_and_rerank
)
import os
from dotenv import load_dotenv

load_dotenv()
import llm_config
from llm_config import get_llm_response

# Page config
st.set_page_config(
    page_title="NeoRAG Cup 2025 - Demo",
    page_icon="üöÄ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS
st.markdown("""
<style>
    .metric-card {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 0.5rem;
        border-left: 4px solid #ff6b6b;
        margin: 0.5rem 0;
    }
    .chat-message {
        padding: 1rem;
        border-radius: 0.5rem;
        margin: 0.5rem 0;
    }
    .user-message {
        background-color: #e3f2fd;
        border-left: 4px solid #2196f3;
    }
    .assistant-message {
        background-color: #ffffff;
        padding: 1rem;
        border-radius: 0.75rem;
        border-left: 4px solid #9c27b0;
        box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        color: #333333;
        font-family: "Segoe UI", Tahoma, Geneva, Verdana, sans-serif;
        transition: background-color 0.3s ease;
    }
    .assistant-message:hover {
        background-color: #f5f0f8;
    }
    .sidebar .sidebar-content {
        background-color: #f8f9fa;
    }
    .stTabs [data-baseweb="tab-list"] {
        gap: 8px;
    }
    .stTabs [data-baseweb="tab"] {
        height: 50px;
        padding-left: 20px;
        padding-right: 20px;
    }
</style>
""", unsafe_allow_html=True)

# Initialize session state
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []
if 'vector_db' not in st.session_state:
    st.session_state.vector_db = None
if 'embedding_model' not in st.session_state:
    st.session_state.embedding_model = None
if 'reranker' not in st.session_state:
    st.session_state.reranker = None
if 'initialized' not in st.session_state:
    st.session_state.initialized = False

# Header
st.title("üöÄ NeoRAG Cup 2025 - Demo System")
st.markdown("---")

# Sidebar Configuration
with st.sidebar:
    st.header("‚öôÔ∏è C·∫•u h√¨nh h·ªá th·ªëng")
    
    # Default optimal configuration
    st.info("üéØ **C·∫•u h√¨nh t·ªëi ∆∞u m·∫∑c ƒë·ªãnh:** ChromaDB + BGE-M3 + ViRanker + Query Expansion")
    
    # LLM Configuration
    st.markdown("### ü§ñ LLM Configuration")
    llm_provider = st.selectbox("LLM Provider:", ["nvidia", "groq"], index=0)
    llm_config.LLM_PROVIDER = llm_provider
    if llm_provider == "groq":
        llm_model = "meta-llama/llama-4-maverick-17b-128e-instruct"
        llm_config.GROQ_MODEL = llm_model
    else:
        llm_model = "writer/palmyra-med-70b"
        llm_config.NVIDIA_MODEL = llm_model
    st.info(f"ü§ñ LLM Model c·ªë ƒë·ªãnh: {llm_model}")
    # Advanced LLM settings
    with st.expander("üîß Tham s·ªë LLM", expanded=False):
        col1, col2 = st.columns(2)
        with col1:
            max_tokens = st.slider("Max Tokens", 256, 2048, 1024, 128)
        with col2:
            temperature = st.slider("Temperature", 0.0, 1.0, 0.4, 0.1)
    
    # Status indicators
    st.markdown("### üìä Tr·∫°ng th√°i h·ªá th·ªëng")
    if st.session_state.initialized:
        st.success("‚úÖ H·ªá th·ªëng ƒë√£ s·∫µn s√†ng")
        if st.session_state.vector_db:
            try:
                doc_count = st.session_state.vector_db.count_documents("information")
                st.info(f"üìö {doc_count} documents ƒë√£ ƒë∆∞·ª£c load")
            except:
                pass
    else:
        st.warning("‚ö†Ô∏è Ch∆∞a kh·ªüi t·∫°o")
    
    # Initialize system button
    init_col1, init_col2 = st.columns([2, 1])
    with init_col1:
        if st.button("üîß Kh·ªüi t·∫°o h·ªá th·ªëng", type="primary", use_container_width=True):
            with st.spinner("ƒêang kh·ªüi t·∫°o h·ªá th·ªëng..."):
                try:
                    db_type = "chromadb"
                    model_name = "BAAI/bge-m3"
                    embedding_type = "sentence_transformers"
                    reranker_model = "namdp-ptit/ViRanker"
                    use_reranker = True
                    use_query_expansion = True
                    
                    # Initialize vector database
                    st.session_state.vector_db = VectorDatabase(db_type=db_type)
                    
                    # Initialize embedding model
                    st.session_state.embedding_model = Embeddings(
                        model_name=model_name,
                        type=embedding_type
                    )
                    
                    # Initialize reranker
                    st.session_state.reranker = Reranker(model_name=reranker_model)
                    
                    # Load documents if not already loaded (reusing ingest utility)
                    current_count = st.session_state.vector_db.count_documents("information")
                    if current_count == 0:
                        progress_bar = st.progress(0)
                        def on_progress(done, total):
                            progress_bar.progress(done / max(total, 1))
                        inserted = build_collection_from_docx(
                            doc_path="CLB_PROPTIT.docx",
                            embedding_model=st.session_state.embedding_model,
                            vector_db=st.session_state.vector_db,
                            collection_name="information",
                            rebuild=False,
                            progress_callback=on_progress,
                        )
                        st.success(f"ƒê√£ load {inserted} documents v√†o database!")
                    else:
                        st.info("Documents ƒë√£ t·ªìn t·∫°i trong database.")
                    
                    st.session_state.initialized = True
                    st.success("H·ªá th·ªëng ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o th√†nh c√¥ng!")
                    st.rerun()  # Refresh to update status
                    
                except Exception as e:
                    st.error(f"L·ªói khi kh·ªüi t·∫°o h·ªá th·ªëng: {str(e)}")
    
    with init_col2:
        if st.session_state.initialized:
            if st.button("üîÑ", help="Reset h·ªá th·ªëng"):
                st.session_state.initialized = False
                st.session_state.vector_db = None
                st.session_state.embedding_model = None
                st.session_state.reranker = None
                st.rerun()

# Main content tabs
tab1, tab2 = st.tabs(["üí¨ Chat Demo", "üìà Performance"])

with tab1:
    st.header("üí¨ Chat v·ªõi h·ªá th·ªëng RAG")
    
    if not st.session_state.initialized:
        st.warning("‚ö†Ô∏è Vui l√≤ng kh·ªüi t·∫°o h·ªá th·ªëng ·ªü sidebar tr∆∞·ªõc khi s·ª≠ d·ª•ng!")
    else:
        # Chat configuration
        col1, col2, col3 = st.columns([2, 1, 1])
        
        with col1:
            user_query = st.text_input(
                "H·ªèi v·ªÅ CLB ProPTIT:",
                placeholder="V√≠ d·ª•: CLB ProPTIT ƒë∆∞·ª£c th√†nh l·∫≠p khi n√†o?",
                key="user_input"
            )
        
        with col2:
            top_k = st.selectbox("Top K", [3, 5, 7, 10], index=0)
        
        with col3:
            ask_button = st.button("üöÄ H·ªèi", type="primary")
        
        if ask_button and user_query:
            with st.spinner("ƒêang t√¨m ki·∫øm v√† t·∫°o c√¢u tr·∫£ l·ªùi..."):
                try:
                    # Retrieve relevant documents
                    start_time = time.time()
                    
                    results = retrieve_and_rerank(
                        query=user_query,
                        embedding=st.session_state.embedding_model,
                        vector_db=st.session_state.vector_db,
                        reranker=st.session_state.reranker,
                        k=top_k,
                        use_query_expansion=True
                    )
                    
                    retrieval_time = time.time() - start_time
                    
                    # Display retrieved documents
                    st.subheader("üìã Documents ƒë∆∞·ª£c truy xu·∫•t:")
                    for i, doc in enumerate(results): 
                        with st.expander(f"Document {i+1}: {doc.get('title', 'N/A')}"):
                            st.write(doc.get('information', 'N/A'))
                            if 'score' in doc:
                                st.write(f"**Score:** {doc['score']:.4f}")
                    
                    # Generate answer using selected LLM provider
                    keywords = [word.strip(',.?!"') for word in user_query.split() if len(word) > 2]
                    filtered_docs = []
                    for doc in results:
                        info = doc.get('information', '')
                        segments = [sent for sent in info.split('.') if any(kw.lower() in sent.lower() for kw in keywords)]
                        filtered = '.'.join(segments) if segments else info
                        filtered_docs.append(filtered)
                    context = "\n\n".join(filtered_docs)
                    max_context_chars = 3000
                    if len(context) > max_context_chars:
                        context = context[:max_context_chars] + "\n\n...(N·ªôi dung ƒë√£ b·ªã c·∫Øt ng·∫Øn)..."
                    
                    # Build prompt
                    prompt = f"""
                    **B·ªëi c·∫£nh:**
                    B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n gia v·ªÅ C√¢u l·∫°c b·ªô L·∫≠p tr√¨nh ProPTIT. Nhi·ªám v·ª• c·ªßa b·∫°n l√† cung c·∫•p c√°c c√¢u tr·∫£ l·ªùi ch√≠nh x√°c v√† h·ªØu √≠ch d·ª±a *duy nh·∫•t* v√†o th√¥ng tin ƒë∆∞·ª£c cung c·∫•p.

                    **V√≠ d·ª• (Few-shot Examples):**
                    *   **V√≠ d·ª• 1: Tr·∫£ l·ªùi v·ªÅ quy tr√¨nh tuy·ªÉn th√†nh vi√™n**
                        *   **C√¢u h·ªèi:** "CLB tuy·ªÉn th√†nh vi√™n nh∆∞ th·∫ø n√†o ·∫°?"
                        *   **C√¢u tr·∫£ l·ªùi:** "Qu√° tr√¨nh tuy·ªÉn th√†nh vi√™n c·ªßa CLB g·ªìm 3 v√≤ng: ƒë·∫ßu ti√™n l√† v√≤ng CV, sau ƒë√≥ s·∫Ω ƒë·∫øn v√≤ng Ph·ªèng v·∫•n v√† cu·ªëi c√πng l√† v√≤ng Training c·ªßa CLB. Th√¥ng tin chi ti·∫øt c·ªßa c√°c v√≤ng s·∫Ω ƒë∆∞·ª£c CLB c·∫≠p nh·∫≠t tr√™n fanpage."

                    *   **V√≠ d·ª• 2: Th√¥ng tin kh√¥ng c√≥ trong ng·ªØ c·∫£nh**
                        *   **C√¢u h·ªèi:** "CLB c√≥ bao nhi√™u th√†nh vi√™n hi·ªán t·∫°i?"
                        *   **C√¢u tr·∫£ l·ªùi:** "Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin v·ªÅ s·ªë l∆∞·ª£ng th√†nh vi√™n hi·ªán t·∫°i c·ªßa CLB trong t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p."

                    **Th√¥ng tin tham kh·∫£o:**
                    ---
                    {context}
                    ---

                    **Y√™u c·∫ßu:**
                    D·ª±a v√†o **duy nh·∫•t** "Th√¥ng tin tham kh·∫£o" ·ªü tr√™n v√† h·ªçc theo phong c√°ch t·ª´ c√°c v√≠ d·ª•, h√£y tr·∫£ l·ªùi c√¢u h·ªèi sau ƒë√¢y c·ªßa ng∆∞·ªùi d√πng.

                    **C√¢u h·ªèi:** "{user_query}"

                    **Quy t·∫Øc tr·∫£ l·ªùi:**
                    1.  **Ch√≠nh x√°c v√† Trung th·ª±c:** Ch·ªâ s·ª≠ d·ª•ng th√¥ng tin ƒë√£ cho. N·∫øu th√¥ng tin kh√¥ng c√≥ trong t√†i li·ªáu, h√£y tr·∫£ l·ªùi nh∆∞ "V√≠ d·ª• 2".
                    2.  **Chi ti·∫øt v√† R√µ r√†ng:** Tr·∫£ l·ªùi ƒë·∫ßy ƒë·ªß, chi ti·∫øt. S·ª≠ d·ª•ng g·∫°ch ƒë·∫ßu d√≤ng ho·∫∑c ƒë·ªãnh d·∫°ng ph√π h·ª£p n·∫øu c√¢u tr·∫£ l·ªùi c√≥ nhi·ªÅu √Ω ho·∫∑c c·∫ßn li·ªát k√™.
                    3.  **T·ª± nhi√™n v√† Th√¢n thi·ªán:** S·ª≠ d·ª•ng ng√¥n ng·ªØ ti·∫øng Vi·ªát t·ª± nhi√™n, gi·ªçng vƒÉn th√¢n thi·ªán nh∆∞ ƒëang tr√≤ chuy·ªán.
                    4.  **Kh√¥ng suy di·ªÖn:** Tuy·ªát ƒë·ªëi kh√¥ng suy di·ªÖn, kh√¥ng b·ªãa ƒë·∫∑t ho·∫∑c th√™m th√¥ng tin kh√¥ng c√≥ trong vƒÉn b·∫£n.

                    **C√¢u tr·∫£ l·ªùi c·ªßa b·∫°n (b·∫±ng ti·∫øng Vi·ªát):**
                    """
                    # Prepare messages
                    messages = [
                        {"role": "system", "content": "B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n gia v·ªÅ CLB L·∫≠p tr√¨nh ProPTIT, lu√¥n tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát d·ª±a tr√™n th√¥ng tin ƒë∆∞·ª£c cung c·∫•p."},
                        {"role": "user", "content": prompt}
                    ]
                    # Get response
                    answer = get_llm_response(messages, temperature=temperature, max_tokens=max_tokens)
                    
                    # Add to chat history
                    st.session_state.chat_history.append({
                        "query": user_query,
                        "answer": answer,
                        "retrieval_time": retrieval_time,
                        "num_docs": len(results)
                    })
                    
                    # Display answer
                    st.subheader("ü§ñ C√¢u tr·∫£ l·ªùi:")
                    st.markdown(f'<div class="assistant-message">{answer}</div>', unsafe_allow_html=True)
                    
                    # Display metrics
                    col1, col2, col3, col4 = st.columns(4)
                    with col1:
                        st.metric("‚è±Ô∏è Th·ªùi gian retrieval", f"{retrieval_time:.2f}s")
                    with col2:
                        st.metric("üìä S·ªë documents", len(results))
                    with col3:
                        st.metric("üîç Top K", top_k)
                    with col4:
                        st.metric("ü§ñ Model", llm_model.split('/')[-1][:15] + "...")
                        
                except Exception as e:
                    st.error(f"L·ªói khi x·ª≠ l√Ω c√¢u h·ªèi: {str(e)}")
        
        # Display chat history
        if st.session_state.chat_history:
            st.subheader("üìú L·ªãch s·ª≠ chat")
            for i, chat in enumerate(reversed(st.session_state.chat_history[-5:])):  # Show last 5
                with st.expander(f"Q{len(st.session_state.chat_history)-i}: {chat['query'][:50]}..."):
                    st.markdown(f"**C√¢u h·ªèi:** {chat['query']}")
                    st.markdown(f"**Tr·∫£ l·ªùi:** {chat['answer']}")
                    st.markdown(f"**Th·ªùi gian:** {chat['retrieval_time']:.2f}s | **Docs:** {chat['num_docs']}")

with tab2:
    st.header("üìà Performance Analysis")
    
    # Metrics data
    metrics_data = {
        "Train": {
            "Retrieval": {
                'k': [3, 5, 7],
                'hit@k': [0.94, 0.97, 0.99],
                'recall@k': [0.79, 0.91, 0.95],
                'precision@k': [0.51, 0.36, 0.27],
                'f1@k': [0.62, 0.52, 0.43],
                'map@k': [0.74, 0.69, 0.66],
                'mrr@k': [0.75, 0.72, 0.71],
                'ndcg@k': [0.79, 0.77, 0.76],
                'context_precision@k': [0.83, 0.64, 0.65],
                'context_recall@k': [0.63, 0.53, 0.49],
                'context_entities_recall@k': [0.77, 0.83, 0.84]
            },
            "LLM": {
                'k': [3, 5, 7],
                'string_presence@k': [0.74, 0.76, 0.75],
                'rouge_l@k': [0.25, 0.25, 0.25],
                'bleu_4@k': [0.06, 0.05, 0.05],
                'groundedness@k': [0.94, 0.96, 0.96],
                'response_relevancy@k': [0.82, 0.82, 0.82],
                'noise_sensitivity@k': [0.17, 0.15, 0.14]
            }
        },
        "Test": {
            "Retrieval": {
                'k': [3, 5, 7],
                'hit@k': [0.97, 0.97, 0.97],
                'recall@k': [0.82, 0.89, 0.90],
                'precision@k': [0.53, 0.36, 0.27],
                'f1@k': [0.65, 0.51, 0.41],
                'map@k': [0.87, 0.82, 0.79],
                'mrr@k': [0.88, 0.86, 0.86],
                'ndcg@k': [0.90, 0.87, 0.86],
                'context_precision@k': [0.96, 0.77, 0.76],
                'context_recall@k': [0.88, 0.73, 0.72],
                'context_entities_recall@k': [0.94, 0.96, 0.96]
            },
            "LLM": {
                'k': [3, 5, 7],
                'string_presence@k': [0.85, 0.86, 0.86],
                'rouge_l@k': [0.55, 0.58, 0.59],
                'bleu_4@k': [0.35, 0.38, 0.40],
                'groundedness@k': [1.00, 1.00, 1.00],
                'response_relevancy@k': [0.81, 0.81, 0.82],
                'noise_sensitivity@k': [0.02, 0.02, 0.00]
            }
        }
    }
    
    # Display metrics for each dataset
    for dataset in ["Train", "Test"]:
        st.subheader(f"üìä {dataset} Dataset")
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("**üîç Retrieval Metrics**")
            df_ret = pd.DataFrame(metrics_data[dataset]["Retrieval"])
            st.dataframe(df_ret, use_container_width=True)
        
        with col2:
            st.markdown("**ü§ñ LLM Metrics**")
            df_llm = pd.DataFrame(metrics_data[dataset]["LLM"])
            st.dataframe(df_llm, use_container_width=True)
    
    # Individual metric testing
    if st.session_state.initialized:
        with st.expander("üî¨ Test Individual Metrics", expanded=False):
            col1, col2, col3 = st.columns(3)
            with col1:
                test_k = st.slider("K value", 1, 10, 5)
            with col2:
                metric_type = st.selectbox("Metric", [
                    "hit_k", "recall_k", "precision_k", "context_precision_k", 
                    "context_recall_k", "context_entities_recall_k", "ndcg_k"
                ])
            with col3:
                test_expansion = st.checkbox("Use Query Expansion", value=True)
            
            if st.button("üß™ Test Metric"):
                with st.spinner(f"Testing {metric_type}@{test_k}..."):
                    try:
                        if metric_type == "hit_k":
                            result = hit_k("CLB_PROPTIT.csv", "train_data_proptit.xlsx", 
                                         st.session_state.embedding_model, st.session_state.vector_db, 
                                         k=test_k, reranker=st.session_state.reranker, 
                                         use_query_expansion=test_expansion)
                        elif metric_type == "recall_k":
                            result = recall_k("CLB_PROPTIT.csv", "train_data_proptit.xlsx", 
                                            st.session_state.embedding_model, st.session_state.vector_db, 
                                            k=test_k, reranker=st.session_state.reranker, 
                                            use_query_expansion=test_expansion)
                        elif metric_type == "precision_k":
                            result = precision_k("CLB_PROPTIT.csv", "train_data_proptit.xlsx", 
                                               st.session_state.embedding_model, st.session_state.vector_db, 
                                               k=test_k, reranker=st.session_state.reranker, 
                                               use_query_expansion=test_expansion)
                        elif metric_type == "ndcg_k":
                            result = ndcg_k("CLB_PROPTIT.csv", "train_data_proptit.xlsx", 
                                          st.session_state.embedding_model, st.session_state.vector_db, 
                                          k=test_k, reranker=st.session_state.reranker, 
                                          use_query_expansion=test_expansion)
                        
                        st.success(f"**{metric_type}@{test_k}:** {result:.4f}")
                        
                        # Compare with baseline if available
                        baseline_data = metrics_data["Train"]["Retrieval"] if metric_type in ["hit_k", "recall_k", "precision_k", "ndcg_k"] else {}
                        if test_k in [3, 5, 7] and f"{metric_type}@{test_k}" in [f"{k}@{v}" for k, v in zip(baseline_data.keys(), baseline_data.values()) if k != 'k']:
                            baseline_val = baseline_data[metric_type.replace('_', '@')][baseline_data['k'].index(test_k)]
                            improvement = ((result - baseline_val) / baseline_val) * 100
                            
                            if improvement > 0:
                                st.success(f"üéâ C·∫£i thi·ªán {improvement:.2f}% so v·ªõi baseline!")
                            else:
                                st.warning(f"üìâ Th·∫•p h∆°n baseline {abs(improvement):.2f}%")
                                
                    except Exception as e:
                        st.error(f"L·ªói khi test metric: {str(e)}")

# Footer
st.markdown("---")
st.markdown("""
<div style='text-align: center; color: #666666;'>
    <p>üöÄ <b>NeoRAG Cup 2025</b> - Powered by H·∫£i Long</p>
    <p>Built with ‚ù§Ô∏è using Streamlit</p>
</div>
""", unsafe_allow_html=True)
