# üöÄ NeoRAG Cup 2025 - Implementation Guide

## üìã T·ªïng quan nhi·ªám v·ª•

**M·ª•c ti√™u**: X√¢y d·ª±ng h·ªá th·ªëng RAG (Retrieval-Augmented Generation) v·ªõi domain CLB ProPTIT ƒë·ªÉ ƒë·∫°t hi·ªáu nƒÉng cao h∆°n baseline model.

**Th·ªùi gian**: 4 tu·∫ßn (ƒë·∫øn ng√†y pitching)

**Ti√™u ch√≠ ch·∫•m ƒëi·ªÉm**:
- Ki·∫øn tr√∫c pipeline: 30%
- Hi·ªáu nƒÉng benchmark: 40% 
- Ch·∫•t l∆∞·ª£ng demo: 20%
- K·ªπ nƒÉng thuy·∫øt tr√¨nh: 10%

---

## üéØ Baseline Performance (c·∫ßn v∆∞·ª£t qua)

### Retrieval Metrics - Train:
| K | hit@k | recall@k | precision@k | f1@k | map@k | mrr@k | ndcg@k | context_precision@k | context_recall@k |
|---|-------|----------|-------------|------|-------|-------|--------|---------------------|------------------|
| 3 | 0.31  | 0.19     | 0.12        | 0.15 | 0.23  | 0.23  | 0.25   | 0.63                | 0.50             |
| 5 | 0.46  | 0.28     | 0.10        | 0.15 | 0.23  | 0.27  | 0.31   | 0.56                | 0.44             |
| 7 | 0.57  | 0.35     | 0.09        | 0.15 | 0.23  | 0.28  | 0.35   | 0.54                | 0.40             |

### LLM Answer Metrics - Train:
| K | string_presence@k | rouge_l@k | bleu_4@k | groundedness@k | response_relevancy@k |
|---|-------------------|-----------|----------|----------------|----------------------|
| 3 | 0.35              | 0.21      | 0.03     | 0.57           | 0.80                 |
| 5 | 0.40              | 0.23      | 0.03     | 0.61           | 0.80                 |
| 7 | 0.41              | 0.22      | 0.04     | 0.64           | 0.80                 |

---

## üîß B∆Ø·ªöC 1: THI·∫æT L·∫¨P M√îI TR∆Ø·ªúNG

### 1.1 T·∫°o file .env
T·∫°o file `.env` trong th∆∞ m·ª•c g·ªëc v·ªõi n·ªôi dung:

```env
OPENAI_API_KEY=your_openai_api_key_here

# Vector Databases (ch·ªçn m·ªôt ho·∫∑c nhi·ªÅu)
MONGODB_URI=mongodb://localhost:27017/
QDRANT_URL=https://your-cluster.qdrant.tech
QDRANT_KEY=your_qdrant_key
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_KEY=your_supabase_anon_key

GEMINI_API_KEY=your_gemini_api_key
```

### 1.2 Install dependencies
```bash
pip install -r requirements.txt
```

### 1.3 Ki·ªÉm tra c√°c packages c·∫ßn thi·∫øt
```bash
pip install python-dotenv openai pymongo chromadb qdrant-client supabase sentence-transformers
```

---

## üõ†Ô∏è B∆Ø·ªöC 2: S·ª¨A CODE TRONG C√ÅC FILE

### 2.1 File `main.py` - C·∫•u h√¨nh c∆° b·∫£n

**D√≤ng 11**: Ch·ªçn vector database
```python
vector_db = VectorDatabase(db_type="mongodb")  # ho·∫∑c "chromadb", "qdrant", "supabase"
```

**D√≤ng 15**: Ch·ªçn embedding model
```python
embedding = Embeddings(model_name="text-embedding-3-large", type="openai")
```

**Alternatives ƒë·ªÉ th·ª≠ nghi·ªám**:
```python
# OpenAI embeddings (t·ªët nh∆∞ng t·ªën ph√≠)
embedding = Embeddings(model_name="text-embedding-3-large", type="openai")
embedding = Embeddings(model_name="text-embedding-ada-002", type="openai")

# Sentence Transformers (mi·ªÖn ph√≠, ch·∫°y local)
embedding = Embeddings(model_name="all-mpnet-base-v2", type="sentence_transformers")
embedding = Embeddings(model_name="all-MiniLM-L6-v2", type="sentence_transformers")

# Google Gemini
embedding = Embeddings(model_name="text-embedding-004", type="gemini")
```

### 2.2 File `metrics_rag.py` - C√°c FIX_ME c·∫ßn s·ª≠a

#### A. Trong c√°c h√†m retrieval metrics:

**Pattern chung cho user_embedding**:
```python
user_embedding = embedding.encode(query)
```

**Pattern chung cho results**:
```python
results = vector_db.query("information", user_embedding, limit=k)
```

**Pattern chung cho retrieved_docs**:
```python
retrieved_docs = []
for i, result in enumerate(results):
    # Gi·∫£ s·ª≠ title c√≥ format "Document X" where X is document ID
    doc_title = result.get('title', '')
    if doc_title.startswith('Document '):
        doc_id = int(doc_title.split(' ')[1])
        retrieved_docs.append(doc_id)
```

#### B. Trong c√°c h√†m LLM answer metrics:

**System prompt template**:
```python
system_content = """B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n cung c·∫•p th√¥ng tin v·ªÅ C√¢u l·∫°c b·ªô L·∫≠p tr√¨nh ProPTIT.
B·∫°n s·∫Ω nh·∫≠n ƒë∆∞·ª£c d·ªØ li·ªáu ng·ªØ c·∫£nh (context) t·ª´ m·ªôt h·ªá th·ªëng RAG ch·ª©a c√°c th√¥ng tin ch√≠nh x√°c v·ªÅ CLB.

Nguy√™n t·∫Øc tr·∫£ l·ªùi:
1. Ch·ªâ s·ª≠ d·ª•ng th√¥ng tin t·ª´ context ƒë∆∞·ª£c cung c·∫•p ƒë·ªÉ tr·∫£ l·ªùi
2. Tr√¨nh b√†y c√¢u tr·∫£ l·ªùi r√µ r√†ng, d·ªÖ hi·ªÉu
3. Tuy·ªát ƒë·ªëi kh√¥ng suy ƒëo√°n ho·∫∑c b·ªãa th√¥ng tin
4. Gi·ªØ phong c√°ch tr·∫£ l·ªùi th√¢n thi·ªán, chuy√™n nghi·ªáp

Nhi·ªám v·ª•: Tr·∫£ l·ªùi c√°c c√¢u h·ªèi v·ªÅ CLB L·∫≠p tr√¨nh ProPTIT d·ª±a tr√™n context ƒë∆∞·ª£c cung c·∫•p."""
```

**Context building**:
```python
context = "Content t·ª´ c√°c t√†i li·ªáu li√™n quan:\n"
context += "\n".join([result["information"] for result in results])
```

**User message**:
```python
user_content = context + "\n\nC√¢u h·ªèi: " + query
```

**OpenAI API call**:
```python
from openai import OpenAI
client = OpenAI()

response = client.chat.completions.create(
    model="gpt-3.5-turbo",  # ho·∫∑c "gpt-4"
    messages=messages,
    temperature=0.3,
    max_tokens=500
)
reply = response.choices[0].message.content
```

#### C. Specific fixes cho m·ªôt s·ªë h√†m:

**ndcg_k function - similarity score**:
```python
# L·∫•y embedding c·ªßa document t·ª´ CLB_PROPTIT.csv
doc_info = df_clb.iloc[doc-1]['VƒÉn b·∫£n']  # doc-1 v√¨ index b·∫Øt ƒë·∫ßu t·ª´ 0
doc_embedding = embedding.encode(doc_info)
similarity_score = similarity(user_embedding, doc_embedding)
```

---

## üöÄ B∆Ø·ªöC 3: CHI·∫æN L∆Ø·ª¢C T·ªêI ∆ØU H√ìA

### 3.1 C·∫£i thi·ªán Retrieval (40% ƒëi·ªÉm s·ªë)

#### A. Embedding Model Optimization
```python
# Test multiple models v√† so s√°nh hi·ªáu nƒÉng
models_to_test = [
    ("text-embedding-3-large", "openai"),
    ("text-embedding-ada-002", "openai"),
    ("all-mpnet-base-v2", "sentence_transformers"),
    ("all-MiniLM-L6-v2", "sentence_transformers"),
]
```

#### B. Document Chunking Strategy
```python
# Thay v√¨ l∆∞u t·ª´ng paragraph, chia documents th√†nh chunks t·ªëi ∆∞u
def chunk_document(text, chunk_size=300, overlap=50):
    words = text.split()
    chunks = []
    for i in range(0, len(words), chunk_size - overlap):
        chunk = ' '.join(words[i:i + chunk_size])
        chunks.append(chunk)
    return chunks
```

#### C. Query Enhancement
```python
def enhance_query(original_query, embedding_model):
    """Generate multiple variations of the query"""
    
    # Query expansion
    expanded_query = f"{original_query} CLB ProPTIT l·∫≠p tr√¨nh"
    
    # Query rewriting using LLM
    rewrite_prompt = f"""
    H√£y vi·∫øt l·∫°i c√¢u h·ªèi sau ƒë·ªÉ t√¨m ki·∫øm th√¥ng tin v·ªÅ CLB ProPTIT hi·ªáu qu·∫£ h∆°n:
    C√¢u h·ªèi g·ªëc: {original_query}
    C√¢u h·ªèi m·ªõi:
    """
    
    # Return multiple query variations
    return [original_query, expanded_query, rewritten_query]
```

#### D. Hybrid Search Implementation
```python
def hybrid_search(query, embedding, vector_db, k=5):
    """Combine dense and sparse retrieval"""
    
    # Dense retrieval (semantic)
    query_embedding = embedding.encode(query)
    dense_results = vector_db.query("information", query_embedding, limit=k*2)
    
    # Sparse retrieval (keyword matching)
    sparse_results = keyword_search(query, k*2)
    
    # Combine and rerank results
    combined_results = combine_and_rerank(dense_results, sparse_results, k)
    
    return combined_results
```

### 3.2 C·∫£i thi·ªán Generation

#### A. Advanced Prompt Engineering
```python
def create_advanced_system_prompt():
    return """B·∫°n l√† chuy√™n gia v·ªÅ C√¢u l·∫°c b·ªô L·∫≠p tr√¨nh ProPTIT v·ªõi ki·∫øn th·ª©c s√¢u r·ªông.

NHI·ªÜM V·ª§:
- Tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a CH√çNH X√ÅC tr√™n context ƒë∆∞·ª£c cung c·∫•p
- S·ª≠ d·ª•ng th√¥ng tin t·ª´ nhi·ªÅu ngu·ªìn trong context n·∫øu c√≥
- Tr√¨nh b√†y theo c·∫•u tr√∫c logic, r√µ r√†ng

QUY T·∫ÆC:
1. N·∫øu context c√≥ th√¥ng tin ƒë·∫ßy ƒë·ªß ‚Üí Tr·∫£ l·ªùi chi ti·∫øt
2. N·∫øu context c√≥ th√¥ng tin m·ªôt ph·∫ßn ‚Üí Tr·∫£ l·ªùi ph·∫ßn c√≥ v√† n√≥i r√µ gi·ªõi h·∫°n
3. N·∫øu context kh√¥ng c√≥ th√¥ng tin ‚Üí "D·ª±a tr√™n th√¥ng tin hi·ªán c√≥, t√¥i kh√¥ng th·ªÉ..."

ƒê·ªäNH D·∫†NG TR·∫¢ L·ªúI:
- S·ª≠ d·ª•ng bullet points cho danh s√°ch
- Highlight th√¥ng tin quan tr·ªçng
- Cung c·∫•p context khi c·∫ßn thi·∫øt"""
```

#### B. Few-shot Learning
```python
def add_few_shot_examples():
    examples = """
EXAMPLE 1:
Context: "CLB ƒë∆∞·ª£c th√†nh l·∫≠p ng√†y 9/10/2011..."
Question: CLB ƒë∆∞·ª£c th√†nh l·∫≠p khi n√†o?
Answer: C√¢u l·∫°c b·ªô L·∫≠p tr√¨nh ProPTIT ƒë∆∞·ª£c th√†nh l·∫≠p ng√†y 9/10/2011.

EXAMPLE 2:
Context: "Team AI, Team Mobile, Team Data..."
Question: CLB c√≥ nh·ªØng team n√†o?
Answer: CLB ProPTIT c√≥ c√°c team d·ª± √°n sau: Team AI, Team Mobile, Team Data, Team Game, Team Web, Team Backend.
"""
    return examples
```

### 3.3 Reranking Implementation
```python
def rerank_results(query, initial_results, top_k=5):
    """Rerank retrieved results using cross-encoder"""
    
    scores = []
    for result in initial_results:
        # Calculate relevance score
        score = calculate_relevance(query, result['information'])
        scores.append((result, score))
    
    # Sort by score and return top k
    sorted_results = sorted(scores, key=lambda x: x[1], reverse=True)
    return [result[0] for result in sorted_results[:top_k]]
```

---

## üìä B∆Ø·ªöC 4: MONITORING V√Ä EVALUATION

### 4.1 T·∫°o script ƒë√°nh gi√° t·ª± ƒë·ªông
```python
# create file: evaluate_model.py
def evaluate_all_configurations():
    """Test different configurations and log results"""
    
    configurations = [
        {"db": "mongodb", "embedding": ("text-embedding-3-large", "openai")},
        {"db": "chromadb", "embedding": ("all-mpnet-base-v2", "sentence_transformers")},
        # Add more configurations
    ]
    
    results = {}
    for config in configurations:
        print(f"Testing config: {config}")
        metrics = run_evaluation(config)
        results[str(config)] = metrics
        
    # Save results for comparison
    save_results(results)
```

### 4.2 Tracking improvements
```python
# T·∫°o file log ƒë·ªÉ theo d√µi c·∫£i thi·ªán
def log_improvement(baseline_metrics, current_metrics):
    improvements = {}
    for metric in baseline_metrics:
        if metric in current_metrics:
            improvement = current_metrics[metric] - baseline_metrics[metric]
            improvements[metric] = {
                'baseline': baseline_metrics[metric],
                'current': current_metrics[metric],
                'improvement': improvement,
                'percentage': (improvement / baseline_metrics[metric]) * 100
            }
    return improvements
```

---

## üìà B∆Ø·ªöC 5: ADVANCED TECHNIQUES

### 5.1 Multi-stage Retrieval
```python
def multi_stage_retrieval(query, embedding, vector_db):
    # Stage 1: Broad retrieval
    initial_results = vector_db.query("information", embedding.encode(query), limit=20)
    
    # Stage 2: Query expansion based on initial results
    expanded_queries = generate_related_queries(query, initial_results[:3])
    
    # Stage 3: Retrieve with expanded queries
    final_results = []
    for expanded_query in expanded_queries:
        results = vector_db.query("information", embedding.encode(expanded_query), limit=5)
        final_results.extend(results)
    
    # Stage 4: Deduplicate and rerank
    return deduplicate_and_rerank(final_results, query, top_k=5)
```

### 5.2 Context Optimization
```python
def optimize_context(retrieved_docs, query, max_tokens=2000):
    """Select most relevant parts of retrieved documents"""
    
    # Score each sentence in each document
    scored_sentences = []
    for doc in retrieved_docs:
        sentences = doc['information'].split('. ')
        for sentence in sentences:
            score = calculate_sentence_relevance(sentence, query)
            scored_sentences.append((sentence, score, doc['title']))
    
    # Select top sentences within token limit
    sorted_sentences = sorted(scored_sentences, key=lambda x: x[1], reverse=True)
    
    context = ""
    token_count = 0
    for sentence, score, doc_title in sorted_sentences:
        sentence_tokens = len(sentence.split())
        if token_count + sentence_tokens <= max_tokens:
            context += f"{sentence}. "
            token_count += sentence_tokens
        else:
            break
    
    return context
```

---

## üéØ B∆Ø·ªöC 6: PLAN TH·ª∞C HI·ªÜN 4 TU·∫¶N

### Tu·∫ßn 1 (Hi·ªán t·∫°i): Foundation Setup
- [ ] Setup m√¥i tr∆∞·ªùng, t·∫°o .env file
- [ ] S·ª≠a t·∫•t c·∫£ FIX_ME ƒë·ªÉ code ch·∫°y ƒë∆∞·ª£c
- [ ] Ch·∫°y baseline model v√† x√°c nh·∫≠n k·∫øt qu·∫£
- [ ] Test c√°c embedding models kh√°c nhau

### Tu·∫ßn 2: Core Optimization
- [ ] Implement chunking strategies
- [ ] Test different vector databases
- [ ] Implement query enhancement
- [ ] Optimize system prompts

### Tu·∫ßn 3: Advanced Techniques
- [ ] Implement hybrid search
- [ ] Add reranking mechanism
- [ ] Implement multi-stage retrieval
- [ ] Fine-tune hyperparameters

### Tu·∫ßn 4: Demo & Presentation
- [ ] Create Streamlit demo app
- [ ] Prepare presentation slides
- [ ] Practice demo v√† Q&A
- [ ] Final testing v√† bug fixes

---

## üö® TIPS QUAN TR·ªåNG

### ƒê·ªÉ ƒë·∫°t hi·ªáu su·∫•t cao:

1. **Focus on metrics quan tr·ªçng nh·∫•t**:
   - `hit@k` v√† `recall@k` cho retrieval
   - `groundedness@k` ƒë·ªÉ gi·∫£m hallucination
   - `context_precision@k` ƒë·ªÉ tƒÉng ch·∫•t l∆∞·ª£ng context

2. **Th·ª≠ nghi·ªám systematic**:
   - Test t·ª´ng component ri√™ng bi·ªát
   - A/B test different approaches
   - Keep track of what works

3. **Optimization order**:
   - Embedding model (biggest impact)
   - Chunking strategy
   - System prompt
   - Reranking
   - Advanced techniques

4. **Common pitfalls to avoid**:
   - Over-engineering without testing
   - Ignoring computational cost
   - Not validating on test data
   - Poor error handling

---

## üìù CHECKLIST TR∆Ø·ªöC KHI N·ªòP

- [ ] Code ch·∫°y kh√¥ng l·ªói tr√™n to√†n b·ªô dataset
- [ ] Metrics cao h∆°n baseline tr√™n √≠t nh·∫•t 80% c√°c ch·ªâ s·ªë
- [ ] Demo app ho·∫°t ƒë·ªông m∆∞·ª£t m√†
- [ ] Slides thuy·∫øt tr√¨nh ho√†n ch·ªânh (ki·∫øn tr√∫c + k·∫øt qu·∫£ + demo)
- [ ] ƒê√£ practice thuy·∫øt tr√¨nh trong 30 ph√∫t
- [ ] Code ƒë∆∞·ª£c comment v√† organize t·ªët

---

**Ch√∫c b·∫°n th√†nh c√¥ng v·ªõi NeoRAG Cup 2025! üèÜ**

*L∆∞u √Ω: File n√†y s·∫Ω ƒë∆∞·ª£c c·∫≠p nh·∫≠t khi c√≥ th√™m insights t·ª´ qu√° tr√¨nh th·ª≠ nghi·ªám.*
