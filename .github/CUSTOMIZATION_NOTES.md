# ğŸ”§ CUSTOMIZATION GUIDE - NeoRAG Cup 2025

## ğŸ“‹ Tá»•ng quan cÃ¡c Ä‘iá»ƒm cÃ³ thá»ƒ tÃ¹y chá»‰nh

### ğŸ¤– **1. LLM Models & APIs**

#### 1.1 Thay Ä‘á»•i LLM Provider (file: `metrics_rag.py`)

### ğŸ“Š **2. Embedding Models**

#### 2.1 Thay Ä‘á»•i Embedding Model (file: `main.py`, line ~18)
```python
# ğŸ”§ HIá»†N Táº I: Ollama
embedding = Embeddings(model_name="nomic-embed-text", type="ollama")
```

---

### ğŸ—ƒï¸ **3. Vector Databases**

#### 3.1 Thay Ä‘á»•i Vector DB (file: `main.py`, line ~14)
```python
# ğŸ”§ HIá»†N Táº I: MongoDB
vector_db = VectorDatabase(db_type="mongodb")
```

---

### ğŸ“ **4. System Prompts & Instructions**

#### 4.1 Prompt (Xuáº¥t hiá»‡n nhiá»u láº§n trong `metrics_rag.py`)
ğŸ”§ **Vá»‹ trÃ­**: TÃ¬m `"Sá»­a prompt náº¿u muá»‘n"`

---

### âš™ï¸ **5. Retrieval & Ranking**

#### 5.1 Retrieval Parameters (Táº¥t cáº£ metrics functions)
```python
# ğŸ”§ CÃ“ THá»‚ THAY Äá»”I:
results = vector_db.query("information", user_embedding, limit=k)

# Options:
- limit=k: Sá»‘ documents retrieve
- similarity thresholds
- Reranking algorithms
- Query expansion techniques
```

#### 5.2 Document Processing
ğŸ”§ **Vá»‹ trÃ­**: `main.py`, document chunking logic

**CÃ³ thá»ƒ optimize:**
- Chunking strategy (sentence vs paragraph vs semantic)
- Overlap between chunks
- Metadata enrichment

---

### ğŸ¯ **7. Performance Optimization**

#### 7.1 Caching Strategy
ğŸ”§ **CÃ³ thá»ƒ thÃªm:**
- Cache embeddings
- Cache LLM responses  
- Batch processing
- Async operations

#### 7.2 Resource Management
ğŸ”§ **CÃ³ thá»ƒ optimize:**
- API rate limiting
- Memory usage for large datasets
- Parallel processing

---

### ğŸ”„ **8. Advanced Techniques**

#### 8.1 Query Enhancement
ğŸ”§ **CÃ³ thá»ƒ implement:**
```python
# Trong cÃ¡c retrieval functions, trÆ°á»›c khi embed query:
# query = query_expansion(query)
# query = query_rewrite(query, context)
# user_embedding = ensemble_embedding([emb1, emb2, emb3])
```

#### 8.2 Post-Processing
ğŸ”§ **CÃ³ thá»ƒ thÃªm:**
- Response filtering
- Fact verification
- Output formatting
- Confidence scoring

---

## ğŸ† **Recommendations Ä‘á»ƒ vÆ°á»£t Baseline**

### **Immediate wins:**
1. **Embedding Model**: Thá»­ `text-embedding-3-large` hoáº·c domain-specific models
2. **LLM Model**: Upgrade lÃªn `gemini-1.5-pro` hoáº·c `gpt-4`
3. **Retrieval**: TÄƒng k vÃ  implement reranking
4. **Prompts**: Fine-tune system prompts cho domain ProPTIT

### **Advanced optimizations:**
1. **Hybrid Search**: Combine vector + keyword search
2. **Query Expansion**: Sá»­ dá»¥ng LLM Ä‘á»ƒ má»Ÿ rá»™ng query
3. **Ensemble Methods**: Káº¿t há»£p multiple models
4. **Fine-tuning**: Train custom embedding/reranking models

### **Evaluation improvements:**
1. **Custom Metrics**: ThÃªm domain-specific evaluation
2. **Human Evaluation**: Supplement vá»›i manual scoring
3. **A/B Testing**: Compare different configurations

---

## ğŸ“ **Quick Start Customization**

1. **Copy `.env.example` to `.env`** vÃ  Ä‘iá»n API keys
2. **Thay Ä‘á»•i MODEL_NAME** trong `metrics_rag.py` (line ~16)
3. **Thay Ä‘á»•i embedding model** trong `main.py` (line ~18)
4. **Cháº¡y baseline** Ä‘á»ƒ cÃ³ reference performance
5. **Iterate tá»«ng component** má»™t cÃ¡ch systematic

---

## ğŸ› **Troubleshooting**

- **API Errors**: Kiá»ƒm tra `.env` file vÃ  API quotas
- **Memory Issues**: Giáº£m batch size hoáº·c k values
- **Slow Performance**: Consider caching vÃ  parallel processing
- **Low Scores**: Debug tá»«ng metric riÃªng biá»‡t trÆ°á»›c khi optimize tá»•ng thá»ƒ
