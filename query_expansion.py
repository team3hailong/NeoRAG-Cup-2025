import re
import time
from typing import List, Dict, Any
from dotenv import load_dotenv
import os

# Import centralized LLM configuration  
from llm_config import get_llm_response as get_llm_response_global, get_config_info

load_dotenv()

class QueryExpansion:
    """
    L·ªõp QueryExpansion th·ª±c hi·ªán c√°c k·ªπ thu·∫≠t m·ªü r·ªông c√¢u truy v·∫•n ƒë·ªÉ c·∫£i thi·ªán hi·ªáu su·∫•t retrieval trong RAG.
    
    C√°c k·ªπ thu·∫≠t ƒë∆∞·ª£c √°p d·ª•ng:
    1. Query Rewriting - Vi·∫øt l·∫°i c√¢u h·ªèi v·ªõi nhi·ªÅu c√°ch di·ªÖn ƒë·∫°t kh√°c nhau
    2. Query Decomposition - Ph√¢n t√°ch c√¢u h·ªèi ph·ª©c t·∫°p th√†nh c√°c c√¢u h·ªèi con ƒë∆°n gi·∫£n
    3. Synonym/Paraphrase Expansion - M·ªü r·ªông v·ªõi t·ª´ ƒë·ªìng nghƒ©a v√† c√°ch di·ªÖn ƒë·∫°t kh√°c
    4. Context-Aware Expansion - M·ªü r·ªông d·ª±a tr√™n ng·ªØ c·∫£nh CLB ProPTIT
    5. Multi-Perspective Query - T·∫°o c√°c g√≥c nh√¨n kh√°c nhau cho c√πng m·ªôt c√¢u h·ªèi
    """
    
    def __init__(self):
        # Use centralized LLM configuration
        config_info = get_config_info()
        print(f"ü§ñ QueryExpansion using {config_info['provider'].upper()} - {config_info['model']}")
        self.llm_available = config_info['client_initialized']
        
        # Domain-specific keywords cho CLB ProPTIT - ƒë∆∞·ª£c tr√≠ch xu·∫•t t·ª´ d·ªØ li·ªáu th·ª±c t·∫ø
        self.proptit_keywords = {
            "clb": [
                'clb', 'club', 'c√¢u l·∫°c b·ªô'
            ],
            "proptit": [
                'l·∫≠p tr√¨nh ptit', 'proptit', 'programming ptit', 'pro'
            ],
            "thanh_vien": [
                'th√†nh vi√™n', 'member', 'sinh vi√™n', 'h·ªçc vi√™n', 'ng∆∞·ªùi tham gia', 
                'b·∫°n', 'em', 'anh', 'ch·ªã', 'c√°c em', 'm·ªçi ng∆∞·ªùi', 'h·ªçc sinh', 'mem'
            ],
            "hoat_dong": [
                'ho·∫°t ƒë·ªông', 's·ª± ki·ªán', 'event', 'workshop', 'seminar',
                'training', 'sinh ho·∫°t', 'bu·ªïi h·ªçc', 'ƒë√†o t·∫°o', 't·ªï ch·ª©c',
                'biggame', 'cu·ªôc thi', 'contest', 'competition', 'tournament'
            ],
            "lap_trinh": [
                'l·∫≠p tr√¨nh', 'programming', 'code', 'coding', 'thu·∫≠t to√°n', 'algorithm', 'CTDL',
                'ph√°t tri·ªÉn ph·∫ßn m·ªÅm', 'software development', 'c·∫•u tr√∫c d·ªØ li·ªáu', 'gi·∫£i thu·∫≠t',
                'DSA', 'CTDL&GT', 'oop', 'h∆∞·ªõng ƒë·ªëi t∆∞·ª£ng'
            ],
            "ptit": [
                'ptit', 'h·ªçc vi·ªán', 'academy', 'tr∆∞·ªùng', 'university',
                'h·ªçc vi·ªán b∆∞u ch√≠nh vi·ªÖn th√¥ng', 'h·ªçc vi·ªán c√¥ng ngh·ªá bcvt',
                'bcvt', 'hv'
            ],
            "team_du_an": [
                'team', 'nh√≥m', 'd·ª± √°n', 'project', 's·∫£n ph·∫©m'
            ],
            "ky_nang": [
                'k·ªπ nƒÉng', 'skill', 'nƒÉng l·ª±c', 'kh·∫£ nƒÉng',
                'l√†m vi·ªác nh√≥m', 'teamwork', 'thuy·∫øt tr√¨nh',
                'qu·∫£n l√Ω', 'management', 'l√£nh ƒë·∫°o', 'giao ti·∫øp'
            ],
            "hoc_tap": [
                'h·ªçc t·∫≠p', 'learning', 'ƒë√†o t·∫°o', 'training', 'gi√°o d·ª•c', 'education',
                'kh√≥a h·ªçc', 'course', 'l·ªô tr√¨nh', 'l·ªô tr√¨nh h·ªçc', 'roadmap', 'roadmap h·ªçc t·∫≠p',
                'khung ch∆∞∆°ng tr√¨nh l·∫≠p tr√¨nh', 'k·ªπ nƒÉng l·∫≠p tr√¨nh c∆° b·∫£n', 'm√¥n h·ªçc', 'subject'
            ],
            "cuoc_thi_giai_thuong": [
                'cu·ªôc thi', 'contest', 'competition', 'tournament', 'gi·∫£i th∆∞·ªüng', 'award',
                'gi·∫£i nh·∫•t', 'gi·∫£i nh√¨', 'gi·∫£i ba', 'gi·∫£i khuy·∫øn kh√≠ch', 'prize',
                'huy ch∆∞∆°ng', 'medal', 'v√†ng', 'b·∫°c', 'ƒë·ªìng', 'v√¥ ƒë·ªãch', 'champion'
            ],
            "su_kien_dac_biet": [
                'biggame', 'spoj tournament', 'procwar', 'code battle', 'game jam',
                'marathon', 'sinh nh·∫≠t', 'camping', 'picnic', 'd√£ ngo·∫°i',
                'pama cup', 'progapp', 'icpc', 'digital race', 'Olympic'
            ],
            "cong_nghe": [
                'c++', 'java', 'python', 'javascript', 'html', 'css', 'sql',
                'h·ªçc m√°y', 'h·ªçc s√¢u', 'ML', 'DL', 'tr√≠ tu·ªá nh√¢n t·∫°o',
                'web', 'mobile', 'game', 'backend', 'frontend', 'fullstack',
                'react', 'angular', 'vue', 'nodejs', 'django', 'spring'
            ],
            "van_hoa_clb": [
                'ph∆∞∆°ng ch√¢m', 'chia s·∫ª ƒë·ªÉ c√πng nhau ph√°t tri·ªÉn', 'truy·ªÅn th·ªëng',
                'vƒÉn h√≥a', 'g·∫Øn k·∫øt', 'ƒëo√†n k·∫øt', 't∆∞∆°ng th√¢n t∆∞∆°ng √°i',
                'n·ªôi quy', 'quy ƒë·ªãnh', 'nghƒ©a v·ª•', 'quy·ªÅn l·ª£i', 'l·∫≠p tr√¨nh t·ª´ tr√°i tim',
                't√°c phong', 'vƒÉn h√≥a ·ª©ng x·ª≠', 'quy t·∫Øc ·ª©ng x·ª≠', 'phong c√°ch giao ti·∫øp',
                'n√©t vƒÉn h√≥a CLB', 'etiquette', 'manners', '·ª©ng x·ª≠ c√≥ vƒÉn h√≥a',
                'trang ph·ª•c', 'dress code', 'ƒë·ªìng ph·ª•c', 'quy ƒë·ªãnh trang ph·ª•c',
                's·ª≠ d·ª•ng tr·ª• s·ªü', 'c∆° s·ªü v·∫≠t ch·∫•t', 'ph√≤ng ban', 'ph√≤ng h·ªçp',
                'th√†nh vi√™n ti√™u bi·ªÉu', 'ti√™u ch√≠', 'ƒëi·ªÅu ki·ªán x√©t ch·ªçn', 
                'ti√™u chu·∫©n ƒë√°nh gi√°', 'c√°ch th·ª©c b·∫ßu ch·ªçn', 'reward', 'recognition',
                'khen th∆∞·ªüng', 'tuy√™n d∆∞∆°ng', 'ƒëi·ªÉm r√®n luy·ªán', 'vi ph·∫°m'
            ]
        }
    
    def _get_llm_response(self, messages: List[Dict], max_retries: int = 3) -> str:
        """Helper function ƒë·ªÉ g·ªçi LLM v·ªõi retry logic s·ª≠ d·ª•ng c·∫•u h√¨nh chung"""
        if not self.llm_available:
            print("‚ö†Ô∏è  LLM not available, returning empty response")
            return ""
            
        return get_llm_response_global(
            messages=messages,
            temperature=0.4,
            max_tokens=256,
            max_retries=max_retries
        )
    
    def combined_llm_expansion(self, query: str) -> List[str]:
        system_prompt = """T·∫°o c√°c bi·∫øn th·ªÉ c√¢u h·ªèi cho CLB ProPTIT. Tr·∫£ v·ªÅ format JSON:
{"rewrites": [...], "decomposed": [...], "context_aware": [...]}

Y√™u c·∫ßu:
- rewrites: 2 c√°ch vi·∫øt l·∫°i kh√°c nhau
- decomposed: Chia th√†nh c√¢u h·ªèi con (n·∫øu ph·ª©c t·∫°p)  
- context_aware: 2 c√¢u h·ªèi v·ªõi ng·ªØ c·∫£nh CLB ProPTIT c·ª• th·ªÉ

Ng·ªØ c·∫£nh CLB ProPTIT:
- Th√†nh l·∫≠p 9/10/2011, ph∆∞∆°ng ch√¢m "Chia s·∫ª ƒë·ªÉ c√πng ph√°t tri·ªÉn"
- 6 team: AI, Mobile, Data, Game, Web, Backend
- Quy tr√¨nh: 3 v√≤ng (CV, PV, Training), ch·ªâ tuy·ªÉn nƒÉm 1
- S·ª± ki·ªán: BigGame, SPOJ, PROCWAR, Code Battle
- L·ªô tr√¨nh: C ‚Üí C++ ‚Üí CTDL&GT ‚Üí OOP ‚Üí Java"""
        
        user_prompt = f"C√¢u h·ªèi: {query}"
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        
        response = self._get_llm_response(messages)
        
        try:
            import json
            # T√¨m JSON trong response
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group())
                expanded = [query]
                expanded.extend(result.get("rewrites", []))
                expanded.extend(result.get("decomposed", []))
                expanded.extend(result.get("context_aware", []))
                return expanded
        except:
            pass
        
        # Fallback 
        return self.query_rewriting(query)
    
    def query_rewriting(self, query: str, num_variants: int = 2) -> List[str]:  
        system_prompt = """Vi·∫øt l·∫°i c√¢u h·ªèi CLB ProPTIT th√†nh 2 c√°ch kh√°c. Format: ["c√°ch 1", "c√°ch 2"]"""
        user_prompt = f"C√¢u h·ªèi: {query}"
        
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        
        response = self._get_llm_response(messages)
        
        try:
            import ast
            list_match = re.search(r'\[.*?\]', response, re.DOTALL)
            if list_match:
                variants = ast.literal_eval(list_match.group())
                return [query] + variants[:num_variants]
            else:
                lines = [line.strip().strip('"\'') for line in response.split('\n') if line.strip()]
                return [query] + lines[:num_variants]
        except:
            return [query, query.replace("CLB", "C√¢u l·∫°c b·ªô"), query.replace("ProPTIT", "L·∫≠p tr√¨nh PTIT")]
    
    def query_decomposition(self, query: str) -> List[str]:
        # Chia nh·ªè c√¢u h·ªèi ph·ª©c t·∫°p
        if ' v√† ' in query:
            parts = query.split(' v√† ')
            if len(parts) == 2:
                return [query] + [part.strip() + '?' for part in parts if part.strip()]
        
        # LLM decomposition cho c√¢u ph·ª©c t·∫°p
        if len(query.split()) > 10:  # Ch·ªâ d√πng LLM cho c√¢u d√†i
            system_prompt = """Chia c√¢u h·ªèi ph·ª©c t·∫°p th√†nh c√¢u con ƒë∆°n gi·∫£n. Format: ["c√¢u 1", "c√¢u 2"]"""
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"C√¢u h·ªèi: {query}"}
            ]
            
            response = self._get_llm_response(messages)
            try:
                import ast
                list_match = re.search(r'\[.*?\]', response, re.DOTALL)
                if list_match:
                    sub_queries = ast.literal_eval(list_match.group())
                    return [query] + sub_queries
            except:
                pass
        
        return [query]
    
    def synonym_expansion(self, query: str) -> List[str]:
        expanded_queries = [query]
        
        # Thay th·∫ø t·ª´ ƒë·ªìng nghƒ©a
        for category, synonyms in self.proptit_keywords.items():
            for synonym in synonyms[:3]: 
                pattern = re.compile(r'\b' + re.escape(synonym) + r'\b', flags=re.IGNORECASE)
                if pattern.search(query):
                    for alternative in synonyms[:2]:  
                        if alternative.lower() != synonym.lower():
                            new_query = pattern.sub(alternative, query)
                            if new_query != query and new_query not in expanded_queries:
                                expanded_queries.append(new_query)
                                if len(expanded_queries) >= 4:  
                                    return expanded_queries
                    break
        
        return expanded_queries
    
    def context_aware_expansion(self, query: str) -> List[str]:
        expanded = [query]
        query_lower = query.lower()
        
        # Ph√¢n t√≠ch ng·ªØ c·∫£nh c√¢u h·ªèi d·ª±a tr√™n intent v√† keywords
        
        # 1. C√¢u h·ªèi v·ªÅ th·ªùi gian th√†nh l·∫≠p, l·ªãch s·ª≠
        if any(word in query_lower for word in ["th√†nh l·∫≠p", "khi n√†o", "nƒÉm n√†o", "l·ªãch s·ª≠"]):
            expanded.extend([
                "CLB ProPTIT ƒë∆∞·ª£c th√†nh l·∫≠p ng√†y 9/10/2011",
                "L·ªãch s·ª≠ h√¨nh th√†nh CLB ProPTIT",
                "Ng∆∞·ªùi s√°ng l·∫≠p CLB ProPTIT"
            ])
        
        # 2. C√¢u h·ªèi v·ªÅ s·ªë l∆∞·ª£ng th√†nh vi√™n
        elif any(word in query_lower for word in ["bao nhi√™u th√†nh vi√™n", "s·ªë l∆∞·ª£ng", "c√≥ bao nhi·ªÅu"]):
            expanded.extend([
                "Quy m√¥ th√†nh vi√™n CLB ProPTIT",
                "CLB tuy·ªÉn 25 th√†nh vi√™n m·ªói nƒÉm",
                "T·ªïng s·ªë th√†nh vi√™n hi·ªán t·∫°i CLB"
            ])
        
        # 3. C√¢u h·ªèi v·ªÅ quy tr√¨nh tham gia
        elif any(word in query_lower for word in ["tham gia", "v√†o clb", "ƒëƒÉng k√Ω"]):
            expanded.extend([
                "Quy tr√¨nh 3 v√≤ng tuy·ªÉn th√†nh vi√™n ProPTIT",
                "ƒêi·ªÅu ki·ªán tham gia CLB ProPTIT",
                "C√°ch ƒëƒÉng k√Ω v√†o CLB ProPTIT"
            ])
        
        # 4. C√¢u h·ªèi v·ªÅ ho·∫°t ƒë·ªông, s·ª± ki·ªán
        elif any(word in query_lower for word in ["ho·∫°t ƒë·ªông", "s·ª± ki·ªán", "event"]):
            expanded.extend([
                "C√°c s·ª± ki·ªán n·ªïi b·∫≠t c·ªßa CLB ProPTIT",
                "BigGame v√† SPOJ Tournament ProPTIT",
                "L·ªãch ho·∫°t ƒë·ªông h√†ng nƒÉm CLB"
            ])
        
        # 5. C√¢u h·ªèi v·ªÅ teams, c∆° c·∫•u t·ªï ch·ª©c
        elif any(word in query_lower for word in ["team", "nh√≥m", "ph√¢n chia"]) and "th√†nh vi√™n" not in query_lower:
            # T·∫°o bi·∫øn th·ªÉ c√¢u h·ªèi thay v√¨ tr·∫£ v·ªÅ ƒë√°p √°n
            expanded.extend([
                "CLB ProPTIT c√≥ bao nhi√™u team d·ª± √°n?",
                "Danh s√°ch c√°c team d·ª± √°n c·ªßa CLB ProPTIT",
                "C√°c team d·ª± √°n trong CLB ProPTIT g·ªìm nh·ªØng n√†o?"
            ])
        
        # 6. C√¢u h·ªèi v·ªÅ h·ªçc t·∫≠p, l·ªô tr√¨nh
        elif any(word in query_lower for word in ["h·ªçc", "l·ªô tr√¨nh", "training", "ƒë√†o t·∫°o"]):
            expanded.extend([
                "L·ªô tr√¨nh h·ªçc t·∫≠p t·∫°i ProPTIT",
                "Ch∆∞∆°ng tr√¨nh training C++ ProPTIT",
                "CTDL&GT v√† OOP trong CLB"
            ])
        
        # 7. C√¢u h·ªèi v·ªÅ l·ª£i √≠ch, gi√° tr·ªã
        elif any(word in query_lower for word in ["l·ª£i √≠ch", "c√≥ g√¨", "t·∫°i sao", "gi√° tr·ªã"]):
            expanded.extend([
                "L·ª£i √≠ch khi tham gia CLB ProPTIT",
                "K·ªπ nƒÉng ƒë·∫°t ƒë∆∞·ª£c t·ª´ CLB",
                "0.1 ƒëi·ªÉm x√©t h·ªçc b·ªïng ProPTIT"
            ])
        
        return expanded[:4]  
    
    def document_structure_expansion(self, query: str) -> List[str]:
        doc_keywords = {
            "th√†nh l·∫≠p": ["L·ªãch s·ª≠ CLB ProPTIT", "9/10/2011 ProPTIT"],
            "ph∆∞∆°ng ch√¢m": ["Chia s·∫ª ƒë·ªÉ c√πng ph√°t tri·ªÉn", "Slogan ProPTIT"],
            "team": ["6 team ProPTIT", "Team AI, Mobile, Data, Game, Web, Backend"],
            "tuy·ªÉn": ["3 v√≤ng tuy·ªÉn", "CV, Ph·ªèng v·∫•n, Training"],
            "training": ["L·ªô tr√¨nh C ‚Üí C++ ‚Üí CTDL&GT", "OOP Java ProPTIT"],
            "s·ª± ki·ªán": ["BigGame SPOJ PROCWAR", "Code Battle Game C++"]
        }
        
        expanded = [query]
        query_lower = query.lower()
        
        for keyword, expansions in doc_keywords.items():
            if keyword in query_lower:
                expanded.extend(expansions[:2])
                break
        
        return expanded[:3]
    
    def expand_query(self, query: str, techniques: List[str] = None, max_expansions: int = 8) -> List[str]:
        """
        Optimized main expansion function - ∆∞u ti√™n rule-based, gi·∫£m LLM calls
        
        Args:
            query: C√¢u h·ªèi g·ªëc
            techniques: Danh s√°ch k·ªπ thu·∫≠t mu·ªën s·ª≠ d·ª•ng
            max_expansions: S·ªë l∆∞·ª£ng t·ªëi ƒëa c√¢u h·ªèi m·ªü r·ªông (gi·∫£m t·ª´ 10 xu·ªëng 8)
        
        Returns:
            List[str]: Danh s√°ch c√¢u h·ªèi ƒë√£ m·ªü r·ªông
        """
        if techniques is None:
            # ∆Øu ti√™n rule-based techniques tr∆∞·ªõc
            techniques = ["rule_based", "synonym", "context", "combined_llm"]
        
        all_expanded = [query]
        
        try:
            # 2. Synonym expansion 
            if "synonym" in techniques:
                synonyms = self.synonym_expansion(query)
                all_expanded.extend(synonyms[1:])
            
            # 3. Context-aware expansion
            if "context" in techniques:
                context_aware = self.context_aware_expansion(query)
                all_expanded.extend(context_aware[1:])
            
            # 4. Combined LLM expansion 
            if "combined_llm" in techniques and len(all_expanded) < max_expansions:
                combined = self.combined_llm_expansion(query)
                all_expanded.extend(combined[1:])
            
            # Legacy techniques
            if "decomposition" in techniques:
                decomposed = self.query_decomposition(query)
                all_expanded.extend(decomposed[1:])
            
            if "document_structure" in techniques:
                structure_aware = self.document_structure_expansion(query)
                all_expanded.extend(structure_aware[1:])
        
        except Exception as e:
            print(f"Error in query expansion: {e}")
        
        unique_expanded = []
        seen = set()
        for q in all_expanded:
            if q.lower() not in seen:
                unique_expanded.append(q)
                seen.add(q.lower())
        
        return unique_expanded[:max_expansions]
    
    def rank_expanded_queries(self, original_query: str, expanded_queries: List[str], embedding_model) -> List[str]:
        """
        X·∫øp h·∫°ng c√°c c√¢u h·ªèi m·ªü r·ªông d·ª±a tr√™n ƒë·ªô t∆∞∆°ng ƒë·ªìng v·ªõi c√¢u h·ªèi g·ªëc
        
        Args:
            original_query: C√¢u h·ªèi g·ªëc
            expanded_queries: Danh s√°ch c√¢u h·ªèi ƒë√£ m·ªü r·ªông
            embedding_model: Model ƒë·ªÉ t·∫°o embedding
        
        Returns:
            List[str]: Danh s√°ch c√¢u h·ªèi ƒë√£ ƒë∆∞·ª£c x·∫øp h·∫°ng
        """
        import torch
        
        if len(expanded_queries) <= 1:
            return expanded_queries
        
        try:
            # T·∫°o embedding cho c√¢u h·ªèi g·ªëc
            original_embedding = embedding_model.encode(original_query)
            
            # T√≠nh similarity scores
            scores = []
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
            original_tensor = torch.tensor(original_embedding, device=device, dtype=torch.float)
            
            for query in expanded_queries:
                if query == original_query:
                    scores.append((query, 1.0))  # Perfect score for original
                else:
                    query_embedding = embedding_model.encode(query)
                    query_tensor = torch.tensor(query_embedding, device=device, dtype=torch.float)
                    
                    # Cosine similarity
                    norm1 = torch.norm(original_tensor)
                    norm2 = torch.norm(query_tensor)
                    if norm1.item() == 0 or norm2.item() == 0:
                        similarity = 0.0
                    else:
                        cos_sim = torch.dot(original_tensor, query_tensor) / (norm1 * norm2)
                        similarity = cos_sim.item()
                    
                    scores.append((query, similarity))
            
            # Sort by similarity (descending)
            scores.sort(key=lambda x: x[1], reverse=True)
            
            return [query for query, _ in scores]
        
        except Exception as e:
            print(f"Error in ranking queries: {e}")
            return expanded_queries


def test_query_expansion():
    """Function ƒë·ªÉ test c√°c k·ªπ thu·∫≠t query expansion t·ªëi ∆∞u"""
    expander = QueryExpansion()
    
    test_queries = [
        "CLB ProPTIT c√≥ nh·ªØng ho·∫°t ƒë·ªông g√¨?",
        "L√†m th·∫ø n√†o ƒë·ªÉ tham gia CLB?",
        "Nh·ªØng th√†nh vi√™n n·ªïi b·∫≠t c·ªßa CLB l√† ai?",
        "CLB ƒë∆∞·ª£c th√†nh l·∫≠p khi n√†o v√† c√≥ bao nhi√™u th√†nh vi√™n?"
    ]
    
    for query in test_queries:
        print(f"\n{'='*50}")
        print(f"Original Query: {query}")
        print(f"{'='*50}")
        
        print("\n2. Synonym Expansion (Rule-based):")
        synonyms = expander.synonym_expansion(query)
        for i, syn in enumerate(synonyms):
            print(f"   {i+1}. {syn}")
        
        print("\n3. Context-Aware Expansion (Template-based):")
        context = expander.context_aware_expansion(query)
        for i, ctx in enumerate(context):
            print(f"   {i+1}. {ctx}")
        
        print("\n4. Combined LLM Expansion (1 API call):")
        combined = expander.combined_llm_expansion(query)
        for i, comb in enumerate(combined):
            print(f"   {i+1}. {comb}")
        
        print("\n5. Optimized All Techniques Combined:")
        all_expanded = expander.expand_query(query, max_expansions=8)
        for i, exp in enumerate(all_expanded):
            print(f"   {i+1}. {exp}")
        
        print(f"\nTotal expansions: {len(all_expanded)} (vs old version ~10-15)")


if __name__ == "__main__":
    test_query_expansion()
