# ğŸ¯ HÆ°á»›ng Dáº«n Fine-tune Reranker cho ProPTIT RAG System

## ğŸ“‹ Má»¥c Lá»¥c
1. [Giá»›i thiá»‡u](#giá»›i-thiá»‡u)
2. [Chuáº©n bá»‹](#chuáº©n-bá»‹)
3. [Cháº¡y Fine-tune](#cháº¡y-fine-tune)
4. [Tá»‘i Æ°u cho Hardware háº¡n cháº¿](#tá»‘i-Æ°u-cho-hardware-háº¡n-cháº¿)
5. [Sá»­ dá»¥ng Model Ä‘Ã£ Fine-tune](#sá»­-dá»¥ng-model-Ä‘Ã£-fine-tune)
6. [Troubleshooting](#troubleshooting)

---

## ğŸ“ Giá»›i thiá»‡u

Script `fine_tune_reranker.py` cho phÃ©p báº¡n fine-tune má»™t cross-encoder reranker model trÃªn domain-specific data cá»§a CLB ProPTIT. Reranker Ä‘Æ°á»£c train Ä‘á»ƒ:

- **Positive pairs**: Xáº¿p háº¡ng cao cÃ¡c document cÃ³ liÃªn quan Ä‘áº¿n query (ground truth)
- **Hard negatives**: PhÃ¢n biá»‡t Ä‘Æ°á»£c cÃ¡c document tÆ°Æ¡ng tá»± nhÆ°ng khÃ´ng pháº£i Ä‘Ã¡p Ã¡n Ä‘Ãºng
- **Random negatives**: Reject cÃ¡c document khÃ´ng liÃªn quan

### Kiáº¿n trÃºc Training

```
Query + Document â†’ Cross-Encoder â†’ Score (0-1)
                                    â†“
                              BCE Loss vá»›i labels:
                              - 1 cho positive pairs
                              - 0 cho negative pairs
```

**Hard Negative Mining**: Sá»­ dá»¥ng TF-IDF/BM25 Ä‘á»ƒ tÃ¬m cÃ¡c document "gáº§n" vá»›i query nhÆ°ng khÃ´ng pháº£i ground truth, giÃºp model há»c phÃ¢n biá»‡t tá»‘t hÆ¡n.

---

## ğŸ› ï¸ Chuáº©n bá»‹

### 1. Kiá»ƒm tra Hardware

Vá»›i giá»›i háº¡n há»‡ thá»‘ng:
- **RAM**: 12.7 GB
- **GPU RAM**: 15 GB  
- **Disk**: 112.6 GB

Script Ä‘Ã£ Ä‘Æ°á»£c optimize vá»›i:
- Batch size nhá» (4-8)
- Gradient accumulation (8 steps)
- Mixed precision (FP16)
- Gradient checkpointing
- Checkpoint saving Ä‘á»ƒ recovery

### 2. CÃ i Ä‘áº·t Dependencies

Äáº£m báº£o Ä‘Ã£ cÃ i Ä‘áº·t cÃ¡c packages cáº§n thiáº¿t:

```bash
pip install -r requirements.txt
```

CÃ¡c package chÃ­nh:
- `transformers` - Hugging Face transformers
- `FlagEmbedding` - Cross-encoder reranker
- `torch` - PyTorch
- `scikit-learn` - TF-IDF cho hard negative mining
- `pandas`, `openpyxl` - Data loading

### 3. Kiá»ƒm tra Data

Äáº£m báº£o cÃ³ cÃ¡c file:
- `CLB_PROPTIT.csv`: Corpus (99 documents)
- `train_data_proptit.xlsx`: Training queries (100 queries)

---

## ğŸš€ Cháº¡y Fine-tune

### Cáº¥u hÃ¬nh CÆ¡ báº£n (Khuyáº¿n nghá»‹ cho GPU 15GB)

```bash
python fine_tune_reranker.py \
  --corpus_csv CLB_PROPTIT.csv \
  --train_xlsx train_data_proptit.xlsx \
  --base_model namdp-ptit/ViRanker \
  --batch_size 4 \
  --gradient_accumulation_steps 8 \
  --epochs 5 \
  --lr 2e-5 \
  --fp16 \
  --gradient_checkpointing \
  --dev_ratio 0.2 \
  --num_hard_negatives 2 \
  --num_random_negatives 1
```

### Giáº£i thÃ­ch Parameters

| Parameter | MÃ´ táº£ | GiÃ¡ trá»‹ khuyáº¿n nghá»‹ |
|-----------|-------|---------------------|
| `--base_model` | Model reranker pretrained | `namdp-ptit/ViRanker` hoáº·c `BAAI/bge-reranker-v2-m3` |
| `--batch_size` | Sá»‘ samples/batch (per device) | 4-8 |
| `--gradient_accumulation_steps` | Accumulate N batches trÆ°á»›c khi update | 8 (effective batch=32) |
| `--epochs` | Sá»‘ epoch training | 5-10 |
| `--lr` | Learning rate | 2e-5 (standard cho fine-tune) |
| `--fp16` | Mixed precision FP16 | Báº­t cho GPU |
| `--gradient_checkpointing` | Tiáº¿t kiá»‡m GPU memory | Báº­t náº¿u OOM |
| `--dev_ratio` | % data cho validation | 0.2 (20%) |
| `--num_hard_negatives` | Hard negatives/positive | 2-3 |
| `--num_random_negatives` | Random negatives/positive | 1 |
| `--max_seq_length` | Max tokens cho query+doc | 512 |
| `--save_steps` | LÆ°u checkpoint má»—i N steps | 100 |
| `--eval_steps` | Evaluate má»—i N steps | 100 |

---

## âš™ï¸ Tá»‘i Æ°u cho Hardware háº¡n cháº¿

### Chiáº¿n lÆ°á»£c 1: Giáº£m Memory Usage

**Náº¿u gáº·p OOM (Out of Memory):**

```bash
python fine_tune_reranker.py \
  --batch_size 2 \
  --gradient_accumulation_steps 16 \
  --fp16 \
  --gradient_checkpointing \
  --max_seq_length 384
```

### Chiáº¿n lÆ°á»£c 2: Quick Test Run

**Cháº¡y nhanh Ä‘á»ƒ test setup:**

```bash
python fine_tune_reranker.py \
  --batch_size 4 \
  --gradient_accumulation_steps 4 \
  --epochs 1 \
  --max_steps 50 \
  --dev_ratio 0.1 \
  --num_hard_negatives 1
```

### Chiáº¿n lÆ°á»£c 3: Maximize Quality (náº¿u cÃ³ Ä‘á»§ resources)

```bash
python fine_tune_reranker.py \
  --batch_size 8 \
  --gradient_accumulation_steps 4 \
  --epochs 10 \
  --lr 1e-5 \
  --dev_ratio 0.2 \
  --num_hard_negatives 3 \
  --num_random_negatives 2 \
  --hard_neg_top_k 100
```

### TÃ­nh toÃ¡n Effective Batch Size

```
Effective Batch Size = batch_size Ã— gradient_accumulation_steps
```

**VÃ­ dá»¥:**
- `batch_size=4` + `gradient_accumulation_steps=8` â†’ Effective = 32
- TÆ°Æ¡ng Ä‘Æ°Æ¡ng training vá»›i batch size 32 nhÆ°ng chá»‰ dÃ¹ng memory cho 4 samples

---

## ğŸ“¦ Sá»­ dá»¥ng Model Ä‘Ã£ Fine-tune

### 1. Tá»± Ä‘á»™ng Load trong main.py

File `main.py` Ä‘Ã£ Ä‘Æ°á»£c cáº­p nháº­t Ä‘á»ƒ tá»± Ä‘á»™ng tÃ¬m vÃ  load fine-tuned reranker má»›i nháº¥t:

```python
# Tá»± Ä‘á»™ng tÃ¬m fine-tuned reranker model má»›i nháº¥t
import glob
finetuned_reranker_dirs = sorted(glob.glob("outputs/reranker-finetuned-*"), reverse=True)
if finetuned_reranker_dirs and os.path.exists(os.path.join(finetuned_reranker_dirs[0], "config.json")):
    print(f"[Reranker] Using fine-tuned reranker: {finetuned_reranker_dirs[0]}")
    reranker = Reranker(model_name=finetuned_reranker_dirs[0])
else:
    print("[Reranker] Using pretrained reranker: namdp-ptit/ViRanker")
    reranker = Reranker(model_name="namdp-ptit/ViRanker")
```

### 2. Manual Load

```python
from rerank import Reranker

# Thay tháº¿ path báº±ng output directory cá»§a báº¡n
reranker = Reranker(model_name="outputs/reranker-finetuned-20250101-120000")

# Sá»­ dá»¥ng
query = "CLB PROPTIT hoáº¡t Ä‘á»™ng nhÆ° tháº¿ nÃ o?"
passages = [doc1, doc2, doc3, ...]

scores, ranked_passages = reranker(query, passages)
```

### 3. Kiá»ƒm tra Quality

Sau khi fine-tune, cháº¡y metrics Ä‘á»ƒ Ä‘Ã¡nh giÃ¡:

```bash
python main.py
```

So sÃ¡nh cÃ¡c metrics vá»›i baseline (xem `README.md` hoáº·c `.github/copilot-instructions.md`):
- **context_precision@k**: TÄƒng â†’ Ã­t noise hÆ¡n
- **context_recall@k**: TÄƒng â†’ tÃ¬m Ä‘Æ°á»£c nhiá»u relevant docs hÆ¡n
- **hit@k**: TÄƒng â†’ ground truth xuáº¥t hiá»‡n trong top-k
- **mrr@k**: TÄƒng â†’ ground truth xuáº¥t hiá»‡n á»Ÿ vá»‹ trÃ­ cao hÆ¡n

---

## ğŸ”§ Troubleshooting

### Problem 1: CUDA Out of Memory

**Triá»‡u chá»©ng:**
```
RuntimeError: CUDA out of memory
```

**Giáº£i phÃ¡p:**
1. Giáº£m `--batch_size` (thá»­ 2 hoáº·c 1)
2. Báº­t `--gradient_checkpointing`
3. Giáº£m `--max_seq_length` (thá»­ 384 hoáº·c 256)
4. TÄƒng `--gradient_accumulation_steps` Ä‘á»ƒ giá»¯ effective batch size

```bash
python fine_tune_reranker.py --batch_size 2 --gradient_accumulation_steps 16 --gradient_checkpointing
```

### Problem 2: Training quÃ¡ cháº­m

**Giáº£i phÃ¡p:**
1. Äáº£m báº£o Ä‘Ã£ báº­t `--fp16`
2. Kiá»ƒm tra GPU Ä‘ang Ä‘Æ°á»£c sá»­ dá»¥ng: `torch.cuda.is_available()`
3. Giáº£m `--num_hard_negatives` vÃ  `--num_random_negatives`
4. TÄƒng `--batch_size` náº¿u cÃ³ Ä‘á»§ memory

### Problem 3: Model khÃ´ng converge / Loss khÃ´ng giáº£m

**NguyÃªn nhÃ¢n cÃ³ thá»ƒ:**
- Learning rate quÃ¡ cao hoáº·c quÃ¡ tháº¥p
- Dá»¯ liá»‡u khÃ´ng Ä‘á»§ Ä‘a dáº¡ng
- Hard negatives quÃ¡ dá»…

**Giáº£i phÃ¡p:**
1. Thá»­ learning rate khÃ¡c (1e-5, 5e-6)
2. TÄƒng sá»‘ epoch
3. TÄƒng `--num_hard_negatives` vÃ  `--hard_neg_top_k`
4. Kiá»ƒm tra dev metrics Ä‘á»ƒ xÃ¡c nháº­n overfitting

```bash
# Thá»­ vá»›i learning rate tháº¥p hÆ¡n
python fine_tune_reranker.py --lr 1e-5 --epochs 10
```

### Problem 4: Checkpoint bá»‹ corrupted

**Giáº£i phÃ¡p:**
Script tá»± Ä‘á»™ng lÆ°u checkpoint má»—i `--save_steps`. Náº¿u training bá»‹ giÃ¡n Ä‘oáº¡n:

```bash
# Resume tá»« checkpoint má»›i nháº¥t
python fine_tune_reranker.py --base_model outputs/reranker-finetuned-XXXXXX/checkpoint-500
```

### Problem 5: Windows DataLoader multiprocessing issues

**Triá»‡u chá»©ng:**
```
RuntimeError: DataLoader worker (pid XXXX) is killed
```

**Giáº£i phÃ¡p:**
Script Ä‘Ã£ set `dataloader_num_workers=0` máº·c Ä‘á»‹nh. Náº¿u váº«n gáº·p lá»—i, Ä‘áº£m báº£o:
- Cháº¡y script trong `if __name__ == "__main__":` block
- KhÃ´ng sá»­ dá»¥ng Jupyter Notebook (cháº¡y tá»« terminal)

---

## ğŸ“Š Monitoring Training

### 1. Training Logs

Script sáº½ in ra:
- Loss má»—i `--logging_steps`
- Dev metrics má»—i `--eval_steps`
- Progress bar vá»›i tqdm

### 2. Output Files

Sau khi training xong, directory `outputs/reranker-finetuned-XXXXXX/` chá»©a:
- `pytorch_model.bin`: Model weights
- `config.json`: Model config
- `tokenizer_config.json`: Tokenizer config
- `training_info.json`: Training hyperparameters
- `dev_metrics.json`: Final dev set metrics
- `checkpoint-XXX/`: Intermediate checkpoints

### 3. Dev Metrics

Kiá»ƒm tra `dev_metrics.json`:
```json
{
  "accuracy": 0.85,
  "avg_pos_score": 0.78,
  "avg_neg_score": 0.32,
  "margin": 0.46
}
```

**Giáº£i thÃ­ch:**
- `accuracy`: Tá»· lá»‡ phÃ¢n loáº¡i Ä‘Ãºng (positive vs negative)
- `avg_pos_score`: Äiá»ƒm trung bÃ¬nh cho positive pairs (cÃ ng cao cÃ ng tá»‘t)
- `avg_neg_score`: Äiá»ƒm trung bÃ¬nh cho negative pairs (cÃ ng tháº¥p cÃ ng tá»‘t)
- `margin`: Khoáº£ng cÃ¡ch giá»¯a pos vÃ  neg scores (cÃ ng lá»›n cÃ ng tá»‘t)

**Target:**
- Accuracy > 0.80
- Margin > 0.40

---

## ğŸ¯ Best Practices

### 1. Data Quality
- Äáº£m báº£o ground truth documents chÃ­nh xÃ¡c
- Review vÃ  clean data trÆ°á»›c khi train
- Xem xÃ©t augment thÃªm queries náº¿u cÃ³ thá»ƒ

### 2. Hyperparameter Tuning
- Báº¯t Ä‘áº§u vá»›i default parameters
- Adjust dá»±a trÃªn dev metrics
- Grid search trÃªn learning rate vÃ  epochs

### 3. Model Selection
- `namdp-ptit/ViRanker`: Tá»‘t cho tiáº¿ng Viá»‡t
- `BAAI/bge-reranker-v2-m3`: Multilingual, performant

### 4. Training Strategy
- Train vá»›i full data sau khi verify vá»›i small test
- Save best model based on dev metrics
- Keep checkpoints Ä‘á»ƒ rollback náº¿u overfit

### 5. Evaluation
- LuÃ´n evaluate trÃªn test set sau khi train
- So sÃ¡nh vá»›i baseline pretrained model
- Test trÃªn real queries Ä‘á»ƒ xÃ¡c nháº­n quality

---

## ğŸ“ Example: Full Pipeline

```bash
# 1. Test setup nhanh (5-10 phÃºt)
python fine_tune_reranker.py \
  --batch_size 4 \
  --epochs 1 \
  --max_steps 20

# 2. Náº¿u OK, cháº¡y full training (30-60 phÃºt)
python fine_tune_reranker.py \
  --batch_size 4 \
  --gradient_accumulation_steps 8 \
  --epochs 5 \
  --fp16 \
  --gradient_checkpointing

# 3. Evaluate
python main.py

# 4. So sÃ¡nh metrics vá»›i baseline trong README.md
```

---

## ğŸ¤ Contributing & Support

Náº¿u gáº·p váº¥n Ä‘á»:
1. Kiá»ƒm tra [Troubleshooting](#troubleshooting)
2. Review training logs
3. Thá»­ adjust hyperparameters
4. LiÃªn há»‡ team AI - CLB ProPTIT

**Good luck vá»›i fine-tuning! ğŸš€**
